Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.
Loading pretrained model
Loading datasets
Training
Trainable parameters: 0.010% (0.307M/3085.939M)
Starting training..., iters: 100
Iter 1: Val loss 2.526, Val took 30.352s
Iter 10: Train loss 2.512, Learning Rate 2.000e-05, It/sec 0.642, Tokens/sec 786.978, Trained Tokens 12260, Peak mem 8.854 GB
Iter 20: Train loss 2.247, Learning Rate 2.000e-05, It/sec 0.638, Tokens/sec 796.720, Trained Tokens 24741, Peak mem 8.854 GB
Iter 30: Train loss 1.962, Learning Rate 2.000e-05, It/sec 0.633, Tokens/sec 792.547, Trained Tokens 37253, Peak mem 8.854 GB
Iter 40: Train loss 1.703, Learning Rate 2.000e-05, It/sec 0.640, Tokens/sec 803.589, Trained Tokens 49815, Peak mem 8.854 GB
Iter 50: Train loss 1.498, Learning Rate 2.000e-05, It/sec 0.639, Tokens/sec 781.795, Trained Tokens 62050, Peak mem 8.854 GB
Iter 60: Train loss 1.230, Learning Rate 2.000e-05, It/sec 0.650, Tokens/sec 798.469, Trained Tokens 74334, Peak mem 8.854 GB
Iter 70: Train loss 0.998, Learning Rate 2.000e-05, It/sec 0.650, Tokens/sec 795.626, Trained Tokens 86582, Peak mem 8.854 GB
Iter 80: Train loss 0.720, Learning Rate 2.000e-05, It/sec 0.654, Tokens/sec 803.635, Trained Tokens 98862, Peak mem 8.854 GB
Iter 90: Train loss 0.504, Learning Rate 2.000e-05, It/sec 0.645, Tokens/sec 795.789, Trained Tokens 111207, Peak mem 8.854 GB
Iter 100: Val loss 0.297, Val took 30.347s
Iter 100: Train loss 0.366, Learning Rate 2.000e-05, It/sec 0.646, Tokens/sec 796.724, Trained Tokens 123540, Peak mem 8.854 GB
Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.
Saved final weights to adapters/adapters.safetensors.
