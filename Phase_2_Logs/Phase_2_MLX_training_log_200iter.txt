Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.
Loading pretrained model
Loading datasets
Training
Trainable parameters: 0.010% (0.307M/3085.939M)
Starting training..., iters: 200
Iter 1: Val loss 1.639, Val took 28.362s
Iter 10: Train loss 1.702, Learning Rate 2.000e-05, It/sec 0.231, Tokens/sec 688.211, Trained Tokens 29800, Peak mem 15.979 GB
Iter 20: Train loss 1.474, Learning Rate 2.000e-05, It/sec 0.205, Tokens/sec 669.081, Trained Tokens 62491, Peak mem 18.855 GB
Iter 30: Train loss 1.337, Learning Rate 2.000e-05, It/sec 0.196, Tokens/sec 645.680, Trained Tokens 95499, Peak mem 21.590 GB
Iter 40: Train loss 1.266, Learning Rate 2.000e-05, It/sec 0.249, Tokens/sec 712.187, Trained Tokens 124057, Peak mem 21.590 GB
Iter 50: Train loss 1.069, Learning Rate 2.000e-05, It/sec 0.233, Tokens/sec 726.165, Trained Tokens 155256, Peak mem 21.590 GB
Iter 60: Train loss 0.864, Learning Rate 2.000e-05, It/sec 0.186, Tokens/sec 603.471, Trained Tokens 187706, Peak mem 24.673 GB
Iter 70: Train loss 0.700, Learning Rate 2.000e-05, It/sec 0.180, Tokens/sec 575.738, Trained Tokens 219727, Peak mem 24.673 GB
Iter 80: Train loss 0.560, Learning Rate 2.000e-05, It/sec 0.226, Tokens/sec 694.141, Trained Tokens 250491, Peak mem 24.673 GB
Iter 90: Train loss 0.458, Learning Rate 2.000e-05, It/sec 0.195, Tokens/sec 645.155, Trained Tokens 283536, Peak mem 24.673 GB
Iter 100: Train loss 0.338, Learning Rate 2.000e-05, It/sec 0.215, Tokens/sec 683.492, Trained Tokens 315331, Peak mem 24.673 GB
Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.
Iter 110: Train loss 0.275, Learning Rate 2.000e-05, It/sec 0.182, Tokens/sec 606.812, Trained Tokens 348688, Peak mem 24.675 GB
Iter 120: Train loss 0.248, Learning Rate 2.000e-05, It/sec 0.201, Tokens/sec 617.527, Trained Tokens 379384, Peak mem 24.675 GB
Iter 130: Train loss 0.237, Learning Rate 2.000e-05, It/sec 0.240, Tokens/sec 722.182, Trained Tokens 409513, Peak mem 24.675 GB
Iter 140: Train loss 0.218, Learning Rate 2.000e-05, It/sec 0.217, Tokens/sec 649.216, Trained Tokens 439454, Peak mem 24.675 GB
Iter 150: Train loss 0.220, Learning Rate 2.000e-05, It/sec 0.228, Tokens/sec 692.324, Trained Tokens 469840, Peak mem 24.675 GB
Iter 160: Train loss 0.219, Learning Rate 2.000e-05, It/sec 0.194, Tokens/sec 629.364, Trained Tokens 502201, Peak mem 24.675 GB
Iter 170: Train loss 0.198, Learning Rate 2.000e-05, It/sec 0.197, Tokens/sec 622.545, Trained Tokens 533810, Peak mem 24.675 GB
Iter 180: Train loss 0.209, Learning Rate 2.000e-05, It/sec 0.196, Tokens/sec 638.838, Trained Tokens 566428, Peak mem 24.675 GB
Iter 190: Train loss 0.178, Learning Rate 2.000e-05, It/sec 0.245, Tokens/sec 723.989, Trained Tokens 596018, Peak mem 24.675 GB
Iter 200: Val loss 0.203, Val took 28.175s
Iter 200: Train loss 0.165, Learning Rate 2.000e-05, It/sec 0.211, Tokens/sec 653.213, Trained Tokens 626984, Peak mem 24.675 GB
Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.
Saved final weights to adapters/adapters.safetensors.
