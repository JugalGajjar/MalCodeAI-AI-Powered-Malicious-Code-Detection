Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.
Loading pretrained model
Loading datasets
Training
Trainable parameters: 0.010% (0.307M/3085.939M)
Starting training..., iters: 100
Iter 1: Val loss 1.639, Val took 28.339s
Iter 10: Train loss 1.682, Learning Rate 3.000e-05, It/sec 0.231, Tokens/sec 689.564, Trained Tokens 29800, Peak mem 15.979 GB
Iter 20: Train loss 1.388, Learning Rate 3.000e-05, It/sec 0.205, Tokens/sec 670.765, Trained Tokens 62491, Peak mem 18.855 GB
Iter 30: Train loss 1.186, Learning Rate 3.000e-05, It/sec 0.192, Tokens/sec 633.235, Trained Tokens 95499, Peak mem 21.590 GB
Iter 40: Train loss 1.006, Learning Rate 3.000e-05, It/sec 0.247, Tokens/sec 704.846, Trained Tokens 124057, Peak mem 21.590 GB
Iter 50: Train loss 0.743, Learning Rate 3.000e-05, It/sec 0.233, Tokens/sec 725.532, Trained Tokens 155256, Peak mem 21.590 GB
Iter 60: Train loss 0.486, Learning Rate 3.000e-05, It/sec 0.186, Tokens/sec 604.066, Trained Tokens 187706, Peak mem 24.673 GB
Iter 70: Train loss 0.341, Learning Rate 3.000e-05, It/sec 0.181, Tokens/sec 579.905, Trained Tokens 219727, Peak mem 24.673 GB
Iter 80: Train loss 0.268, Learning Rate 3.000e-05, It/sec 0.226, Tokens/sec 696.801, Trained Tokens 250491, Peak mem 24.673 GB
Iter 90: Train loss 0.273, Learning Rate 3.000e-05, It/sec 0.194, Tokens/sec 640.223, Trained Tokens 283536, Peak mem 24.673 GB
Iter 100: Val loss 0.221, Val took 28.256s
Iter 100: Train loss 0.212, Learning Rate 3.000e-05, It/sec 0.215, Tokens/sec 684.959, Trained Tokens 315331, Peak mem 24.673 GB
Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.
Saved final weights to adapters/adapters.safetensors.
