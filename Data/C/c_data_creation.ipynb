{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {filepath}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath):\n",
    "    try:\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(f\"Data saved successfully to {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_dict[\"C\"] = \"\"\"\n",
    "You are an expert C code analyst. Your task is to dissect a given C code snippet and provide a structured JSON response detailing all of its components and overall functionality. You must strictly adhere to the following JSON format:\n",
    "{\n",
    "  \"output\": {\n",
    "    \"programming_language\": \"C\",\n",
    "    \"components\": [\n",
    "      {\n",
    "        \"component_type\": \"<TYPE_OF_C_COMPONENT>\",\n",
    "        \"component_name\": \"<NAME_OF_COMPONENT_IF_APPLICABLE>\",\n",
    "        \"component_code\": \"<THE_ACTUAL_C_CODE_OF_THE_COMPONENT>\",\n",
    "        \"component_description\": \"<DETAILED_DESCRIPTION_OF_COMPONENT_FUNCTIONALITY>\"\n",
    "      },\n",
    "      { /* ... more components ... */ }\n",
    "    ],\n",
    "    \"overall_description\": \"<DETAILED_SUMMARY/DESCRIPTION_OF_THE_ENTIRE_C_CODE_FUNCTIONALITY>\"\n",
    "  }\n",
    "}\n",
    "\n",
    "**Instructions:**\n",
    "1. Identify the Programming Language: The script is written in C.\n",
    "2. Component Types: Use appropriate component types, for example, HEADER_INCLUDE, MACRO_DEFINITION, GLOBAL_VARIABLE, FUNCTION_DEFINITION, STRUCT_DEFINITION, MAIN_FUNCTION, LOOP, CONDITIONAL_STATEMENT.\n",
    "3. Component Names: Provide the correct identifier for each component (function name, variable name, struct name, etc.), or NULL if not applicable.\n",
    "4. Component Code: Include the complete, unmodified C code for each component.\n",
    "5. Component Descriptions: Provide a detailed, technical explanation of what each component does, including data structures, control flow, memory management, and function interactions.\n",
    "6. Overall Description: Provide a detailed summary of the entire C code, explaining its purpose, architecture, and how components interact.\n",
    "7. Strict JSON Output: Your ENTIRE response must be ONLY the valid JSON object. Do not include any explanations, introductions, or additional text outside the JSON structure.\n",
    "\n",
    "Analyze the following C code properly and return ONLY the JSON response with no additional text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_json(input_string):\n",
    "    try:\n",
    "        data = json.loads(input_string)\n",
    "        return data, None\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(data, system_prompt):\n",
    "    output_data = {}\n",
    "\n",
    "    for key, value in data.items():\n",
    "        model_name = \"qwen2.5-coder:32b\"\n",
    "\n",
    "        response: ChatResponse = chat(model=model_name, messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': system_prompt,\n",
    "            },\n",
    "            { \n",
    "                'role': 'user',\n",
    "                'content': value,\n",
    "            }\n",
    "        ])\n",
    "\n",
    "        output_data[key] = {\n",
    "            \"input\" : value,\n",
    "            \"output\" : response['message']['content']\n",
    "        }\n",
    "            \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(\"c_train.json\")\n",
    "\n",
    "num_keys = 25\n",
    "if not isinstance(data, dict):\n",
    "    raise TypeError(\"Input must be a dictionary.\")\n",
    "\n",
    "if not isinstance(num_keys, int) or num_keys < 0:\n",
    "    raise ValueError(\"num_keys must be a non-negative integer.\")\n",
    "\n",
    "sliced_dict = {}\n",
    "count = 0\n",
    "for key, value in data.items():\n",
    "    if count < num_keys:\n",
    "        sliced_dict[key] = value\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "del data\n",
    "del count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sliced dictionary: 25\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of sliced dictionary:\", len(sliced_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = make_data(sliced_dict, system_prompt_dict[\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_LITE_DELEGATES_GPU_GL_GL_TEXTURE_H_\n",
      "#define TENSORFLOW_LITE_DELEGATES_GPU_GL_GL_TEXTURE_H_\n",
      "\n",
      "#include \"absl/types/span.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/data_type.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/status.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/tensor.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/types.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/gl/gl_call.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/gl/portable_gl31.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace gpu {\n",
      "namespace gl {\n",
      "\n",
      "// Texture is an RAII wrapper for OpenGL texture object.\n",
      "// See https://www.khronos.org/opengl/wiki/Texture for more information.\n",
      "//\n",
      "// Texture is moveable but not copyable.\n",
      "class GlTexture {\n",
      " public:\n",
      "  // Creates invalid texture.\n",
      "  GlTexture()\n",
      "      : GlTexture(GL_INVALID_ENUM, GL_INVALID_INDEX, GL_INVALID_ENUM, 0, 0,\n",
      "                  false) {}\n",
      "\n",
      "  GlTexture(GLenum target, GLuint id, GLenum format, size_t bytes_size,\n",
      "            GLint layer, bool owned)\n",
      "      : id_(id),\n",
      "        target_(target),\n",
      "        format_(format),\n",
      "        bytes_size_(bytes_size),\n",
      "        layer_(layer),\n",
      "        owned_(owned) {}\n",
      "\n",
      "  // Move-only\n",
      "  GlTexture(GlTexture&& texture);\n",
      "  GlTexture& operator=(GlTexture&& texture);\n",
      "  GlTexture(const GlTexture&) = delete;\n",
      "  GlTexture& operator=(const GlTexture&) = delete;\n",
      "\n",
      "  ~GlTexture();\n",
      "\n",
      "  // Binds a texture as an image to the given index.\n",
      "  absl::Status BindAsReadonlyImage(uint32_t index) const;\n",
      "\n",
      "  // Bind texture as an image for write access at given index.\n",
      "  absl::Status BindAsWriteonlyImage(uint32_t index) const;\n",
      "\n",
      "  // Bind texture as an image for read-write access at given index.\n",
      "  absl::Status BindAsReadWriteImage(uint32_t index) const;\n",
      "\n",
      "  // Binds a texture as a sampler to the given index.\n",
      "  absl::Status BindAsSampler2D(uint32_t index) const;\n",
      "\n",
      "  GLenum target() const { return target_; }\n",
      "\n",
      "  GLuint id() const { return id_; }\n",
      "\n",
      "  GLenum format() const { return format_; }\n",
      "\n",
      "  GLint layer() const { return layer_; }\n",
      "\n",
      "  bool is_valid() const { return id_ != GL_INVALID_INDEX; }\n",
      "\n",
      "  size_t bytes_size() const { return bytes_size_; }\n",
      "\n",
      "  // @return true if this object actually owns corresponding GL buffer\n",
      "  //         and manages it's lifetime.\n",
      "  bool has_ownership() const { return owned_; }\n",
      "\n",
      " private:\n",
      "  void Invalidate();\n",
      "\n",
      "  absl::Status BindImage(uint32_t index, GLenum access) const;\n",
      "\n",
      "  GLuint id_;\n",
      "  GLenum target_;\n",
      "  GLenum format_;\n",
      "  size_t bytes_size_;\n",
      "  GLint layer_;\n",
      "  bool owned_;\n",
      "};\n",
      "\n",
      "// Creates new 2D image texture that will be filled with float32 data once which\n",
      "// will be used for reading.\n",
      "//\n",
      "// @param size defines 2D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTexture(const uint2& size,\n",
      "                                        absl::Span<const float> data,\n",
      "                                        GlTexture* gl_texture);\n",
      "\n",
      "// Creates new 2D image texture that will be filled with float16 data once which\n",
      "// will be used for reading.\n",
      "//\n",
      "// @param size defines 2D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTextureF16(const uint2& size,\n",
      "                                           absl::Span<const uint16_t> data,\n",
      "                                           GlTexture* gl_texture);\n",
      "\n",
      "// Creates new 2D image texture that will be filled with uint8 data once which\n",
      "// will be used for reading.\n",
      "//\n",
      "// @param size defines 2D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTextureU8(const uint2& size,\n",
      "                                          absl::Span<const uint8_t> data,\n",
      "                                          GlTexture* gl_texture);\n",
      "\n",
      "// Creates new 3D RGBA image texture that will be filled with float32 data once\n",
      "// which will be used for reading.\n",
      "//\n",
      "// @param size defines 3D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTexture(const uint3& size,\n",
      "                                        absl::Span<const float> data,\n",
      "                                        GlTexture* gl_texture);\n",
      "\n",
      "// Creates new 3D RGBA image texture that will be filled with float16 data once\n",
      "// which will be used for reading.\n",
      "//\n",
      "// @param size defines 3D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTextureF16(const uint3& size,\n",
      "                                           absl::Span<const uint16_t> data,\n",
      "                                           GlTexture* gl_texture);\n",
      "\n",
      "// Creates new RGBA 2D image texture\n",
      "//\n",
      "// @param size defines 2D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadWriteRgbaImageTexture(DataType data_type,\n",
      "                                             const uint2& size,\n",
      "                                             GlTexture* gl_texture);\n",
      "\n",
      "// Creates new RGBA 3D image texture\n",
      "//\n",
      "// @param size defines 3D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadWriteRgbaImageTexture(DataType data_type,\n",
      "                                             const uint3& size,\n",
      "                                             GlTexture* gl_texture);\n",
      "\n",
      "namespace gl_texture_internal {\n",
      "\n",
      "// RAII for creating and/or owning texture id.\n",
      "class TextureId {\n",
      " public:\n",
      "  TextureId() : id_(GL_INVALID_INDEX) {\n",
      "    TFLITE_GPU_CALL_GL(glGenTextures, 1 /* number of textures*/, &id_)\n",
      "        .IgnoreError();\n",
      "  }\n",
      "\n",
      "  explicit TextureId(GLuint id) : id_(id) {}\n",
      "\n",
      "  ~TextureId() {\n",
      "    if (id_ != GL_INVALID_INDEX) {\n",
      "      TFLITE_GPU_CALL_GL(glDeleteTextures, 1, &id_).IgnoreError();\n",
      "    }\n",
      "  }\n",
      "\n",
      "  GLuint id() const { return id_; }\n",
      "\n",
      "  GLuint Release() {\n",
      "    GLuint id = GL_INVALID_INDEX;\n",
      "    std::swap(id, id_);\n",
      "    return id;\n",
      "  }\n",
      "\n",
      " private:\n",
      "  GLuint id_;\n",
      "};\n",
      "\n",
      "// RAII for binding and unbinding a texture.\n",
      "class TextureBinder {\n",
      " public:\n",
      "  TextureBinder(GLenum target, GLuint id) : target_(target) {\n",
      "    TFLITE_GPU_CALL_GL(glBindTexture, target_, id).IgnoreError();\n",
      "  }\n",
      "\n",
      "  ~TextureBinder() {\n",
      "    TFLITE_GPU_CALL_GL(glBindTexture, target_, 0).IgnoreError();\n",
      "  }\n",
      "\n",
      " private:\n",
      "  const GLenum target_;\n",
      "};\n",
      "\n",
      "}  // namespace gl_texture_internal\n",
      "}  // namespace gl\n",
      "}  // namespace gpu\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_DELEGATES_GPU_GL_GL_TEXTURE_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/types/span.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl::Span library for handling array views in a type-safe and efficient manner.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/delegates/gpu/common/data_type.h\\\"\",\n",
      "        \"component_description\": \"Includes the data_type definitions used by TensorFlow Lite GPU delegate for specifying tensor data types.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/delegates/gpu/common/tensor.h\\\"\",\n",
      "        \"component_description\": \"Includes the common tensor definitions used by TensorFlow Lite GPU delegate for handling tensors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/delegates/gpu/gl/gl_call.h\\\"\",\n",
      "        \"component_description\": \"Includes the OpenGL call wrapper utility used to ensure proper error checking and logging of OpenGL API calls.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"GlTexture\",\n",
      "        \"component_code\": \"class GlTexture { ... };\",\n",
      "        \"component_description\": \"Defines a RAII class for managing OpenGL texture resources, including creation, binding, and deletion. It provides methods to bind the texture as an image with specified access rights and checks ownership of the underlying GL buffer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CreateReadOnlyImageTexture\",\n",
      "        \"component_code\": \"absl::Status CreateReadOnlyImageTexture(const uint2& size, absl::Span<const float> data, GlTexture* gl_texture);\",\n",
      "        \"component_description\": \"Function to create a new 2D image texture filled with float32 data for read-only usage.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CreateReadOnlyImageTextureF16\",\n",
      "        \"component_code\": \"absl::Status CreateReadOnlyImageTextureF16(const uint2& size, absl::Span<const uint16_t> data, GlTexture* gl_texture);\",\n",
      "        \"component_description\": \"Function to create a new 2D image texture filled with float16 data for read-only usage.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CreateReadOnlyImageTextureU8\",\n",
      "        \"component_code\": \"absl::Status CreateReadOnlyImageTextureU8(const uint2& size, absl::Span<const uint8_t> data, GlTexture* gl_texture);\",\n",
      "        \"component_description\": \"Function to create a new 2D image texture filled with uint8 data for read-only usage.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CreateReadWriteRgbaImageTexture\",\n",
      "        \"component_code\": \"absl::Status CreateReadWriteRgbaImageTexture(DataType data_type, const uint2& size, GlTexture* gl_texture);\",\n",
      "        \"component_description\": \"Function to create a new 2D RGBA image texture that can be used for both reading and writing.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"TextureId\",\n",
      "        \"component_code\": \"class TextureId { ... };\",\n",
      "        \"component_description\": \"Defines an RAII class for managing OpenGL texture IDs. It generates a new texture ID on construction and deletes it when the object is destroyed.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"TextureBinder\",\n",
      "        \"component_code\": \"class TextureBinder { ... };\",\n",
      "        \"component_description\": \"Defines an RAII class for binding and unbinding OpenGL textures. It binds the specified texture on construction and unbinds it when the object is destroyed.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_LITE_KERNELS_SHIM_TENSOR_VIEW_H_\n",
      "#define TENSORFLOW_LITE_KERNELS_SHIM_TENSOR_VIEW_H_\n",
      "\n",
      "#include \"absl/status/statusor.h\"\n",
      "#include \"absl/strings/string_view.h\"\n",
      "#include \"absl/types/span.h\"\n",
      "#include \"absl/types/variant.h\"\n",
      "#include \"tensorflow/core/platform/logging.h\"\n",
      "#include \"tensorflow/core/platform/tstring.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace shim {\n",
      "\n",
      "// A type deduction template which is specialized for TF and TFLite.\n",
      "// That is it maps\n",
      "//   ::tensorflow::Tensor -> tflite::shim::TfTensorView\n",
      "//   ::TfLiteTensor -> tflite::shim::TfLiteTensorView\n",
      "template <typename W>\n",
      "struct TensorViewSubType {};\n",
      "\n",
      "// Common denominator for ::tflite::TfLiteTensor and ::tensorflow::Tensor.\n",
      "// It is a \"view\" over the underlying tensor without taking ownership.\n",
      "// Objects of this class can also mutate the underlying tensor depending on\n",
      "// whether the underlying tensor is \"const\" qualified or not.\n",
      "//\n",
      "// Movable and copyable.\n",
      "// It can be instantiated with the New() factory function. eg.\n",
      "//   TfTensorView t           = TensorView::New(&tf_tensor);\n",
      "//   const TfTensorView t     = TensorView::New(&const_tf_tensor);\n",
      "//   TfLiteTensorView t       = TensorView::New(&tflite_tensor);\n",
      "//   const TfLiteTensorView t = TensorView::New(&const_tflite_tensor);\n",
      "class TensorView {\n",
      " protected:\n",
      "  // Union over all data types\n",
      "  using DataVariantType =\n",
      "      absl::variant<absl::Span<bool>, absl::Span<uint8_t>, absl::Span<uint64_t>,\n",
      "                    absl::Span<int8_t>, absl::Span<int16_t>,\n",
      "                    absl::Span<int32_t>, absl::Span<int64_t>, absl::Span<float>,\n",
      "                    absl::Span<double>, absl::Span<::tensorflow::tstring>>;\n",
      "\n",
      "  // An interface while provides convenient row-major indexing over the\n",
      "  // underlying tensor.\n",
      "  // Example usage:\n",
      "  //\n",
      "  //   // A scalar view\n",
      "  //   const TensorView t_float\n",
      "  //   float val = t_float.AsScalar<float>();\n",
      "  //\n",
      "  //   // A vector view\n",
      "  //   const TensorView t_int;\n",
      "  //   auto t_int_vec = t_int.As<int32_t, /*RANK=*/ 1>();\n",
      "  //   int sum = t_int_vec(0) + t_int_vec(1);\n",
      "  //\n",
      "  //   // A matrix view\n",
      "  //   TensorView t_str;\n",
      "  //   auto t_str_mat = t_str.As<tensorflow::tstring, /*RANK=*/ 2>();\n",
      "  //   t_str_mat(0, 0) = \"abc\";\n",
      "  //   t_str_mat(2, 3) = \"def\";\n",
      "  template <typename DType, int RANK>\n",
      "  class Tensor {\n",
      "   public:\n",
      "    explicit Tensor(TensorView *t)\n",
      "        : data_(t->Data<DType>()), shape_(t->Shape()) {\n",
      "      DCHECK_EQ(RANK, shape_.size());\n",
      "      ComputeRowSizes();\n",
      "    }\n",
      "\n",
      "    explicit Tensor(const TensorView *t)\n",
      "        : data_(t->Data<DType>()), shape_(t->Shape()) {\n",
      "      DCHECK_EQ(RANK, shape_.size());\n",
      "      ComputeRowSizes();\n",
      "    }\n",
      "\n",
      "    // indexing operator\n",
      "    template <typename... IndexTypes>\n",
      "    inline DType &operator()(IndexTypes... indices) {\n",
      "      const auto idx = RowMajorIndex(std::array<int, RANK>{{indices...}});\n",
      "      return data_[idx];\n",
      "    }\n",
      "\n",
      "    // const indexing operator\n",
      "    template <typename... IndexTypes>\n",
      "    inline const DType &operator()(IndexTypes... indices) const {\n",
      "      const auto idx = RowMajorIndex(std::array<int, RANK>{{indices...}});\n",
      "      return data_.at(idx);\n",
      "    }\n",
      "\n",
      "    // Pointer accessor\n",
      "    typename absl::Span<DType>::pointer Ptr() { return data_.data(); }\n",
      "    constexpr typename absl::Span<DType>::const_pointer Ptr() const {\n",
      "      return data_.data();\n",
      "    }\n",
      "\n",
      "    // Size of the given dimension\n",
      "    inline int Dim(int dim_i) const {\n",
      "      DCHECK(RANK > 0 && dim_i < RANK) << \"dim: \" << dim_i << \" rank:\" << RANK;\n",
      "      // Handle negative indices\n",
      "      if (dim_i < 0) dim_i = ((dim_i % RANK) + RANK) % RANK;\n",
      "      return shape_[dim_i];\n",
      "    }\n",
      "\n",
      "    // The tensor's rank: number of dimensions\n",
      "    /*[[nodiscard]]*/ constexpr std::size_t Rank() const { return RANK; }\n",
      "\n",
      "   private:\n",
      "    // Computes the row-major index\n",
      "    inline std::size_t RowMajorIndex(\n",
      "        const std::array<int, RANK> &indices) const {\n",
      "      std::size_t ret = 0;\n",
      "      for (int i = 0; i < RANK; ++i) ret += indices[i] * row_sizes_[i];\n",
      "      return ret;\n",
      "    }\n",
      "\n",
      "    // Pre computes row sizes to convert multi dim indices into a row major\n",
      "    // index\n",
      "    void ComputeRowSizes() {\n",
      "      // Precompute row sizes for row major index computation\n",
      "      if (RANK > 0) {\n",
      "        row_sizes_[RANK - 1] = 1;\n",
      "        for (int i = RANK - 2; i >= 0; --i) {\n",
      "          row_sizes_[i] = row_sizes_[i + 1] * shape_[i + 1];\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "\n",
      "    absl::Span<DType> data_;\n",
      "    const absl::Span<int> shape_;\n",
      "    std::size_t row_sizes_[RANK]{};\n",
      "  };\n",
      "\n",
      " public:\n",
      "  // Factory which gets specialized for different wrapped tensor types.\n",
      "  template <typename W>\n",
      "  static absl::StatusOr<typename TensorViewSubType<W>::Type> New(\n",
      "      W *wrapped_tensor);\n",
      "\n",
      " protected:\n",
      "  // Move constructor\n",
      "  TensorView(TensorView &&o) = default;\n",
      "  // Copy constructor\n",
      "  TensorView(const TensorView &o) = default;\n",
      "  // Move assignment operator\n",
      "  TensorView &operator=(TensorView &&o) = default;\n",
      "  // Copy assignment operator\n",
      "  TensorView &operator=(const TensorView &) = default;\n",
      "\n",
      " public:\n",
      "  // Dtor\n",
      "  virtual ~TensorView() = default;\n",
      "\n",
      "  // Accessors\n",
      "\n",
      "  // Shape\n",
      "  absl::Span<int> Shape() { return shape_; }\n",
      "  /*[[nodiscard]]*/ const absl::Span<int> Shape() const { return shape_; }\n",
      "\n",
      "  // Data\n",
      "  template <typename DType>\n",
      "  absl::Span<DType> &Data() {\n",
      "    return absl::get<absl::Span<DType>>(data_);\n",
      "  }\n",
      "  template <typename DType>\n",
      "  constexpr absl::Span<DType> Data() const {\n",
      "    return absl::get<absl::Span<DType>>(data_);\n",
      "  }\n",
      "\n",
      "  // Reads the tensor given the dtype and its rank and provides an indexing\n",
      "  // operator.\n",
      "  template <typename DType, int RANK>\n",
      "  Tensor<DType, RANK> As() {\n",
      "    return Tensor<DType, RANK>(this);\n",
      "  }\n",
      "\n",
      "  // Const version of As()\n",
      "  template <typename DType, int RANK>\n",
      "  const Tensor<DType, RANK> As() const {\n",
      "    return Tensor<DType, RANK>(this);\n",
      "  }\n",
      "\n",
      "  // Read the given tensor as a scalar or return error if it isn't\n",
      "  template <typename DType>\n",
      "  DType &AsScalar();\n",
      "\n",
      "  template <typename DType>\n",
      "  const DType &AsScalar() const;\n",
      "\n",
      " protected:\n",
      "  // Templated constructor. Since it's not possible to specify the template\n",
      "  // argument directly we place a dummy argument of that type so compiler\n",
      "  // can deduce the right template parameter\n",
      "  template <typename DType>\n",
      "  TensorView(const absl::Span<int> shape, void *data,\n",
      "             const std::size_t data_size, const DType &)\n",
      "      : shape_(shape),\n",
      "        data_(absl::Span<DType>(reinterpret_cast<DType *>(data),\n",
      "                                data_size / sizeof(DType))) {}\n",
      "\n",
      "  // Return the total number of elements given the shape.\n",
      "  static constexpr std::size_t NumElements(const absl::Span<int> shape) {\n",
      "    std::size_t ret = 1;\n",
      "    for (const auto dim : shape) ret *= dim;\n",
      "    return ret;\n",
      "  }\n",
      "\n",
      "  // Tensor shape\n",
      "  // Note: using int rather than size_t to avoid conversion to from TfLite shape\n",
      "  absl::Span<int> shape_;\n",
      "  // Tensor data\n",
      "  DataVariantType data_;\n",
      "};\n",
      "\n",
      "// Add or remove const qualifier to O based on whether it is in I.\n",
      "// For example\n",
      "//   MatchConstNess<const TfLiteTensor, TensorView>::Type == const TensorView\n",
      "//   MatchConstNess<TfLiteTensor, TensorView>::Type == TensorView\n",
      "//   MatchConstNess<TfLiteTensor, const TensorView>::Type == TensorView\n",
      "template <typename I, typename O>\n",
      "struct MatchConstNess {\n",
      "  using Type = std::conditional_t<std::is_const<I>::value, std::add_const_t<O>,\n",
      "                                  std::remove_const_t<O>>;\n",
      "};\n",
      "\n",
      "///////////////////////////// Implementation\n",
      "\n",
      "template <typename DType>\n",
      "DType &TensorView::AsScalar() {\n",
      "  DCHECK_EQ(shape_.size(), 0) << \"Tensor is not a scalar\";\n",
      "  return Data<DType>()[0];\n",
      "}\n",
      "\n",
      "template <typename DType>\n",
      "const DType &TensorView::AsScalar() const {\n",
      "  DCHECK_EQ(shape_.size(), 0) << \"Tensor is not a scalar\";\n",
      "  return Data<DType>().at(0);\n",
      "}\n",
      "\n",
      "}  // namespace shim\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_KERNELS_SHIM_TENSOR_VIEW_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "The provided code snippet defines a C++ class template `TensorView` designed to provide a generic interface for accessing and manipulating tensor data, which can be wrapped around different types of tensors such as those from TensorFlow Lite or other frameworks. Here is a detailed breakdown of the components and functionalities:\n",
      "\n",
      "### Class Template: `TensorView`\n",
      "\n",
      "#### Overview\n",
      "- **Purpose**: To abstract away the specifics of underlying tensor representations (e.g., TensorFlow Lite tensors) while providing common access patterns.\n",
      "- **Features**:\n",
      "  - Supports dynamic data types using `absl::Span` and `std::variant`.\n",
      "  - Provides shape information and multi-dimensional indexing via the nested `Tensor` class.\n",
      "  - Facilitates scalar access with error checking.\n",
      "\n",
      "#### Key Components\n",
      "\n",
      "1. **Nested Class: `Tensor`**\n",
      "   - Purpose: To provide a higher-level interface for accessing tensor elements using multi-dimensional indices.\n",
      "   - Features:\n",
      "     - Supports arbitrary rank (dimensionality) of tensors.\n",
      "     - Pre-computes row sizes to efficiently convert multi-dimensional indices into flat (row-major) indices.\n",
      "\n",
      "2. **Factory Method: `New`**\n",
      "   - Purpose: To create specialized instances of `TensorView` for different wrapped tensor types.\n",
      "   - Usage:\n",
      "     ```cpp\n",
      "     auto tensor_view = TensorView::New(wrapped_tensor);\n",
      "     ```\n",
      "\n",
      "3. **Data Storage and Access**\n",
      "   - Uses `absl::Span<int>` to store the shape of the tensor.\n",
      "   - Employs a `std::variant` (`DataVariantType`) to hold data spans of different types (e.g., `float`, `int`).\n",
      "   - Provides templated methods `Data<DType>()` for accessing the underlying data span.\n",
      "\n",
      "4. **Shape Accessor**\n",
      "   - Returns the shape of the tensor as an `absl::Span<int>`.\n",
      "\n",
      "5. **Scalar Access Methods**\n",
      "   - Methods `AsScalar<DType>()` and their const counterparts allow reading a tensor's value when it is guaranteed to be a scalar (i.e., has no dimensions).\n",
      "\n",
      "6. **Protected Members and Constructors**\n",
      "   - Includes move/copy constructors and assignment operators.\n",
      "   - Uses a templated protected constructor to initialize the `TensorView` with shape, data pointer, and element count.\n",
      "\n",
      "7. **Helper Struct: `MatchConstNess`**\n",
      "   - Purpose: To match the const-ness of types `I` and `O`. This is useful when creating specialized instances of `TensorView`.\n",
      "   - Usage:\n",
      "     ```cpp\n",
      "     using ResultType = MatchConstNess<const SomeType, TensorView>::Type;\n",
      "     // ResultType will be `const TensorView`\n",
      "     ```\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "Here's an example demonstrating how you might use the `TensorView` class:\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include \"tensorflow/lite/kernels/shim/tensor_view.h\"\n",
      "\n",
      "// Assuming we have a wrapped tensor type, e.g., TfLiteTensor\n",
      "struct WrappedTensor {\n",
      "    int* shape;\n",
      "    size_t num_dimensions;\n",
      "    void* data;\n",
      "    size_t bytes;\n",
      "};\n",
      "\n",
      "namespace tflite {\n",
      "namespace shim {\n",
      "\n",
      "template <>\n",
      "struct TensorViewSubType<WrappedTensor> {\n",
      "    using Type = TensorView;\n",
      "};\n",
      "\n",
      "absl::StatusOr<typename TensorViewSubType<WrappedTensor>::Type> TensorView::New(WrappedTensor* wrapped_tensor) {\n",
      "    std::vector<int> shape(wrapped_tensor->num_dimensions);\n",
      "    for (size_t i = 0; i < wrapped_tensor->num_dimensions; ++i) {\n",
      "        shape[i] = wrapped_tensor->shape[i];\n",
      "    }\n",
      "    return TensorView(absl::Span<int>(shape.data(), shape.size()),\n",
      "                      wrapped_tensor->data,\n",
      "                      wrapped_tensor->bytes,\n",
      "                      std::integral_constant<float, 0>{}); // Assuming float data type for simplicity\n",
      "}\n",
      "\n",
      "}  // namespace shim\n",
      "}  // namespace tflite\n",
      "\n",
      "int main() {\n",
      "    WrappedTensor tensor = {/* Initialize with actual values */};\n",
      "\n",
      "    auto status_or_tensor_view = tflite::shim::TensorView::New(&tensor);\n",
      "    if (!status_or_tensor_view.ok()) {\n",
      "        std::cerr << \"Failed to create TensorView: \" << status_or_tensor_view.status().ToString() << \"\\n\";\n",
      "        return 1;\n",
      "    }\n",
      "\n",
      "    tflite::shim::TensorView& tensor_view = *status_or_tensor_view;\n",
      "\n",
      "    // Access the shape\n",
      "    auto shape = tensor_view.Shape();\n",
      "    std::cout << \"Shape: \";\n",
      "    for (int dim : shape) {\n",
      "        std::cout << dim << \" \";\n",
      "    }\n",
      "    std::cout << \"\\n\";\n",
      "\n",
      "    // Access a scalar value if it is a 0-dimensional tensor\n",
      "    try {\n",
      "        float& scalar_value = tensor_view.AsScalar<float>();\n",
      "        std::cout << \"Scalar Value: \" << scalar_value << \"\\n\";\n",
      "    } catch (const std::exception& e) {\n",
      "        std::cerr << \"Error accessing scalar: \" << e.what() << \"\\n\";\n",
      "    }\n",
      "\n",
      "    // Access elements using the Tensor class\n",
      "    tflite::shim::TensorView::Tensor tensor_access = tensor_view.Tensor();\n",
      "    float value = tensor_access(0, 1); // Example multi-dimensional access\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Notes\n",
      "- The example assumes that `WrappedTensor` is a struct or class representing the underlying tensor data.\n",
      "- Specialization of `TensorViewSubType` for `WrappedTensor` is required to indicate which type should be returned by `New`.\n",
      "- Error handling with `absl::StatusOr` ensures robustness when creating and using `TensorView` instances.\n",
      "\n",
      "This design allows for flexible and type-safe interaction with tensors, making it easier to integrate different tensor representations into a unified interface.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\n",
      "#define TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\n",
      "\n",
      "#include \"tensorflow/core/platform/types.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace generator {\n",
      "namespace cpp {\n",
      "\n",
      "struct CppConfig {\n",
      "  string category;\n",
      "  string unit;\n",
      "  std::vector<string> namespaces;\n",
      "\n",
      "  explicit CppConfig() = default;\n",
      "  explicit CppConfig(const string &category,\n",
      "                     const string &name_space = \"tensorflow::ops\");\n",
      "};\n",
      "\n",
      "}  // namespace cpp\n",
      "}  // namespace generator\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\\n#define TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\\n#endif  // TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\",\n",
      "        \"component_description\": \"A header guard to prevent multiple inclusions of the header file, ensuring that the contents are only processed once during compilation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/core/platform/types.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/types.h\\\"\",\n",
      "        \"component_description\": \"Includes a header file from TensorFlow's core platform that likely defines fundamental types used in the codebase.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\\n}\",\n",
      "        \"component_description\": \"Defines the 'tensorflow' namespace, encapsulating all subsequent declarations within it to avoid name collisions with other libraries or user code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"generator\",\n",
      "        \"component_code\": \"namespace generator {\\n}\",\n",
      "        \"component_description\": \"Defines the 'generator' namespace within 'tensorflow', further encapsulating declarations related to code generation functionalities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"cpp\",\n",
      "        \"component_code\": \"namespace cpp {\\n}\",\n",
      "        \"component_description\": \"Defines the 'cpp' namespace within 'tensorflow::generator', specifically for C++-related code generation functionalities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"STRUCT_DEFINITION\",\n",
      "        \"component_name\": \"CppConfig\",\n",
      "        \"component_code\": \"struct CppConfig {\\n  string category;\\n  string unit;\\n  std::vector<string> namespaces;\\n\\n  explicit CppConfig() = default;\\n  explicit CppConfig(const string &category,\\n                     const string &name_space = \\\"tensorflow::ops\\\");\\n};\",\n",
      "        \"component_description\": \"Defines a struct named 'CppConfig' within the 'tensorflow::generator::cpp' namespace. It contains three members: 'category', 'unit', and 'namespaces'. The struct provides two constructors: a default constructor that initializes the object with default values, and another constructor that allows setting initial values for 'category' and optionally 'name_space'.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file defines a configuration structure named 'CppConfig' within the 'tensorflow::generator::cpp' namespace. The purpose of this structure is to hold settings related to C++ code generation, such as category, unit, and namespaces. It includes an explicit default constructor and another constructor for initializing these settings. The use of namespaces helps in organizing the code and avoiding name conflicts with other parts of the TensorFlow library or external libraries.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_KERNELS_SPARSE_REORDER_OP_H_\n",
      "#define TENSORFLOW_CORE_KERNELS_SPARSE_REORDER_OP_H_\n",
      "\n",
      "#include \"tensorflow/core/framework/op_kernel.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "namespace functor {\n",
      "\n",
      "template <typename Device, typename T>\n",
      "struct SparseReorderFunctor {\n",
      "  void operator()(OpKernelContext* context, const Tensor& input_ind,\n",
      "                  const Tensor& input_val, const Tensor& input_shape_in);\n",
      "};\n",
      "\n",
      "}  // namespace functor\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_KERNELS_SPARSE_REORDER_OP_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/framework/op_kernel.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for OpKernel, which is a base class for TensorFlow operations that operate on Tensors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow { ... }\",\n",
      "        \"component_description\": \"Defines the 'tensorflow' namespace to encapsulate all components related to TensorFlow's core functionalities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"functor\",\n",
      "        \"component_code\": \"namespace functor { ... }\",\n",
      "        \"component_description\": \"Defines a nested 'functor' namespace within the 'tensorflow' namespace, typically used for defining function objects or functors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"TEMPLATE_STRUCT_DEFINITION\",\n",
      "        \"component_name\": \"SparseReorderFunctor\",\n",
      "        \"component_code\": \"template <typename Device, typename T> struct SparseReorderFunctor { ... };\",\n",
      "        \"component_description\": \"Defines a template struct named 'SparseReorderFunctor' that is parameterized by 'Device' and 'T'. This functor is designed to reorder sparse data structures in TensorFlow operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"operator()\",\n",
      "        \"component_code\": \"void operator()(OpKernelContext* context, const Tensor& input_ind, const Tensor& input_val, const Tensor& input_shape_in);\",\n",
      "        \"component_description\": \"Defines the function call operator for 'SparseReorderFunctor'. This operator takes an OpKernelContext pointer and three constant Tensors as parameters, which represent indices, values, and shape of a sparse tensor respectively. It is used to reorder these sparse data components.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"The provided C++ code snippet defines a header file for TensorFlow that introduces a template struct named 'SparseReorderFunctor'. This functor is designed to handle the reordering of sparse tensors within TensorFlow operations. The code includes necessary headers, uses namespaces to avoid name clashes, and outlines the functionality through a function call operator that takes a context and three tensor inputs.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_CORE_COMMON_RUNTIME_RING_REDUCER_H_\n",
      "#define TENSORFLOW_CORE_COMMON_RUNTIME_RING_REDUCER_H_\n",
      "\n",
      "#include <deque>\n",
      "#include <memory>\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"tensorflow/core/common_runtime/base_collective_executor.h\"\n",
      "#include \"tensorflow/core/common_runtime/ring_alg.h\"\n",
      "#include \"tensorflow/core/framework/collective.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "class Device;\n",
      "\n",
      "// Ring-algorithm implementation of collective all-reduce.\n",
      "class RingReducer : public RingAlg {\n",
      " public:\n",
      "  RingReducer() : RingAlg(REDUCTION_COLLECTIVE, \"Reduce\") {}\n",
      "  ~RingReducer() override;\n",
      "\n",
      "  // Begins async execution of the ring reduce algorithm.\n",
      "  // Must be called in a blockable thread.\n",
      "  // TODO(b/80529858): remove the previous warning when we have a dedicated\n",
      "  // collective threadpool.\n",
      "  void Run(StatusCallback done) override;\n",
      "\n",
      "  Status InitializeCollectiveParams(CollectiveParams* col_params) override;\n",
      "\n",
      " protected:\n",
      "  void InitRingField(RingField* rf, int chunk_idx, int subdiv_idx,\n",
      "                     int field_idx) override;\n",
      "\n",
      " private:\n",
      "  void ContinueAfterInputCopy();\n",
      "  bool RunAsyncParts();\n",
      "\n",
      "  Tensor group_size_tensor_;\n",
      "  Notification group_size_tensor_ready_;\n",
      "\n",
      "  friend class RingReducerTest;\n",
      "  friend class RingReducerInitParamsTest;\n",
      "};\n",
      "\n",
      "}  // namespace tensorflow\n",
      "#endif  // TENSORFLOW_CORE_COMMON_RUNTIME_RING_REDUCER_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <deque>\\n#include <memory>\\n#include <string>\\n#include <vector>\",\n",
      "        \"component_description\": \"Includes standard C++ library headers for data structures and memory management.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/common_runtime/base_collective_executor.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for the BaseCollectiveExecutor class, which is a part of TensorFlow's collective runtime.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/common_runtime/ring_alg.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for the RingAlg class, which provides an interface for ring-based algorithms in TensorFlow's collective runtime.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/framework/collective.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for collective operations framework definitions, which are used for synchronization and communication among devices in TensorFlow.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\",\n",
      "        \"component_description\": \"Declares a namespace named 'tensorflow' to encapsulate the code within it, avoiding name collisions with other libraries and namespaces.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"RingReducer\",\n",
      "        \"component_code\": \"class RingReducer : public RingAlg {\\n public:\\n  RingReducer() : RingAlg(REDUCTION_COLLECTIVE, \\\"Reduce\\\") {}\\n  ~RingReducer() override;\\n\\n  // Begins async execution of the ring reduce algorithm.\\n  // Must be called in a blockable thread.\\n  // TODO(b/80529858): remove the previous warning when we have a dedicated\\n  // collective threadpool.\\n  void Run(StatusCallback done) override;\\n\\n  Status InitializeCollectiveParams(CollectiveParams* col_params) override;\\n\\n protected:\\n  void InitRingField(RingField* rf, int chunk_idx, int subdiv_idx,\\n                     int field_idx) override;\\n\\n private:\\n  void ContinueAfterInputCopy();\\n  bool RunAsyncParts();\\n\\n  Tensor group_size_tensor_;\\n  Notification group_size_tensor_ready_;\\n\\n  friend class RingReducerTest;\\n  friend class RingReducerInitParamsTest;\\n};\",\n",
      "        \"component_description\": \"Defines a class 'RingReducer' that inherits from 'RingAlg'. It provides an implementation of the ring-based all-reduce algorithm. The class contains constructors, destructors, public methods for running and initializing collective operations, protected methods for initializing ring fields, private helper methods for asynchronous execution, and member variables for managing group size tensors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_CLOSURE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"}  // namespace tensorflow\",\n",
      "        \"component_description\": \"Closes the 'tensorflow' namespace.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"The provided C++ code defines a class named RingReducer within the TensorFlow framework. This class is designed to implement an all-reduce algorithm using a ring-based approach, which is essential for efficient communication and synchronization among multiple devices during distributed machine learning tasks. The class inherits from RingAlg, providing specific functionalities such as asynchronous execution of the ring reduce algorithm, initialization of collective parameters, and management of tensor data related to group sizes.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_CC_EXPERIMENTAL_LIBEXPORT_SAVE_H_\n",
      "#define TENSORFLOW_CC_EXPERIMENTAL_LIBEXPORT_SAVE_H_\n",
      "\n",
      "#include <string>\n",
      "\n",
      "#include \"tensorflow/core/platform/status.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace libexport {\n",
      "\n",
      "// Writes a saved model to disk.\n",
      "//\n",
      "// Writes a saved model to the given `export_dir`.\n",
      "TF_EXPORT Status Save(const std::string& export_dir);\n",
      "\n",
      "}  // namespace libexport\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CC_EXPERIMENTAL_EXPORT_EXPORT_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"string\",\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the C++ Standard Library string header, providing facilities to manipulate strings.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/core/platform/status.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/status.h\\\"\",\n",
      "        \"component_description\": \"Includes a TensorFlow-specific header that defines the Status class used for error handling in TensorFlow operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow { ... }\",\n",
      "        \"component_description\": \"Defines a namespace named 'tensorflow' to encapsulate code related to TensorFlow functionalities, avoiding name collisions with other libraries.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"libexport\",\n",
      "        \"component_code\": \"namespace libexport { ... }\",\n",
      "        \"component_description\": \"Defines a nested namespace named 'libexport' within the 'tensorflow' namespace to encapsulate code related to exporting functionalities of TensorFlow.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"Save\",\n",
      "        \"component_code\": \"TF_EXPORT Status Save(const std::string& export_dir);\",\n",
      "        \"component_description\": \"Declares a function named 'Save' that takes a constant reference to a string representing the export directory. It returns a Status object indicating the result of writing a saved model to the specified directory.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This C++ header file defines an interface for exporting TensorFlow models. It includes necessary headers, encapsulates the code within 'tensorflow::libexport' namespaces, and declares a function named 'Save'. The 'Save' function is designed to write a saved model to a specified directory on disk and returns a Status object to indicate the success or failure of the operation.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<gh_stars>1000+\n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_LITE_EXPERIMENTAL_DELEGATES_COREML_BUILDERS_CONVOLUTION_OP_BUILDER_H_\n",
      "#define TENSORFLOW_LITE_EXPERIMENTAL_DELEGATES_COREML_BUILDERS_CONVOLUTION_OP_BUILDER_H_\n",
      "\n",
      "#include \"tensorflow/lite/builtin_ops.h\"\n",
      "#include \"tensorflow/lite/c/builtin_op_data.h\"\n",
      "#include \"tensorflow/lite/c/common.h\"\n",
      "#include \"tensorflow/lite/delegates/coreml/builders/op_builder.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace delegates {\n",
      "namespace coreml {\n",
      "\n",
      "enum class ConvolutionType { kConv, kDepthwiseConv, kTransposeConv };\n",
      "\n",
      "// Layer that provides convolution and depthwise convolution.\n",
      "class ConvolutionOpBuilder : public OpBuilder {\n",
      " public:\n",
      "  explicit ConvolutionOpBuilder(GraphBuilder* graph_builder,\n",
      "                                ConvolutionType conv_type)\n",
      "      : OpBuilder(graph_builder), conv_type_(conv_type) {}\n",
      "\n",
      "  const std::string& DebugName() override;\n",
      "\n",
      "  CoreML::Specification::NeuralNetworkLayer* Build() override;\n",
      "\n",
      "  TfLiteStatus PopulateSubgraph(TfLiteContext* context) override;\n",
      "\n",
      "  void SetOutputChannels(uint64_t output_channels);\n",
      "\n",
      "  void SetNGroups(uint64_t n_groups);\n",
      "\n",
      "  void SetWeights(TfLiteTensor* weights);\n",
      "\n",
      "  void SetBias(TfLiteTensor* bias);\n",
      "\n",
      "  void SetOutputShape(TfLiteTensor* output_shape);\n",
      "\n",
      "  void SetParams(void* builtin_data);\n",
      "\n",
      "  TfLiteStatus RegisterInputs(const TfLiteIntArray* inputs,\n",
      "                              TfLiteContext* context) override;\n",
      "\n",
      "  TfLiteStatus RegisterOutputs(const TfLiteIntArray* outputs,\n",
      "                               TfLiteContext* context) override;\n",
      "\n",
      " private:\n",
      "  void FillCoreMLWeights();\n",
      "  void FillCoreMLBias();\n",
      "\n",
      "  // Transpose TFLite kernel weights to CoreML kernel weights.\n",
      "  // Should be called after setting CoreML's kernel shapes.\n",
      "  void TransposeKernelWeights();\n",
      "\n",
      "  uint64_t output_channels_;\n",
      "  uint64_t n_groups_ = 1;\n",
      "\n",
      "  ConvolutionType conv_type_;\n",
      "\n",
      "  // using default dilation_factor (1, 1)\n",
      "  // CoreML ConvolutionLayerParams.isDeconvolution == false\n",
      "  TfLiteTensor* weights_ = nullptr;\n",
      "  TfLiteTensor* bias_ = nullptr;\n",
      "  // Only used for TransposeConv.\n",
      "  TfLiteTensor* output_shape_ = nullptr;\n",
      "};\n",
      "\n",
      "}  // namespace coreml\n",
      "}  // namespace delegates\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_EXPERIMENTAL_DELEGATES_COREML_BUILDERS_CONVOLUTION_OP_BUILDER_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/builtin_ops.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file that defines constants and data structures for TensorFlow Lite's built-in operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/c/builtin_op_data.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file that provides C-style data structures for built-in operator data in TensorFlow Lite.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/c/common.h\\\"\",\n",
      "        \"component_description\": \"Includes the common header file for TensorFlow Lite's C API, providing general utilities and definitions.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/delegates/coreml/builders/op_builder.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file that defines the base class for operation builders in TensorFlow Lite's Core ML delegate.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"ENUM_DEFINITION\",\n",
      "        \"component_name\": \"ConvolutionType\",\n",
      "        \"component_code\": \"enum class ConvolutionType { kConv, kDepthwiseConv, kTransposeConv };\",\n",
      "        \"component_description\": \"Defines an enumeration representing different types of convolution operations: standard convolution (kConv), depthwise convolution (kDepthwiseConv), and transposed convolution (kTransposeConv).\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"ConvolutionOpBuilder\",\n",
      "        \"component_code\": \"class ConvolutionOpBuilder : public OpBuilder { ... };\",\n",
      "        \"component_description\": \"Defines a class named ConvolutionOpBuilder that inherits from OpBuilder. This class is responsible for building Core ML layers corresponding to convolution operations in TensorFlow Lite.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTRUCTOR\",\n",
      "        \"component_name\": \"ConvolutionOpBuilder\",\n",
      "        \"component_code\": \"explicit ConvolutionOpBuilder(GraphBuilder* graph_builder, ConvolutionType conv_type) : OpBuilder(graph_builder), conv_type_(conv_type) {}\",\n",
      "        \"component_description\": \"Constructor for the ConvolutionOpBuilder class. Initializes the base class with a pointer to a GraphBuilder and sets the type of convolution operation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"DebugName\",\n",
      "        \"component_code\": \"const std::string& DebugName() override;\",\n",
      "        \"component_description\": \"Pure virtual function that overrides OpBuilder's DebugName method. Returns a string representing the debug name of the operation builder.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"Build\",\n",
      "        \"component_code\": \"CoreML::Specification::NeuralNetworkLayer* Build() override;\",\n",
      "        \"component_description\": \"Pure virtual function that overrides OpBuilder's Build method. Constructs and returns a pointer to the Core ML neural network layer corresponding to the convolution operation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"PopulateSubgraph\",\n",
      "        \"component_code\": \"TfLiteStatus PopulateSubgraph(TfLiteContext* context) override;\",\n",
      "        \"component_description\": \"Pure virtual function that overrides OpBuilder's PopulateSubgraph method. Populates a subgraph in the TensorFlow Lite context with nodes and tensors required for the convolution operation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetOutputChannels\",\n",
      "        \"component_code\": \"void SetOutputChannels(uint64_t output_channels);\",\n",
      "        \"component_description\": \"Function to set the number of output channels for the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetNGroups\",\n",
      "        \"component_code\": \"void SetNGroups(uint64_t n_groups);\",\n",
      "        \"component_description\": \"Function to set the number of groups for grouped convolutions, used in depthwise convolution.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetWeights\",\n",
      "        \"component_code\": \"void SetWeights(TfLiteTensor* weights);\",\n",
      "        \"component_description\": \"Function to set the tensor containing the weights of the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetBias\",\n",
      "        \"component_code\": \"void SetBias(TfLiteTensor* bias);\",\n",
      "        \"component_description\": \"Function to set the tensor containing the biases for the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetOutputShape\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Implicit function (not explicitly shown in code) that would be used to set the output shape of the convolution operation, typically part of the builder's implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"TransposeWeights\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Implicit function (not explicitly shown in code) that would be used to transpose weights if necessary for Core ML compatibility, typically part of the builder's implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"RegisterInputsOutputs\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Implicit function (not explicitly shown in code) that would be used to register inputs and outputs of the convolution layer, typically part of the builder's implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetOutputShape\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Implicit function (not explicitly shown in code) that would be used to set the output shape of the convolution operation, typically part of the builder's implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"conv_type_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that holds the type of convolution operation, initialized in the constructor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"output_channels_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that stores the number of output channels for the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"n_groups_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that holds the number of groups used in grouped convolutions, such as depthwise convolution.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"weights_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that stores a pointer to the tensor containing the weights for the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"bias_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that stores a pointer to the tensor containing the biases for the convolution layer.\"\n",
      "      }\n",
      "    ],\n",
      "    \"description\": \"This header file defines the ConvolutionOpBuilder class, which is responsible for constructing Core ML layers corresponding to different types of convolution operations in TensorFlow Lite. The builder supports standard convolution, depthwise convolution, and transposed convolution by setting various parameters such as weights, biases, output channels, and groups.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_FRAMEWORK_RUN_HANDLER_H_\n",
      "#define TENSORFLOW_CORE_FRAMEWORK_RUN_HANDLER_H_\n",
      "\n",
      "#include \"tensorflow/core/lib/core/threadpool.h\"\n",
      "#include \"tensorflow/core/lib/histogram/histogram.h\"\n",
      "#include \"tensorflow/core/platform/context.h\"\n",
      "#include \"tensorflow/core/platform/mutex.h\"\n",
      "#include \"tensorflow/core/platform/thread_annotations.h\"\n",
      "#include \"tensorflow/core/protobuf/config.pb.h\"\n",
      "\n",
      "namespace Eigen {\n",
      "struct ThreadPoolDevice;\n",
      "}\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "class RunHandler;\n",
      "\n",
      "// RunHandlerPool is a fixed size pool of pre-allocated RunHandlers\n",
      "// that can be used for tracking inter-op work for a given Session::Run().\n",
      "// RunHandler(s) in the pool are initially 'inactive'. A RunHandler becomes\n",
      "// 'active' when its unique_ptr is returned by Get() and is being used by a\n",
      "// client. It becomes 'inactive' once more when its unique_ptr gets destroyed.\n",
      "//\n",
      "// Expected usage:\n",
      "//\n",
      "// * Create a single RunHandlerPool (say run_handler_pool_).\n",
      "//\n",
      "// * When a Session::Run() is invoked, obtain a handler by:\n",
      "// auto handler = run_handler_pool_->Get();\n",
      "//\n",
      "// * Use handler for scheduling all inter-op work by:\n",
      "// handler->ScheduleInterOpClosure(closure);\n",
      "//\n",
      "// This class is thread safe.\n",
      "class RunHandlerPool {\n",
      " public:\n",
      "  explicit RunHandlerPool(int num_inter_op_threads);\n",
      "\n",
      "  RunHandlerPool(int num_inter_op_threads, int num_intra_op_threads);\n",
      "  ~RunHandlerPool();\n",
      "\n",
      "  // Returns an inactive RunHandler from the pool.\n",
      "  //\n",
      "  // RunHandlers in RunHandlerPool are initially 'inactive'.\n",
      "  // A RunHandler becomes 'active' when its unique_ptr its returned by Get()\n",
      "  // and is being used by a client.  It becomes 'inactive' once more when the\n",
      "  // unique_ptr is destroyed.\n",
      "  //\n",
      "  // Will block unless there is an inactive handler.\n",
      "  std::unique_ptr<RunHandler> Get(\n",
      "      int64_t step_id = 0, int64_t timeout_in_ms = 0,\n",
      "      const RunOptions::Experimental::RunHandlerPoolOptions& options =\n",
      "          RunOptions::Experimental::RunHandlerPoolOptions());\n",
      "\n",
      "  // Get the priorities for active handlers. The return result is with the same\n",
      "  // order of the active handler list.\n",
      "  std::vector<int64_t> GetActiveHandlerPrioritiesForTesting() const;\n",
      "\n",
      " private:\n",
      "  class Impl;\n",
      "  friend class RunHandler;\n",
      "\n",
      "  std::unique_ptr<Impl> impl_;\n",
      "};\n",
      "\n",
      "// RunHandler can be used to schedule inter/intra-op closures to run on a global\n",
      "// pool shared across all Session::Run(s). The closures are enqueued to a\n",
      "// handler specific queue, from which the work is stolen in a priority order\n",
      "// (time of the Get() call).\n",
      "//\n",
      "// It can only be created via RunHandlerPool::Get().\n",
      "//\n",
      "// This class can be used instead of directly scheduling closures on a global\n",
      "// pool since it maintains a global view across all sessions and optimizes pool\n",
      "// scheduling to improve (median and tail) latency.\n",
      "//\n",
      "// This class is thread safe.\n",
      "class RunHandler {\n",
      " public:\n",
      "  void ScheduleInterOpClosure(std::function<void()> fn);\n",
      "  thread::ThreadPoolInterface* AsIntraThreadPoolInterface();\n",
      "\n",
      "  ~RunHandler();\n",
      "\n",
      " private:\n",
      "  class Impl;\n",
      "  friend class RunHandlerPool::Impl;\n",
      "\n",
      "  explicit RunHandler(Impl* impl);\n",
      "\n",
      "  Impl* impl_;  // NOT OWNED.\n",
      "};\n",
      "\n",
      "namespace internal {\n",
      "\n",
      "// TODO(azaks): Refactor with thread:ThreadPool\n",
      "class RunHandlerEnvironment {\n",
      "  typedef Thread EnvThread;\n",
      "  struct TaskImpl {\n",
      "    std::function<void()> f;\n",
      "    Context context;\n",
      "    uint64 trace_id;\n",
      "  };\n",
      "  Env* const env_;\n",
      "  const ThreadOptions thread_options_;\n",
      "  const string name_;\n",
      "\n",
      " public:\n",
      "  struct Task {\n",
      "    std::unique_ptr<TaskImpl> f;\n",
      "  };\n",
      "\n",
      "  RunHandlerEnvironment(Env* env, const ThreadOptions& thread_options,\n",
      "                        const string& name);\n",
      "\n",
      "  EnvThread* CreateThread(std::function<void()> f,\n",
      "                          const std::string& thread_name);\n",
      "\n",
      "  Task CreateTask(std::function<void()> f);\n",
      "\n",
      "  void ExecuteTask(const Task& t);\n",
      "};\n",
      "\n",
      "typedef typename RunHandlerEnvironment::Task Task;\n",
      "typedef Eigen::RunQueue<Task, 1024> Queue;\n",
      "\n",
      "// To reduce cache misses, we use a doubly-linked list of Waiter structs and\n",
      "// queue them in LIFO order rather than the FIFO order used by a single\n",
      "// condition variable.\n",
      "struct Waiter {\n",
      "  Waiter() {\n",
      "    next = this;\n",
      "    prev = this;\n",
      "  }\n",
      "  condition_variable cv;\n",
      "  mutex mu;\n",
      "  Waiter* next;\n",
      "  Waiter* prev;\n",
      "};\n",
      "\n",
      "class ThreadWorkSource {\n",
      " public:\n",
      "  ThreadWorkSource();\n",
      "\n",
      "  ~ThreadWorkSource();\n",
      "\n",
      "  Task EnqueueTask(Task t, bool is_blocking);\n",
      "\n",
      "  Task PopBlockingTask();\n",
      "\n",
      "  Task PopNonBlockingTask(int start_index, bool search_from_all_queue);\n",
      "\n",
      "  void WaitForWork(int max_sleep_micros);\n",
      "\n",
      "  int TaskQueueSize(bool is_blocking);\n",
      "\n",
      "  int64_t GetTracemeId();\n",
      "\n",
      "  void SetTracemeId(int64_t value);\n",
      "\n",
      "  void SetWaiter(uint64 version, Waiter* waiter, mutex* mutex);\n",
      "\n",
      "  int64_t GetInflightTaskCount(bool is_blocking);\n",
      "\n",
      "  void IncrementInflightTaskCount(bool is_blocking);\n",
      "\n",
      "  void DecrementInflightTaskCount(bool is_blocking);\n",
      "\n",
      "  unsigned NonBlockingWorkShardingFactor();\n",
      "\n",
      "  std::string ToString();\n",
      "\n",
      " private:\n",
      "  struct NonBlockingQueue {\n",
      "    mutex queue_op_mu;\n",
      "    char pad[128];\n",
      "    Queue queue;\n",
      "  };\n",
      "\n",
      "  int32 non_blocking_work_sharding_factor_;\n",
      "  Eigen::MaxSizeVector<NonBlockingQueue*> non_blocking_work_queues_;\n",
      "\n",
      "  std::atomic<int64_t> blocking_inflight_;\n",
      "  std::atomic<int64_t> non_blocking_inflight_;\n",
      "\n",
      "  Queue blocking_work_queue_;\n",
      "  mutex blocking_queue_op_mu_;\n",
      "  char pad_[128];\n",
      "  mutex waiters_mu_;\n",
      "  Waiter queue_waiters_ TF_GUARDED_BY(waiters_mu_);\n",
      "  std::atomic<int64_t> traceme_id_;\n",
      "\n",
      "  mutex run_handler_waiter_mu_;\n",
      "  uint64 version_ TF_GUARDED_BY(run_handler_waiter_mu_);\n",
      "  mutex* sub_thread_pool_waiter_mu_ TF_GUARDED_BY(run_handler_waiter_mu_);\n",
      "  Waiter* sub_thread_pool_waiter_ TF_GUARDED_BY(run_handler_waiter_mu_);\n",
      "};\n",
      "\n",
      "class RunHandlerThreadPool {\n",
      " public:\n",
      "  struct PerThread {\n",
      "    constexpr PerThread() : pool(nullptr), thread_id(-1) {}\n",
      "    RunHandlerThreadPool* pool;  // Parent pool, or null for normal threads.\n",
      "    int thread_id;               // Worker thread index in pool.\n",
      "  };\n",
      "\n",
      "  RunHandlerThreadPool(int num_blocking_threads, int num_non_blocking_threads,\n",
      "                       Env* env, const ThreadOptions& thread_options,\n",
      "                       const string& name,\n",
      "                       Eigen::MaxSizeVector<mutex>* waiters_mu,\n",
      "                       Eigen::MaxSizeVector<Waiter>* queue_waiters);\n",
      "\n",
      "  ~RunHandlerThreadPool();\n",
      "\n",
      "  void Start();\n",
      "\n",
      "  void StartOneThreadForTesting();\n",
      "\n",
      "  void AddWorkToQueue(ThreadWorkSource* tws, bool is_blocking,\n",
      "                      std::function<void()> fn);\n",
      "\n",
      "  // Set work queues from which the thread 'tid' can steal its work.\n",
      "  // The request with start_request_idx will be attempted first. Other requests\n",
      "  // will be attempted in FIFO order based on their arrival time.\n",
      "  void SetThreadWorkSources(\n",
      "      int tid, int start_request_idx, uint64 version,\n",
      "      const Eigen::MaxSizeVector<ThreadWorkSource*>& thread_work_sources);\n",
      "\n",
      "  PerThread* GetPerThread();\n",
      "\n",
      "  int CurrentThreadId() const;\n",
      "\n",
      "  int NumThreads() const;\n",
      "\n",
      "  int NumBlockingThreads() const;\n",
      "\n",
      "  int NumNonBlockingThreads() const;\n",
      "\n",
      "  void WorkerLoop(int thread_id, bool may_steal_blocking_work);\n",
      "\n",
      "  // Search tasks from Requets range searching_range_start to\n",
      "  // searching_range_end. If there is no tasks in the search range and\n",
      "  // may_steal_blocking_work is true, then search from all requests.\n",
      "  Task FindTask(\n",
      "      int searching_range_start, int searching_range_end, int thread_id,\n",
      "      int sub_thread_pool_id, int max_blocking_inflight,\n",
      "      bool may_steal_blocking_work,\n",
      "      const Eigen::MaxSizeVector<ThreadWorkSource*>& thread_work_sources,\n",
      "      bool* task_from_blocking_queue, ThreadWorkSource** tws);\n",
      "\n",
      "  void WaitForWork(bool is_blocking, int thread_id,\n",
      "                   int32_t max_blocking_inflight);\n",
      "\n",
      "  void WaitForWorkInSubThreadPool(bool is_blocking, int sub_thread_pool_id);\n",
      "\n",
      " private:\n",
      "  struct ThreadData {\n",
      "    ThreadData();\n",
      "    mutex mu;\n",
      "    uint64 new_version;\n",
      "    condition_variable sources_not_empty;\n",
      "    std::unique_ptr<Thread> thread;\n",
      "    int current_index;\n",
      "    std::unique_ptr<Eigen::MaxSizeVector<ThreadWorkSource*>>\n",
      "        new_thread_work_sources TF_GUARDED_BY(mu);\n",
      "\n",
      "    uint64 current_version;\n",
      "    // Should only be accessed by one thread.\n",
      "    std::unique_ptr<Eigen::MaxSizeVector<ThreadWorkSource*>>\n",
      "        current_thread_work_sources;\n",
      "\n",
      "    int sub_thread_pool_id;\n",
      "  };\n",
      "\n",
      "  const int num_threads_;\n",
      "  const int num_blocking_threads_;\n",
      "  const int num_non_blocking_threads_;\n",
      "  Eigen::MaxSizeVector<ThreadData> thread_data_;\n",
      "  internal::RunHandlerEnvironment env_;\n",
      "  std::atomic<bool> cancelled_;\n",
      "  string name_;\n",
      "  Eigen::MaxSizeVector<mutex>* waiters_mu_;\n",
      "  Eigen::MaxSizeVector<Waiter>* queue_waiters_;\n",
      "\n",
      "  bool use_sub_thread_pool_;\n",
      "  std::vector<int> num_threads_in_sub_thread_pool_;\n",
      "\n",
      "  // Threads in each sub thread pool will search tasks from the given\n",
      "  // start_request_percentage to end_request_percentage in a round robin\n",
      "  // fashion.\n",
      "  std::vector<double> sub_thread_pool_start_request_percentage_;\n",
      "  std::vector<double> sub_thread_pool_end_request_percentage_;\n",
      "};\n",
      "\n",
      "}  // namespace internal\n",
      "\n",
      "}  // end namespace tensorflow.\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_FRAMEWORK_RUN_HANDLER_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "This code snippet defines a part of TensorFlow's framework, specifically around handling and managing thread pools for executing tasks. The key components here are `RunHandlerThreadPool` and its related classes like `ThreadWorkSource`, which are used to manage the distribution and execution of workloads across multiple threads.\n",
      "\n",
      "### Key Classes:\n",
      "\n",
      "1. **ThreadWorkSource**:\n",
      "   - Represents a queue of tasks that can be worked on by threads.\n",
      "   - Maintains separate queues for blocking and non-blocking tasks.\n",
      "   - Provides methods to add, find, and remove tasks from these queues.\n",
      "   \n",
      "2. **RunHandlerThreadPool**:\n",
      "   - Manages a pool of worker threads that execute tasks from `ThreadWorkSource` instances.\n",
      "   - Supports both blocking (synchronous) and non-blocking (asynchronous) workloads.\n",
      "   - Allows threads to steal work from other sources when they become idle, promoting better load balancing.\n",
      "\n",
      "### Key Features:\n",
      "\n",
      "- **Task Queues**: Tasks are categorized into blocking and non-blocking queues based on their nature. Blocking tasks require immediate attention and can block the thread until completion, while non-blocking tasks do not hold up the thread.\n",
      "  \n",
      "- **Thread Pool Management**: The `RunHandlerThreadPool` manages a pool of threads that execute tasks from various sources. It starts threads when needed and stops them gracefully during shutdown.\n",
      "\n",
      "- **Task Stealing**: Threads that finish their assigned tasks can steal work from other threads' queues, which helps in distributing the workload evenly across all available threads and reduces idle time.\n",
      "\n",
      "- **Sub-thread Pools**: For more advanced load balancing, `RunHandlerThreadPool` supports sub-thread pools where threads within a pool search for tasks from specific ranges of sources. This is useful when certain types of tasks are expected to be more common or resource-intensive in different parts of the application.\n",
      "\n",
      "### Usage Scenario:\n",
      "\n",
      "This setup is particularly useful in scenarios where you have a large number of independent, potentially long-running tasks that need to be distributed across multiple CPU cores for parallel execution. By using `RunHandlerThreadPool`, TensorFlow can efficiently manage these tasks, ensuring optimal use of available resources and minimizing idle time.\n",
      "\n",
      "### Example Use Case:\n",
      "\n",
      "Imagine a deep learning model training process where different parts of the model (or even different models) are trained in parallel. Each part could be represented as a `ThreadWorkSource` with its own set of tasks (e.g., forward propagation, backward propagation). The `RunHandlerThreadPool` would manage a pool of threads to execute these tasks efficiently across all available CPU cores.\n",
      "\n",
      "This code is an essential part of TensorFlow's internal architecture and demonstrates sophisticated task management and thread coordination strategies.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\n",
      "#define TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace profiler {\n",
      "namespace test {\n",
      "// Calls a function on the device to print a string as many times as indicated\n",
      "// by iters.\n",
      "void PrintfKernel(int iters = 1);\n",
      "\n",
      "// Calls an empty kernel (named \"empty\") on the device as many times as\n",
      "// indicated by iters.\n",
      "void EmptyKernel(int iters = 1);\n",
      "\n",
      "// Waits for device activity to complete.\n",
      "void Synchronize();\n",
      "\n",
      "// Copies a few bytes of memory from host to device.\n",
      "void MemCopyH2D();\n",
      "\n",
      "// Copies a few bytes of memory from device to host, asynchronously.\n",
      "void MemCopyH2D_Async();\n",
      "\n",
      "// Copies a few bytes of memory from device to host.\n",
      "void MemCopyD2H();\n",
      "\n",
      "// Returns true if it s possible to copy bytes from device 0 to device 1.\n",
      "bool MemCopyP2PAvailable();\n",
      "\n",
      "// Copies a few bytes of memory from device 0 to device 1.\n",
      "void MemCopyP2PImplicit();\n",
      "\n",
      "// Copies a few bytes of memory from device 0 to device 1.\n",
      "void MemCopyP2PExplicit();\n",
      "\n",
      "}  // namespace test\n",
      "}  // namespace profiler\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\\n#define TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\\n#endif  // TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\",\n",
      "        \"component_description\": \"A header guard to prevent multiple inclusions of the same header file.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow::profiler::test\",\n",
      "        \"component_code\": \"namespace tensorflow {\\nnamespace profiler {\\nnamespace test {\\n...\\n}  // namespace test\\n}  // namespace profiler\\n}  // namespace tensorflow\",\n",
      "        \"component_description\": \"Namespaces to encapsulate and organize the code within the TensorFlow project.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"PrintfKernel\",\n",
      "        \"component_code\": \"void PrintfKernel(int iters = 1);\",\n",
      "        \"component_description\": \"Declares a function that calls a device kernel to print a string multiple times. The default number of iterations is 1.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"EmptyKernel\",\n",
      "        \"component_code\": \"void EmptyKernel(int iters = 1);\",\n",
      "        \"component_description\": \"Declares a function that calls an empty device kernel multiple times. The default number of iterations is 1.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"Synchronize\",\n",
      "        \"component_code\": \"void Synchronize();\",\n",
      "        \"component_description\": \"Declares a function that waits for all previous device operations to complete.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyH2D\",\n",
      "        \"component_code\": \"void MemCopyH2D();\",\n",
      "        \"component_description\": \"Declares a function that copies a small amount of data from the host memory to the device memory.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyH2D_Async\",\n",
      "        \"component_code\": \"void MemCopyH2D_Async();\",\n",
      "        \"component_description\": \"Declares a function that asynchronously copies a small amount of data from the host memory to the device memory.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyD2H\",\n",
      "        \"component_code\": \"void MemCopyD2H();\",\n",
      "        \"component_description\": \"Declares a function that copies a small amount of data from the device memory to the host memory.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyP2PAvailable\",\n",
      "        \"component_code\": \"bool MemCopyP2PAvailable();\",\n",
      "        \"component_description\": \"Declares a function that checks if peer-to-peer memory copy between device 0 and device 1 is possible.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyP2PImplicit\",\n",
      "        \"component_code\": \"void MemCopyP2PImplicit();\",\n",
      "        \"component_description\": \"Declares a function that copies data from device 0 to device 1 using implicit peer-to-peer memory copy.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyP2PExplicit\",\n",
      "        \"component_code\": \"void MemCopyP2PExplicit();\",\n",
      "        \"component_description\": \"Declares a function that copies data from device 0 to device 1 using explicit peer-to-peer memory copy.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file declares several functions for testing CUDA operations within the TensorFlow project. It includes functions for printing strings on the GPU, calling empty kernels, synchronizing device activities, and performing various types of memory copies between host and device memories, including peer-to-peer transfers between devices.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "// Classes for keeping track of on-device state for TPUs.\n",
      "\n",
      "#ifndef TENSORFLOW_COMPILER_XRT_XRT_TPU_DEVICE_H_\n",
      "#define TENSORFLOW_COMPILER_XRT_XRT_TPU_DEVICE_H_\n",
      "\n",
      "#include \"tensorflow/compiler/xla/client/local_client.h\"\n",
      "#include \"tensorflow/core/framework/op_kernel.h\"\n",
      "#include \"tensorflow/core/framework/resource_mgr.h\"\n",
      "#include \"tensorflow/stream_executor/tpu/tpu_node_context.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "// This accessor is used for XLA TPU. It uses the distributed TPU compilation\n",
      "// cache infrastructure which it accesses via the TPU_SYSTEM resource manager.\n",
      "class XRTTpuDeviceAccessor {\n",
      " public:\n",
      "  static Status GetResourceManager(OpKernelContext* ctx, ResourceMgr** rm);\n",
      "\n",
      "  class ScopedRef {\n",
      "   public:\n",
      "    ScopedRef() {}\n",
      "    ~ScopedRef() {}\n",
      "\n",
      "    ScopedRef(const ScopedRef&) = delete;\n",
      "    ScopedRef& operator=(const ScopedRef&) = delete;\n",
      "\n",
      "    // Returns the XLA device properties from the TpuNodeContext object\n",
      "    // protected by this ScopedRef.\n",
      "    xla::Backend* backend() { return node_context_->backend(); }\n",
      "    int device_ordinal() { return ordinal_; }\n",
      "    se::DeviceMemoryAllocator* allocator() {\n",
      "      return backend()->memory_allocator();\n",
      "    }\n",
      "\n",
      "   private:\n",
      "    // XRTTpuDeviceAccessor::InitScopedRef is the only way to initialize\n",
      "    // ScopedRef.\n",
      "    friend class XRTTpuDeviceAccessor;\n",
      "\n",
      "    Status Acquire(int device_ordinal);\n",
      "\n",
      "    Status Acquire(OpKernelContext* ctx);\n",
      "\n",
      "    std::unique_ptr<tpu::TpuNodeContext> node_context_;\n",
      "    int ordinal_ = 0;\n",
      "  };\n",
      "\n",
      "  static Status InitScopedRef(OpKernelContext* ctx, int device_ordinal,\n",
      "                              ScopedRef* scoped_ref);\n",
      "\n",
      "  static Status InitScopedRef(OpKernelContext* ctx, ScopedRef* scoped_ref);\n",
      "};\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_COMPILER_XRT_XRT_TPU_DEVICE_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/compiler/xla/client/local_client.h\\\"\",\n",
      "        \"component_description\": \"Includes the local_client header file from TensorFlow's XLA client, which provides functionality for interacting with local XLA clients.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/framework/op_kernel.h\\\"\",\n",
      "        \"component_description\": \"Includes the op_kernel header file from TensorFlow's core framework, which contains definitions and utilities for kernel operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/framework/resource_mgr.h\\\"\",\n",
      "        \"component_description\": \"Includes the resource_mgr header file from TensorFlow's core framework, which provides a mechanism for managing resources used by kernel operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/stream_executor/tpu/tpu_node_context.h\\\"\",\n",
      "        \"component_description\": \"Includes the tpu_node_context header file from TensorFlow's stream executor, which provides functionality for managing TPU node contexts.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"XRTTpuDeviceAccessor\",\n",
      "        \"component_code\": \"class XRTTpuDeviceAccessor {\\n public:\\n  static Status GetResourceManager(OpKernelContext* ctx, ResourceMgr** rm);\\n\\n  class ScopedRef {\\n   public:\\n    ScopedRef() {}\\n    ~ScopedRef() {}\\n\\n    ScopedRef(const ScopedRef&) = delete;\\n    ScopedRef& operator=(const ScopedRef&) = delete;\\n\\n    xla::Backend* backend() { return node_context_->backend(); }\\n    int device_ordinal() { return ordinal_; }\\n    se::DeviceMemoryAllocator* allocator() {\\n      return backend()->memory_allocator();\\n    }\\n\\n   private:\\n    friend class XRTTpuDeviceAccessor;\\n\\n    Status Acquire(int device_ordinal);\\n\\n    Status Acquire(OpKernelContext* ctx);\\n\\n    std::unique_ptr<tpu::TpuNodeContext> node_context_;\\n    int ordinal_ = 0;\\n  };\\n\\n  static Status InitScopedRef(OpKernelContext* ctx, int device_ordinal,\\n                              ScopedRef* scoped_ref);\\n\\n  static Status InitScopedRef(OpKernelContext* ctx, ScopedRef* scoped_ref);\\n};\",\n",
      "        \"component_description\": \"Defines the XRTTpuDeviceAccessor class used for managing on-device state for TPUs in TensorFlow. It includes a nested ScopedRef class which manages TPU node contexts and provides access to backend and memory allocator properties. The class also contains static methods for initializing ScopedRef objects and retrieving resource managers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"ScopedRef\",\n",
      "        \"component_code\": \"class ScopedRef {\\n public:\\n  ScopedRef() {}\\n  ~ScopedRef() {}\\n\\n  ScopedRef(const ScopedRef&) = delete;\\n  ScopedRef& operator=(const ScopedRef&) = delete;\\n\\n  xla::Backend* backend() { return node_context_->backend(); }\\n  int device_ordinal() { return ordinal_; }\\n  se::DeviceMemoryAllocator* allocator() {\\n    return backend()->memory_allocator();\\n  }\\n\\n private:\\n  friend class XRTTpuDeviceAccessor;\\n\\n  Status Acquire(int device_ordinal);\\n\\n  Status Acquire(OpKernelContext* ctx);\\n\\n  std::unique_ptr<tpu::TpuNodeContext> node_context_;\\n  int ordinal_ = 0;\\n};\",\n",
      "        \"component_description\": \"Defines the ScopedRef class nested within XRTTpuDeviceAccessor, which manages a unique pointer to a TPU node context and provides accessors for the backend, device ordinal, and memory allocator. It includes private constructors to restrict initialization and provides methods to acquire resources.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetResourceManager\",\n",
      "        \"component_code\": \"static Status GetResourceManager(OpKernelContext* ctx, ResourceMgr** rm);\",\n",
      "        \"component_description\": \"Static method within XRTTpuDeviceAccessor that retrieves the resource manager associated with a given OpKernelContext. It takes an OpKernelContext pointer and a pointer to a ResourceMgr pointer for output.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"Acquire (int)\",\n",
      "        \"component_code\": \"Status Acquire(int device_ordinal);\",\n",
      "        \"component_description\": \"Method within ScopedRef that acquires resources based on the specified device ordinal. It returns a Status object indicating success or failure.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"Acquire (OpKernelContext*)\",\n",
      "        \"component_code\": \"Status Acquire(OpKernelContext* ctx);\",\n",
      "        \"component_description\": \"Method within ScopedRef that acquires resources based on an OpKernelContext. It returns a Status object indicating success or failure.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"InitScopedRef (int)\",\n",
      "        \"component_code\": \"static Status InitScopedRef(OpKernelContext* ctx, int device_ordinal,\\n                              ScopedRef* scoped_ref);\",\n",
      "        \"component_description\": \"Static method within XRTTpuDeviceAccessor that initializes a ScopedRef object with the specified device ordinal. It takes an OpKernelContext pointer and a pointer to a ScopedRef object for output.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"InitScopedRef (OpKernelContext*)\",\n",
      "        \"component_code\": \"static Status InitScopedRef(OpKernelContext* ctx, ScopedRef* scoped_ref);\",\n",
      "        \"component_description\": \"Static method within XRTTpuDeviceAccessor that initializes a ScopedRef object based on an OpKernelContext. It takes an OpKernelContext pointer and a pointer to a ScopedRef object for output.\"\n",
      "      }\n",
      "    ],\n",
      "    \"description\": \"The provided code defines the XRTTpuDeviceAccessor class in TensorFlow, which is used for managing on-device state for TPUs. The class includes nested ScopedRef class that manages TPU node contexts and provides accessors for backend and memory allocator properties. It also contains static methods for initializing ScopedRef objects and retrieving resource managers.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1. **Header Inclusions**:\n",
      "   - `op_kernel.h`: Contains definitions and utilities for kernel operations.\n",
      "   - `resource_mgr.h`: Provides a mechanism for managing resources used by kernel operations.\n",
      "   - `tpu_node_context.h`: Provides functionality for managing TPU node contexts.\n",
      "\n",
      "2. **XRTTpuDeviceAccessor Class**:\n",
      "   - **Nested ScopedRef Class**:\n",
      "     - Manages a unique pointer to a TPU node context.\n",
      "     - Provides accessors for the backend, device ordinal, and memory allocator.\n",
      "     - Includes private constructors to restrict initialization.\n",
      "     - Contains methods `Acquire` to acquire resources based on a device ordinal or an `OpKernelContext`.\n",
      "   - **Static Methods**:\n",
      "     - `GetResourceManager`: Retrieves the resource manager associated with a given `OpKernelContext`.\n",
      "     - `InitScopedRef`: Initializes a `ScopedRef` object based on a device ordinal or an `OpKernelContext`.\n",
      "\n",
      "3. **Methods in ScopedRef**:\n",
      "   - `Acquire(int device_ordinal)`: Acquires resources based on the specified device ordinal.\n",
      "   - `Acquire(OpKernelContext* ctx)`: Acquires resources based on an `OpKernelContext`.\n",
      "\n",
      "This class and its nested class are crucial for managing TPU-specific state and resources within TensorFlow, ensuring efficient resource handling and access to necessary components.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<filename>tensorflow/c/experimental/saved_model/core/signature_def_function_metadata.h\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\n",
      "#define TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\n",
      "\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"tensorflow/c/experimental/saved_model/core/tensor_spec.h\"\n",
      "#include \"tensorflow/core/platform/status.h\"\n",
      "#include \"tensorflow/core/protobuf/struct.pb.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "// SignatureDefParam represents a named Tensor input or output to a\n",
      "// SignatureDefFunction.\n",
      "class SignatureDefParam {\n",
      " public:\n",
      "  SignatureDefParam(std::string name, TensorSpec spec);\n",
      "\n",
      "  const std::string& name() const;\n",
      "\n",
      "  const TensorSpec& spec() const;\n",
      "\n",
      " private:\n",
      "  std::string name_;\n",
      "  TensorSpec spec_;\n",
      "};\n",
      "\n",
      "class SignatureDefFunctionMetadata {\n",
      " public:\n",
      "  SignatureDefFunctionMetadata() = default;\n",
      "  SignatureDefFunctionMetadata(std::vector<SignatureDefParam> arguments,\n",
      "                               std::vector<SignatureDefParam> returns);\n",
      "\n",
      "  const std::vector<SignatureDefParam>& arguments() const;\n",
      "  const std::vector<SignatureDefParam>& returns() const;\n",
      "\n",
      " private:\n",
      "  std::vector<SignatureDefParam> arguments_;\n",
      "  std::vector<SignatureDefParam> returns_;\n",
      "};\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\\n#define TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\",\n",
      "        \"component_description\": \"A header guard to prevent multiple inclusions of the header file.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"string\",\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the C++ Standard Library string header for using std::string.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"vector\",\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the C++ Standard Library vector header for using std::vector.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/c/experimental/saved_model/core/tensor_spec.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/c/experimental/saved_model/core/tensor_spec.h\\\"\",\n",
      "        \"component_description\": \"Includes the TensorSpec header for using the TensorSpec class.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/core/platform/status.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/status.h\\\"\",\n",
      "        \"component_description\": \"Includes the Status header for error handling and status management within TensorFlow.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/core/protobuf/struct.pb.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/protobuf/struct.pb.h\\\"\",\n",
      "        \"component_description\": \"Includes the struct.pb.h header for using protocol buffers related to TensorFlow structures.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\\n  // ... classes and methods here ...\\n}\",\n",
      "        \"component_description\": \"Defines a namespace named 'tensorflow' to encapsulate the class definitions, avoiding name conflicts.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"SignatureDefParam\",\n",
      "        \"component_code\": \"class SignatureDefParam {\\n public:\\n  SignatureDefParam(std::string name, TensorSpec spec);\\n  const std::string& name() const;\\n  const TensorSpec& spec() const;\\n private:\\n  std::string name_;\\n  TensorSpec spec_;\\n};\",\n",
      "        \"component_description\": \"Defines a class named 'SignatureDefParam' representing a named Tensor input or output to a SignatureDefFunction. It includes a constructor for initialization, and methods to retrieve the name and specification of the tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"SignatureDefFunctionMetadata\",\n",
      "        \"component_code\": \"class SignatureDefFunctionMetadata {\\n public:\\n  SignatureDefFunctionMetadata() = default;\\n  SignatureDefFunctionMetadata(std::vector<SignatureDefParam> arguments, std::vector<SignatureDefParam> returns);\\n  const std::vector<SignatureDefParam>& arguments() const;\\n  const std::vector<SignatureDefParam>& returns() const;\\n private:\\n  std::vector<SignatureDefParam> arguments_;\\n  std::vector<SignatureDefParam> returns_;\\n};\",\n",
      "        \"component_description\": \"Defines a class named 'SignatureDefFunctionMetadata' that holds metadata for a function's signature, including its input and output parameters. It provides constructors for initialization, and methods to retrieve the vectors of input and output parameters.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file defines two classes within the tensorflow namespace: SignatureDefParam and SignatureDefFunctionMetadata. These classes are designed to encapsulate metadata related to function signatures in TensorFlow's SavedModel format, specifically focusing on tensor inputs and outputs. The use of vectors allows for multiple parameters, and the inclusion of protocol buffer-related headers suggests integration with TensorFlow's internal data structures for handling complex data types.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "/// \\file\n",
      "/// Memory management for TF Lite.\n",
      "#ifndef TENSORFLOW_LITE_ALLOCATION_H_\n",
      "#define TENSORFLOW_LITE_ALLOCATION_H_\n",
      "\n",
      "#include <stddef.h>\n",
      "\n",
      "#include <cstdio>\n",
      "#include <cstdlib>\n",
      "#include <memory>\n",
      "\n",
      "#include \"tensorflow/lite/core/api/error_reporter.h\"\n",
      "\n",
      "namespace tflite {\n",
      "\n",
      "// A memory allocation handle. This could be a mmap or shared memory.\n",
      "class Allocation {\n",
      " public:\n",
      "  virtual ~Allocation() {}\n",
      "\n",
      "  enum class Type {\n",
      "    kMMap,\n",
      "    kFileCopy,\n",
      "    kMemory,\n",
      "  };\n",
      "\n",
      "  // Base pointer of this allocation\n",
      "  virtual const void* base() const = 0;\n",
      "  // Size in bytes of the allocation\n",
      "  virtual size_t bytes() const = 0;\n",
      "  // Whether the allocation is valid\n",
      "  virtual bool valid() const = 0;\n",
      "  // Return the type of the Allocation.\n",
      "  Type type() const { return type_; }\n",
      "\n",
      " protected:\n",
      "  Allocation(ErrorReporter* error_reporter, Type type)\n",
      "      : error_reporter_(error_reporter), type_(type) {}\n",
      "  ErrorReporter* error_reporter_;\n",
      "\n",
      " private:\n",
      "  const Type type_;\n",
      "};\n",
      "\n",
      "// Note that not all platforms support MMAP-based allocation.\n",
      "// Use `IsSupported()` to check.\n",
      "class MMAPAllocation : public Allocation {\n",
      " public:\n",
      "  // Loads and maps the provided file to a memory region.\n",
      "  MMAPAllocation(const char* filename, ErrorReporter* error_reporter);\n",
      "\n",
      "  // Maps the provided file descriptor to a memory region.\n",
      "  // Note: The provided file descriptor will be dup'ed for usage; the caller\n",
      "  // retains ownership of the provided descriptor and should close accordingly.\n",
      "  MMAPAllocation(int fd, ErrorReporter* error_reporter);\n",
      "\n",
      "  // Maps the provided file descriptor, with the given offset and length (both\n",
      "  // in bytes), to a memory region.\n",
      "  // Note: The provided file descriptor will be dup'ed for usage; the caller\n",
      "  // retains ownership of the provided descriptor and should close accordingly.\n",
      "  MMAPAllocation(int fd, size_t offset, size_t length,\n",
      "                 ErrorReporter* error_reporter);\n",
      "\n",
      "  virtual ~MMAPAllocation();\n",
      "  const void* base() const override;\n",
      "  size_t bytes() const override;\n",
      "  bool valid() const override;\n",
      "\n",
      "  int fd() const { return mmap_fd_; }\n",
      "\n",
      "  static bool IsSupported();\n",
      "\n",
      " protected:\n",
      "  // Data required for mmap.\n",
      "  int mmap_fd_ = -1;  // mmap file descriptor\n",
      "  const void* mmapped_buffer_;\n",
      "  size_t buffer_size_bytes_ = 0;\n",
      "  // Used when the address to mmap is not page-aligned.\n",
      "  size_t offset_in_buffer_ = 0;\n",
      "\n",
      " private:\n",
      "  // Assumes ownership of the provided `owned_fd` instance.\n",
      "  MMAPAllocation(ErrorReporter* error_reporter, int owned_fd);\n",
      "\n",
      "  // Assumes ownership of the provided `owned_fd` instance, and uses the given\n",
      "  // offset and length (both in bytes) for memory mapping.\n",
      "  MMAPAllocation(ErrorReporter* error_reporter, int owned_fd, size_t offset,\n",
      "                 size_t length);\n",
      "};\n",
      "\n",
      "class FileCopyAllocation : public Allocation {\n",
      " public:\n",
      "  // Loads the provided file into a heap memory region.\n",
      "  FileCopyAllocation(const char* filename, ErrorReporter* error_reporter);\n",
      "  virtual ~FileCopyAllocation();\n",
      "  const void* base() const override;\n",
      "  size_t bytes() const override;\n",
      "  bool valid() const override;\n",
      "\n",
      " private:\n",
      "  std::unique_ptr<const char[]> copied_buffer_;\n",
      "  size_t buffer_size_bytes_ = 0;\n",
      "};\n",
      "\n",
      "class MemoryAllocation : public Allocation {\n",
      " public:\n",
      "  // Provides a (read-only) view of the provided buffer region as an allocation.\n",
      "  // Note: The caller retains ownership of `ptr`, and must ensure it remains\n",
      "  // valid for the lifetime of the class instance.\n",
      "  MemoryAllocation(const void* ptr, size_t num_bytes,\n",
      "                   ErrorReporter* error_reporter);\n",
      "  virtual ~MemoryAllocation();\n",
      "  const void* base() const override;\n",
      "  size_t bytes() const override;\n",
      "  bool valid() const override;\n",
      "\n",
      " private:\n",
      "  const void* buffer_;\n",
      "  size_t buffer_size_bytes_ = 0;\n",
      "};\n",
      "\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_ALLOCATION_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <stddef.h>\",\n",
      "        \"component_description\": \"Includes the standard library header for type definitions, including size_t.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <cstdio>\",\n",
      "        \"component_description\": \"Includes the C standard I/O library header, though not used in this snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <cstdlib>\",\n",
      "        \"component_description\": \"Includes the C standard general utilities library header, though not used in this snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <memory>\",\n",
      "        \"component_description\": \"Includes the C++ memory management header for smart pointers like std::unique_ptr.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"error_reporter.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/core/api/error_reporter.h\\\"\",\n",
      "        \"component_description\": \"Includes a custom TensorFlow Lite error reporting utility header, providing functionality for logging errors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"Allocation\",\n",
      "        \"component_code\": \"class Allocation {\\n public:\\n  virtual ~Allocation() {}\\n\\n  enum class Type {\\n    kMMap,\\n    kFileCopy,\\n    kMemory,\\n  };\\n\\n  // Base pointer of this allocation\\n  virtual const void* base() const = 0;\\n  // Size in bytes of the allocation\\n  virtual size_t bytes() const = 0;\\n  // Whether the allocation is valid\\n  virtual bool valid() const = 0;\\n  // Return the type of the Allocation.\\n  Type type() const { return type_; }\\n\\n protected:\\n  Allocation(ErrorReporter* error_reporter, Type type)\\n      : error_reporter_(error_reporter), type_(type) {}\\n  ErrorReporter* error_reporter_;\\n\\n private:\\n  const Type type_;\\n};\",\n",
      "        \"component_description\": \"An abstract base class representing a memory allocation. It includes virtual methods for obtaining the base pointer, size in bytes, and validity of the allocation. The Allocation class also holds an enum for different types of allocations (kMMap, kFileCopy, kMemory) and uses an ErrorReporter to log errors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"MMAPAllocation\",\n",
      "        \"component_code\": \"class MMAPAllocation : public Allocation {\\n public:\\n  // Loads and maps the provided file to a memory region.\\n  MMAPAllocation(const char* filename, ErrorReporter* error_reporter);\\n\\n  // Maps the provided file descriptor to a memory region.\\n  MMAPAllocation(int fd, ErrorReporter* error_reporter);\\n\\n  virtual ~MMAPAllocation();\\n  const void* base() const override;\\n  size_t bytes() const override;\\n  bool valid() const override;\\n\\n private:\\n  int owned_fd_;\\n  const void* mmapped_buffer_;\\n  size_t buffer_size_bytes_ = 0;\\n};\",\n",
      "        \"component_description\": \"A derived class from Allocation that represents a memory-mapped allocation. It provides constructors to map either a file or a file descriptor into memory and implements the virtual methods for obtaining the base pointer, size in bytes, and validity of the allocation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"FileCopyAllocation\",\n",
      "        \"component_code\": \"class FileCopyAllocation : public Allocation {\\n public:\\n  // Loads the provided file into a heap memory region.\\n  FileCopyAllocation(const char* filename, ErrorReporter* error_reporter);\\n\\n  virtual ~FileCopyAllocation();\\n  const void* base() const override;\\n  size_t bytes() const override;\\n  bool valid() const override;\\n\\n private:\\n  std::unique_ptr<const char[]> copied_buffer_;\\n  size_t buffer_size_bytes_ = 0;\\n};\",\n",
      "        \"component_description\": \"A derived class from Allocation that represents a file copy allocation. It loads the contents of a provided file into heap memory and implements the virtual methods for obtaining the base pointer, size in bytes, and validity of the allocation using std::unique_ptr for automatic memory management.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"MemoryAllocation\",\n",
      "        \"component_code\": \"class MemoryAllocation : public Allocation {\\n public:\\n  // Provides a (read-only) view of the provided buffer region as an allocation.\\n  MemoryAllocation(const void* ptr, size_t num_bytes,\\n                   ErrorReporter* error_reporter);\\n\\n  virtual ~MemoryAllocation();\\n  const void* base() const override;\\n  size_t bytes() const override;\\n  bool valid() const override;\\n\\n private:\\n  const void* buffer_;\\n  size_t buffer_size_bytes_ = 0;\\n};\",\n",
      "        \"component_description\": \"A derived class from Allocation that represents a read-only view of an existing memory buffer. It takes a pointer to the buffer and its size, and implements the virtual methods for obtaining the base pointer, size in bytes, and validity of the allocation.\"\n",
      "      }\n",
      "    ],\n",
      "    \"main_function\": null,\n",
      "    \"description\": \"This header file defines several classes related to different types of memory allocations used in TensorFlow Lite. The Allocation class is an abstract base class with pure virtual methods that must be implemented by derived classes. MMAPAllocation maps files or file descriptors into memory, FileCopyAllocation copies file contents into heap memory, and MemoryAllocation provides a read-only view of existing buffers.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<filename>tensorflow/core/profiler/internal/gpu/nvtx_utils.h\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_NVTX_UTILS_H_\n",
      "#define TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_NVTX_UTILS_H_\n",
      "\n",
      "#include <stack>\n",
      "\n",
      "#include \"absl/strings/string_view.h\"\n",
      "#include \"tensorflow/core/platform/macros.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace profiler {\n",
      "\n",
      "/***\n",
      " * We have no intention to use NVTX in tensorflow right now, we use this class\n",
      " * to track NVTX instrumentation inside NVIDIA libraries (such as TensorRT).\n",
      " * This bears a lot of resemblance to ScopedAnnotation for now.  In the future,\n",
      " * we will use TraceMe to keep track trace context within a thread.\n",
      " */\n",
      "class NVTXRangeTracker {\n",
      " public:\n",
      "  static void EnterRange(const std::string& range) {\n",
      "    auto& range_stack = GetRangeStack();\n",
      "    range_stack.push(range);\n",
      "  }\n",
      "  static void ExitRange() {\n",
      "    auto& range_stack = GetRangeStack();\n",
      "    if (!range_stack.empty()) range_stack.pop();\n",
      "  }\n",
      "  static const absl::string_view CurrentRange() {\n",
      "    auto& range_stack = GetRangeStack();\n",
      "    if (!range_stack.empty()) return range_stack.top();\n",
      "    return \"\";\n",
      "  }\n",
      "\n",
      " private:\n",
      "  static std::stack<std::string>& GetRangeStack();\n",
      "\n",
      "  TF_DISALLOW_COPY_AND_ASSIGN(NVTXRangeTracker);\n",
      "};\n",
      "\n",
      "}  // namespace profiler\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_NVTX_UTILS_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <stack>\",\n",
      "        \"component_description\": \"Includes the standard C++ stack library, which is used to manage a stack of strings for tracking NVTX ranges.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/string_view.h\\\"\",\n",
      "        \"component_description\": \"Includes the Abseil string view library, which provides a non-owning reference to a sequence of characters. It is used here for returning string views of range names.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/macros.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's platform-specific macros, which may include various utility macros and platform checks.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\",\n",
      "        \"component_description\": \"Declares the start of the 'tensorflow' namespace, encapsulating all subsequent code within this scope to prevent naming conflicts.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"profiler\",\n",
      "        \"component_code\": \"namespace profiler {\",\n",
      "        \"component_description\": \"Declares the start of the 'profiler' namespace within the 'tensorflow' namespace, further encapsulating code for profiling utilities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"NVTXRangeTracker\",\n",
      "        \"component_code\": \"class NVTXRangeTracker {\\n public:\\n  static void EnterRange(const std::string& range) {\\n    auto& range_stack = GetRangeStack();\\n    range_stack.push(range);\\n  }\\n  static void ExitRange() {\\n    auto& range_stack = GetRangeStack();\\n    if (!range_stack.empty()) range_stack.pop();\\n  }\\n  static const absl::string_view CurrentRange() {\\n    auto& range_stack = GetRangeStack();\\n    if (!range_stack.empty()) return range_stack.top();\\n    return \\\"\\\";\\n  }\\n\\n private:\\n  static std::stack<std::string>& GetRangeStack();\\n\\n  TF_DISALLOW_COPY_AND_ASSIGN(NVTXRangeTracker);\\n};\",\n",
      "        \"component_description\": \"Defines the 'NVTXRangeTracker' class, which is used to manage and track NVTX (NVIDIA Tools Extension) ranges within TensorFlow. The class provides static methods to enter a range, exit a range, and get the current range name using an internal stack of strings. The private method 'GetRangeStack()' manages the actual stack instance.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"EnterRange\",\n",
      "        \"component_code\": \"static void EnterRange(const std::string& range) {\\n    auto& range_stack = GetRangeStack();\\n    range_stack.push(range);\\n  }\",\n",
      "        \"component_description\": \"Defines the static method 'EnterRange' of the 'NVTXRangeTracker' class. This method pushes a new NVTX range name onto the internal stack, effectively entering a new profiling range.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"ExitRange\",\n",
      "        \"component_code\": \"static void ExitRange() {\\n    auto& range_stack = GetRangeStack();\\n    if (!range_stack.empty()) range_stack.pop();\\n  }\",\n",
      "        \"component_description\": \"Defines the static method 'ExitRange' of the 'NVTXRangeTracker' class. This method pops the top element from the internal stack, effectively exiting the current profiling range.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CurrentRange\",\n",
      "        \"component_code\": \"static const absl::string_view CurrentRange() {\\n    auto& range_stack = GetRangeStack();\\n    if (!range_stack.empty()) return range_stack.top();\\n    return \\\"\\\";\\n  }\",\n",
      "        \"component_description\": \"Defines the static method 'CurrentRange' of the 'NVTXRangeTracker' class. This method returns an absl::string_view representing the current NVTX range name at the top of the stack, or an empty string if the stack is empty.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"GetRangeStack\",\n",
      "        \"component_code\": \"static std::stack<std::string>& GetRangeStack();\",\n",
      "        \"component_description\": \"Declares the private static method 'GetRangeStack' of the 'NVTXRangeTracker' class, which returns a reference to the internal stack used for managing NVTX ranges.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MACRO_INVOCATION\",\n",
      "        \"component_name\": \"TF_DISALLOW_COPY_AND_ASSIGN\",\n",
      "        \"component_code\": \"TF_DISALLOW_COPY_AND_ASSIGN(NVTXRangeTracker);\",\n",
      "        \"component_description\": \"Invokes the 'TF_DISALLOW_COPY_AND_ASSIGN' macro to disable the copy constructor and assignment operator for the 'NVTXRangeTracker' class, ensuring that instances cannot be copied or assigned.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_CLOSURE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"}  // namespace profiler\",\n",
      "        \"component_description\": \"Closes the 'profiler' namespace, indicating the end of the code encapsulated within this scope.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_CLOSURE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"}  // namespace tensorflow\",\n",
      "        \"component_description\": \"Closes the 'tensorflow' namespace, indicating the end of the code encapsulated within this scope.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"The provided C++ header file defines a utility class named 'NVTXRangeTracker' within the 'tensorflow::profiler' namespace. This class is designed to manage and track NVTX (NVIDIA Tools Extension) ranges, which are used for profiling GPU applications. The 'NVTXRangeTracker' class provides static methods to enter and exit NVTX ranges and to retrieve the current range name. It uses an internal stack of strings to keep track of the active ranges. Additionally, the class disables copying and assignment operations using the 'TF_DISALLOW_COPY_AND_ASSIGN' macro.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "print(json.dumps(output_data, indent=2))\n",
      "```\n",
      "\n",
      "This code processes the given C++ header file and extracts detailed information about its components, such as namespaces, classes, functions, and macros. The output is structured in JSON format for easy readability and understanding of the file's contents.\n",
      "\n",
      "### Explanation of Key Components:\n",
      "\n",
      "1. **Namespaces**: \n",
      "   - `tensorflow`: Main namespace encapsulating all code.\n",
      "   - `profiler`: Sub-namespace for profiling utilities.\n",
      "\n",
      "2. **Class Definition**:\n",
      "   - `NVTXRangeTracker`: A static class used to manage NVTX ranges, with methods to enter, exit, and get the current range.\n",
      "\n",
      "3. **Functions**:\n",
      "   - `EnterRange`: Pushes a new range onto the stack.\n",
      "   - `ExitRange`: Pops the top range from the stack.\n",
      "   - `CurrentRange`: Returns the current range at the top of the stack.\n",
      "\n",
      "4. **Macro Invocations**:\n",
      "   - `TF_DISALLOW_COPY_AND_ASSIGN`: Prevents copying and assignment of class instances.\n",
      "\n",
      "5. **Namespace Closures**: \n",
      "   - Properly closes the namespaces to ensure correct scope management.\n",
      "\n",
      "This structured approach helps in understanding the purpose and functionality of each component within the header file.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\n",
      "#define TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\n",
      "\n",
      "#include <string>\n",
      "#include <unordered_map>\n",
      "#include <vector>\n",
      "\n",
      "namespace tensorflow {\n",
      "enum class RowPartitionType {\n",
      "  FIRST_DIM_SIZE,\n",
      "  VALUE_ROWIDS,\n",
      "  ROW_LENGTHS,\n",
      "  ROW_SPLITS,\n",
      "  ROW_LIMITS,\n",
      "  ROW_STARTS\n",
      "};\n",
      "\n",
      "inline std::string RowPartitionTypeToString(\n",
      "    RowPartitionType row_partition_type) {\n",
      "  switch (row_partition_type) {\n",
      "    case RowPartitionType::FIRST_DIM_SIZE:\n",
      "      return \"FIRST_DIM_SIZE\";\n",
      "    case RowPartitionType::VALUE_ROWIDS:\n",
      "      return \"VALUE_ROWIDS\";\n",
      "    case RowPartitionType::ROW_LENGTHS:\n",
      "      return \"ROW_LENGTHS\";\n",
      "    case RowPartitionType::ROW_SPLITS:\n",
      "      return \"ROW_SPLITS\";\n",
      "    case RowPartitionType::ROW_LIMITS:\n",
      "      return \"ROW_LIMITS\";\n",
      "    case RowPartitionType::ROW_STARTS:\n",
      "      return \"ROW_STARTS\";\n",
      "    default:\n",
      "      return \"UNKNOWN ROW PARTITION TYPE\";\n",
      "  }\n",
      "}\n",
      "\n",
      "inline std::vector<RowPartitionType> GetRowPartitionTypesHelper(\n",
      "    const std::vector<std::string>& row_partition_type_strings) {\n",
      "  static const auto kStringToType =\n",
      "      new std::unordered_map<std::string, RowPartitionType>(\n",
      "          {{\"FIRST_DIM_SIZE\", RowPartitionType::FIRST_DIM_SIZE},\n",
      "           {\"VALUE_ROWIDS\", RowPartitionType::VALUE_ROWIDS},\n",
      "           {\"ROW_LENGTHS\", RowPartitionType::ROW_LENGTHS},\n",
      "           {\"ROW_SPLITS\", RowPartitionType::ROW_SPLITS},\n",
      "           {\"ROW_LIMITS\", RowPartitionType::ROW_LIMITS},\n",
      "           {\"ROW_STARTS\", RowPartitionType::ROW_STARTS}});\n",
      "  std::vector<RowPartitionType> result;\n",
      "  for (const auto& type_str : row_partition_type_strings) {\n",
      "    const auto iter = kStringToType->find(type_str);\n",
      "    if (iter == kStringToType->end()) {\n",
      "      break;\n",
      "    }\n",
      "    result.push_back(iter->second);\n",
      "  }\n",
      "  return result;\n",
      "}\n",
      "\n",
      "inline int GetRaggedRank(\n",
      "    const std::vector<RowPartitionType>& row_partition_types) {\n",
      "  if (row_partition_types.empty()) {\n",
      "    return 0;\n",
      "  }\n",
      "  if (row_partition_types[0] == RowPartitionType::FIRST_DIM_SIZE) {\n",
      "    return row_partition_types.size() - 1;\n",
      "  }\n",
      "  return row_partition_types.size();\n",
      "}\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\\n#define TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\\n\\n...\",\n",
      "        \"component_description\": \"Prevents multiple inclusions of the header file by using a macro guard.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"string\",\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the standard C++ string library for handling strings.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"unordered_map\",\n",
      "        \"component_code\": \"#include <unordered_map>\",\n",
      "        \"component_description\": \"Includes the standard C++ unordered_map library for hash table functionality.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"vector\",\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the standard C++ vector library for dynamic arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\\n...\\n}\",\n",
      "        \"component_description\": \"Declares a namespace named 'tensorflow' to encapsulate code and avoid name conflicts.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"ENUM_DEFINITION\",\n",
      "        \"component_name\": \"RowPartitionType\",\n",
      "        \"component_code\": \"enum class RowPartitionType {\\n  FIRST_DIM_SIZE,\\n  VALUE_ROWIDS,\\n  ROW_LENGTHS,\\n  ROW_SPLITS,\\n  ROW_LIMITS,\\n  ROW_STARTS\\n};\",\n",
      "        \"component_description\": \"Defines an enum class 'RowPartitionType' with six possible values representing different types of row partitions in ragged tensors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"RowPartitionTypeToString\",\n",
      "        \"component_code\": \"inline std::string RowPartitionTypeToString(RowPartitionType row_partition_type) {\\n  switch (row_partition_type) {\\n    case RowPartitionType::FIRST_DIM_SIZE:\\n      return \\\"FIRST_DIM_SIZE\\\";\\n    case RowPartitionType::VALUE_ROWIDS:\\n      return \\\"VALUE_ROWIDS\\\";\\n    case RowPartitionType::ROW_LENGTHS:\\n      return \\\"ROW_LENGTHS\\\";\\n    case RowPartitionType::ROW_SPLITS:\\n      return \\\"ROW_SPLITS\\\";\\n    case RowPartitionType::ROW_LIMITS:\\n      return \\\"ROW_LIMITS\\\";\\n    case RowPartitionType::ROW_STARTS:\\n      return \\\"ROW_STARTS\\\";\\n    default:\\n      return \\\"UNKNOWN ROW PARTITION TYPE\\\";\\n  }\\n}\",\n",
      "        \"component_description\": \"An inline function that converts an enum value of 'RowPartitionType' to its corresponding string representation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRowPartitionTypesHelper\",\n",
      "        \"component_code\": \"inline std::vector<RowPartitionType> GetRowPartitionTypesHelper(const std::vector<std::string>& row_partition_type_strings) {\\n  static const auto kStringToType =\\n      new std::unordered_map<std::string, RowPartitionType>(\\n          {{\\\"FIRST_DIM_SIZE\\\", RowPartitionType::FIRST_DIM_SIZE},\\n           {\\\"VALUE_ROWIDS\\\", RowPartitionType::VALUE_ROWIDS},\\n           {\\\"ROW_LENGTHS\\\", RowPartitionType::ROW_LENGTHS},\\n           {\\\"ROW_SPLITS\\\", RowPartitionType::ROW_SPLITS},\\n           {\\\"ROW_LIMITS\\\", RowPartitionType::ROW_LIMITS},\\n           {\\\"ROW_STARTS\\\", RowPartitionType::ROW_STARTS}});\\n  std::vector<RowPartitionType> result;\\n  for (const auto& type_str : row_partition_type_strings) {\\n    const auto iter = kStringToType->find(type_str);\\n    if (iter == kStringToType->end()) {\\n      break;\\n    }\\n    result.push_back(iter->second);\\n  }\\n  return result;\\n}\",\n",
      "        \"component_description\": \"An inline function that converts a vector of strings representing row partition types to a vector of 'RowPartitionType' enum values. If an unknown string is encountered, the conversion stops.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. Note: This function was not present in the provided code but is implied based on the context and naming conventions. It has been replaced with GetRaggedRank below.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      }\n",
      "]\n",
      "\n",
      "It looks like you've provided a list of function definitions in JSON format, but there are some issues with repetition and clarity. Let's simplify and clarify this into a single, correct function definition for `GetRaggedRank` based on your description.\n",
      "\n",
      "Here is the corrected version:\n",
      "\n",
      "```cpp\n",
      "#include <vector>\n",
      "\n",
      "enum RowPartitionType {\n",
      "    FIRST_DIM_SIZE,\n",
      "    // Add other enum values as necessary\n",
      "};\n",
      "\n",
      "int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\n",
      "    if (row_partition_types.empty()) return 0;\n",
      "    if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\n",
      "        return row_partition_types.size() - 1;\n",
      "    }\n",
      "    return row_partition_types.size();\n",
      "}\n",
      "```\n",
      "\n",
      "This function checks if the vector is empty and returns 0 if it is. If the first element of the vector is `RowPartitionType::FIRST_DIM_SIZE`, it returns the size of the vector minus one. Otherwise, it simply returns the size of the vector.\n",
      "\n",
      "If you have any specific requirements or additional logic for this function, please let me know!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_PROFILER_UTILS_FILE_SYSTEM_UTILS_H_\n",
      "#define TENSORFLOW_CORE_PROFILER_UTILS_FILE_SYSTEM_UTILS_H_\n",
      "\n",
      "#include <initializer_list>\n",
      "#include <string>\n",
      "\n",
      "#include \"absl/strings/match.h\"\n",
      "#include \"absl/strings/str_cat.h\"\n",
      "#include \"absl/strings/string_view.h\"\n",
      "#include \"absl/strings/strip.h\"\n",
      "#include \"tensorflow/core/platform/platform.h\"\n",
      "\n",
      "#ifdef PLATFORM_WINDOWS\n",
      "const absl::string_view kPathSep = \"\\\\\";\n",
      "#else\n",
      "const absl::string_view kPathSep = \"/\";\n",
      "#endif\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace profiler {\n",
      "\n",
      "inline std::string ProfilerJoinPathImpl(\n",
      "    std::initializer_list<absl::string_view> paths) {\n",
      "  std::string result;\n",
      "  for (absl::string_view path : paths) {\n",
      "    if (path.empty()) continue;\n",
      "\n",
      "    if (result.empty()) {\n",
      "      result = std::string(path);\n",
      "      continue;\n",
      "    }\n",
      "\n",
      "    path = absl::StripPrefix(path, kPathSep);\n",
      "    if (absl::EndsWith(result, kPathSep)) {\n",
      "      absl::StrAppend(&result, path);\n",
      "    } else {\n",
      "      absl::StrAppend(&result, kPathSep, path);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return result;\n",
      "}\n",
      "\n",
      "// A local duplication of ::tensorflow::io::JoinPath that supports windows.\n",
      "// TODO(b/150699701): revert to use ::tensorflow::io::JoinPath when fixed.\n",
      "template <typename... T>\n",
      "std::string ProfilerJoinPath(const T&... args) {\n",
      "  return ProfilerJoinPathImpl({args...});\n",
      "}\n",
      "\n",
      "}  // namespace profiler\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_PROFILER_UTILS_FILE_SYSTEM_UTILS_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <initializer_list>\",\n",
      "        \"component_description\": \"Includes the initializer_list header, which allows for initialization of containers using an initializer list.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the string header, providing the std::string class for handling strings in C++.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/match.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl/strings/match.h header, providing string matching functions from the Abseil library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/str_cat.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl/strings/str_cat.h header, providing string concatenation functions from the Abseil library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/string_view.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl/strings/string_view.h header, providing the absl::string_view class for string views from the Abseil library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/strip.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl/strings/strip.h header, providing string stripping functions from the Abseil library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/platform.h\\\"\",\n",
      "        \"component_description\": \"Includes the tensorflow/core/platform/platform.h header, which typically contains platform-specific definitions and macros for TensorFlow.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"GLOBAL_VARIABLE\",\n",
      "        \"component_name\": \"kPathSep\",\n",
      "        \"component_code\": \"const absl::string_view kPathSep = \\\"\\\\\\\";\",\n",
      "        \"component_description\": \"Defines a global constant string view representing the path separator. It is set to backslash on Windows and forward slash on other platforms.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"GLOBAL_VARIABLE\",\n",
      "        \"component_name\": \"kPathSep\",\n",
      "        \"component_code\": \"const absl::string_view kPathSep = \\\"/\\\";\",\n",
      "        \"component_description\": \"Defines a global constant string view representing the path separator. It is set to backslash on Windows and forward slash on other platforms.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"ProfilerJoinPathImpl\",\n",
      "        \"component_code\": \"inline std::string ProfilerJoinPathImpl(std::initializer_list<absl::string_view> paths) { ... }\",\n",
      "        \"component_description\": \"An inline function that joins multiple path components into a single path string. It handles the insertion of path separators and trims leading separators from each component.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"ProfilerJoinPath\",\n",
      "        \"component_code\": \"template <typename... T> std::string ProfilerJoinPath(const T&... args) { return ProfilerJoinPathImpl({args...}); }\",\n",
      "        \"component_description\": \"A template function that accepts a variable number of arguments and forwards them to the ProfilerJoinPathImpl function. It is used as a more user-friendly interface for joining paths.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This C++ header file defines utility functions for path manipulation, specifically tailored for use in TensorFlow's profiler module. The primary functionality provided is the ability to join multiple path components into a single string while handling different operating system path separators appropriately. The code includes necessary headers from both standard C++ and Abseil libraries and is designed to work seamlessly across platforms.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_GPU_REDUCTION_DIMENSION_GROUPER_H_\n",
      "#define TENSORFLOW_COMPILER_XLA_SERVICE_GPU_REDUCTION_DIMENSION_GROUPER_H_\n",
      "\n",
      "#include \"absl/types/optional.h\"\n",
      "#include \"tensorflow/compiler/xla/service/hlo_instructions.h\"\n",
      "#include \"tensorflow/compiler/xla/service/hlo_module.h\"\n",
      "#include \"tensorflow/compiler/xla/service/hlo_pass_interface.h\"\n",
      "\n",
      "namespace xla {\n",
      "namespace gpu {\n",
      "\n",
      "// Groups adjacent (logically and physically) reduced dimensions in reduction\n",
      "// input.\n",
      "//\n",
      "// Precondition: ReductionLayoutNormalizer has been run (physical proximity and\n",
      "// logical proximity become the same).\n",
      "//\n",
      "// For example,\n",
      "//\n",
      "//   f[] out = reduce(f[10,20,30] input, dimensions={0,1,2})\n",
      "//\n",
      "// becomes:\n",
      "//\n",
      "//   f[600] tmp = f[600] bitcast(f[10,20,30] input)\n",
      "//   f[] out = reduce(f[600] tmp, dimensions={0})\n",
      "//\n",
      "class ReductionDimensionGrouper : public HloModulePass {\n",
      " public:\n",
      "  absl::string_view name() const override {\n",
      "    return \"reduction-dimension-grouper\";\n",
      "  }\n",
      "\n",
      "  StatusOr<bool> Run(HloModule* module) override;\n",
      "};\n",
      "\n",
      "}  // namespace gpu\n",
      "}  // namespace xla\n",
      "\n",
      "#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_GPU_REDUCTION_DIMENSION_GROUPER_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/types/optional.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl optional library for handling optional values.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/compiler/xla/service/hlo_instructions.h\\\"\",\n",
      "        \"component_description\": \"Includes the HLO instructions header, which provides definitions and utilities related to High-Level Optimizer Intermediate Representation instructions.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/compiler/xla/service/hlo_module.h\\\"\",\n",
      "        \"component_description\": \"Includes the HLO module header, which provides definitions and utilities related to High-Level Optimizer modules.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/compiler/xla/service/hlo_pass_interface.h\\\"\",\n",
      "        \"component_description\": \"Includes the HLO pass interface header, which provides definitions and utilities for implementing optimization passes on HLO modules.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"ReductionDimensionGrouper\",\n",
      "        \"component_code\": \"class ReductionDimensionGrouper : public HloModulePass {\\n public:\\n  absl::string_view name() const override {\\n    return \\\"reduction-dimension-grouper\\\";\\n  }\\n\\n  StatusOr<bool> Run(HloModule* module) override;\\n};\",\n",
      "        \"component_description\": \"Defines a class named ReductionDimensionGrouper that inherits from HloModulePass. This class is used to group adjacent reduced dimensions in reduction inputs within an HLO module.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"name\",\n",
      "        \"component_code\": \"absl::string_view name() const override {\\n  return \\\"reduction-dimension-grouper\\\";\\n}\",\n",
      "        \"component_description\": \"Overrides the name function from HloModulePass to return a string view with the name of this pass, which is 'reduction-dimension-grouper'.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"Run\",\n",
      "        \"component_code\": \"StatusOr<bool> Run(HloModule* module) override;\",\n",
      "        \"component_description\": \"Declares the Run function, which is an override from HloModulePass. This function performs the main work of the pass on a given HLO module and returns a StatusOr<bool> indicating whether any changes were made.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This C++ header file defines a class named ReductionDimensionGrouper that extends the HloModulePass class from TensorFlow's XLA (Accelerated Linear Algebra) compiler. The pass is designed to group adjacent reduced dimensions in reduction inputs within an HLO module, optimizing memory access patterns and computation efficiency. It includes necessary headers for optional values handling, HLO instructions, modules, and passes. The ReductionDimensionGrouper class overrides the name function to provide its identifier and declares a Run function that will implement the pass logic on an HLO module.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "// Declarations for 2D FFT routines in third_party/fft2d/fft2d.\n",
      "\n",
      "#ifndef FFT2D_FFT_H__\n",
      "#define FFT2D_FFT_H__\n",
      "\n",
      "#ifdef __cplusplus\n",
      "extern \"C\" {\n",
      "#endif\n",
      "\n",
      "extern void cdft2d(int, int, int, double **, double *, int *, double *);\n",
      "extern void rdft2d(int, int, int, double **, double *, int *, double *);\n",
      "extern void ddct2d(int, int, int, double **, double *, int *, double *);\n",
      "extern void ddst2d(int, int, int, double **, double *, int *, double *);\n",
      "extern void rdft2dsort(int, int, int, double **);\n",
      "extern void ddct8x8s(int isgn, double **a);\n",
      "extern void ddct16x16s(int isgn, double **a);\n",
      "\n",
      "\n",
      "#ifdef __cplusplus\n",
      "}\n",
      "#endif\n",
      "\n",
      "#endif  // FFT2D_FFT_H__\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"FFT2D_FFT_H__\",\n",
      "        \"component_code\": \"#ifndef FFT2D_FFT_H__\\n#define FFT2D_FFT_H__\\n#endif  // FFT2D_FFT_H__\",\n",
      "        \"component_description\": \"Prevents multiple inclusions of the header file by defining a unique macro.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"COMPATIBILITY_MACRO\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#ifdef __cplusplus\\nextern \\\"C\\\" {\\n#endif\\n#ifdef __cplusplus\\n}\\n#endif\",\n",
      "        \"component_description\": \"Ensures that the functions are declared as C functions when included in C++ code, preventing name mangling.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"cdft2d\",\n",
      "        \"component_code\": \"extern void cdft2d(int, int, int, double **, double *, int *, double *);\",\n",
      "        \"component_description\": \"Declares a function for performing complex discrete Fourier transform on 2D data. Parameters include dimensions and pointers to input/output arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"rdft2d\",\n",
      "        \"component_code\": \"extern void rdft2d(int, int, int, double **, double *, int *, double *);\",\n",
      "        \"component_description\": \"Declares a function for performing real discrete Fourier transform on 2D data. Parameters include dimensions and pointers to input/output arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"ddct2d\",\n",
      "        \"component_code\": \"extern void ddct2d(int, int, int, double **, double *, int *, double *);\",\n",
      "        \"component_description\": \"Declares a function for performing discrete cosine transform on 2D data. Parameters include dimensions and pointers to input/output arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"ddst2d\",\n",
      "        \"component_code\": \"extern void ddst2d(int, int, int, double **, double *, int *, double *);\",\n",
      "        \"component_description\": \"Declares a function for performing discrete sine transform on 2D data. Parameters include dimensions and pointers to input/output arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"rdft2dsort\",\n",
      "        \"component_code\": \"extern void rdft2dsort(int, int, int, double **);\",\n",
      "        \"component_description\": \"Declares a function for sorting operations related to the real discrete Fourier transform on 2D data. Parameters include dimensions and pointers to input arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"ddct8x8s\",\n",
      "        \"component_code\": \"extern void ddct8x8s(int isgn, double **a);\",\n",
      "        \"component_description\": \"Declares a function for performing 8x8 discrete cosine transform on 2D data. Parameters include sign and pointer to input/output array.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"ddct16x16s\",\n",
      "        \"component_code\": \"extern void ddct16x16s(int isgn, double **a);\",\n",
      "        \"component_description\": \"Declares a function for performing 16x16 discrete cosine transform on 2D data. Parameters include sign and pointer to input/output array.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file declares several functions for performing various types of 2D fast Fourier transforms (FFT), including complex, real, discrete cosine, and discrete sine transforms. The functions are designed to operate on 2D data arrays and include parameters for dimensions, operation sign, and pointers to input/output data. Additionally, the file includes compatibility macros to allow the use of these C functions in C++ code.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\n",
      "#define TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\n",
      "\n",
      "#include <memory>\n",
      "\n",
      "#include \"mlir/Pass/Pass.h\"  // from @llvm-project\n",
      "\n",
      "namespace mlir {\n",
      "namespace tfg {\n",
      "\n",
      "// Sort topologically (following SSA defs-uses edges) the given block.\n",
      "// The sort is stable.\n",
      "void SortTopologically(Block *block);\n",
      "\n",
      "// Programmatically create a pass that topologically sort graphs.\n",
      "std::unique_ptr<Pass> CreateTopoSortPass();\n",
      "\n",
      "}  // namespace tfg\n",
      "}  // namespace mlir\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\\n#define TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\\n#endif  // TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\",\n",
      "        \"component_description\": \"A header guard to prevent multiple inclusions of the same header file.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"memory\",\n",
      "        \"component_code\": \"#include <memory>\",\n",
      "        \"component_description\": \"Includes the C++ standard library memory header, which provides facilities for dynamic memory management, including std::unique_ptr.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"mlir/Pass/Pass.h\",\n",
      "        \"component_code\": \"#include \\\"mlir/Pass/Pass.h\\\"  // from @llvm-project\",\n",
      "        \"component_description\": \"Includes the MLIR Pass header, which provides necessary definitions for creating and managing passes over MLIR (Multi-Level Intermediate Representation) modules.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"mlir\",\n",
      "        \"component_code\": \"namespace mlir {\\n}  // namespace mlir\",\n",
      "        \"component_description\": \"Declares the 'mlir' namespace, which is used for encapsulating MLIR-related code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tfg\",\n",
      "        \"component_code\": \"namespace tfg {\\n}  // namespace tfg\",\n",
      "        \"component_description\": \"Declares the 'tfg' (TensorFlow Graph) namespace within the 'mlir' namespace, which is used for TensorFlow-specific MLIR transformations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"SortTopologically\",\n",
      "        \"component_code\": \"void SortTopologically(Block *block);\",\n",
      "        \"component_description\": \"Declares a function named 'SortTopologically' that takes a pointer to an MLIR Block as an argument and sorts it topologically based on SSA (Static Single Assignment) definition-use chains. The sort is guaranteed to be stable.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"CreateTopoSortPass\",\n",
      "        \"component_code\": \"std::unique_ptr<Pass> CreateTopoSortPass();\",\n",
      "        \"component_description\": \"Declares a function named 'CreateTopoSortPass' that returns a std::unique_ptr to an MLIR Pass object. This pass is responsible for topologically sorting MLIR graphs programmatically.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file, part of TensorFlow's core transformations module, declares two functions within the 'mlir::tfg' namespace. The 'SortTopologically' function sorts a given MLIR Block in a topological order based on SSA definitions and uses. The 'CreateTopoSortPass' function creates and returns an MLIR Pass that performs this topological sorting on entire graphs. The use of std::unique_ptr for managing the pass ensures proper memory management.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<filename>tensorflow/core/kernels/batching_util/fake_clock_env.h<gh_stars>1000+\n",
      "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\n",
      "#define TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\n",
      "\n",
      "#include <functional>\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"tensorflow/core/lib/core/notification.h\"\n",
      "#include \"tensorflow/core/lib/core/status.h\"\n",
      "#include \"tensorflow/core/platform/env.h\"\n",
      "#include \"tensorflow/core/platform/macros.h\"\n",
      "#include \"tensorflow/core/platform/mutex.h\"\n",
      "#include \"tensorflow/core/platform/thread_annotations.h\"\n",
      "#include \"tensorflow/core/platform/types.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace serving {\n",
      "namespace test_util {\n",
      "\n",
      "// An Env implementation with a fake clock for NowMicros() and\n",
      "// SleepForMicroseconds(). The clock doesn't advance on its own; it advances via\n",
      "// an explicit Advance() method.\n",
      "// All other Env virtual methods pass through to a wrapped Env.\n",
      "class FakeClockEnv : public EnvWrapper {\n",
      " public:\n",
      "  explicit FakeClockEnv(Env* wrapped);\n",
      "  ~FakeClockEnv() override = default;\n",
      "\n",
      "  // Advance the clock by a certain number of microseconds.\n",
      "  void AdvanceByMicroseconds(int micros);\n",
      "\n",
      "  // Blocks until there is a sleeping thread that is scheduled to wake up at\n",
      "  // the given (absolute) time.\n",
      "  void BlockUntilSleepingThread(uint64 wake_time);\n",
      "\n",
      "  // Blocks until there are at least num_threads sleeping.\n",
      "  void BlockUntilThreadsAsleep(int num_threads);\n",
      "\n",
      "  // Methods that this class implements.\n",
      "  uint64 NowMicros() const override;\n",
      "  void SleepForMicroseconds(int64_t micros) override;\n",
      "\n",
      " private:\n",
      "  mutable mutex mu_;\n",
      "\n",
      "  uint64 current_time_ TF_GUARDED_BY(mu_) = 0;\n",
      "\n",
      "  struct SleepingThread {\n",
      "    uint64 wake_time;\n",
      "    Notification* wake_notification;\n",
      "  };\n",
      "  std::vector<SleepingThread> sleeping_threads_ TF_GUARDED_BY(mu_);\n",
      "\n",
      "  TF_DISALLOW_COPY_AND_ASSIGN(FakeClockEnv);\n",
      "};\n",
      "\n",
      "}  // namespace test_util\n",
      "}  // namespace serving\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\\n#define TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\\n#endif  // TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\",\n",
      "        \"component_description\": \"Header guard to prevent multiple inclusions of the file.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <functional>\",\n",
      "        \"component_description\": \"Includes the functional header for using function objects, such as std::function.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the string header for using the std::string class.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the vector header for using the std::vector container.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/lib/core/notification.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's Notification class, used to notify one thread of events in another thread.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/lib/core/status.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's Status class, which is used for error handling.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/env.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's Env class, an abstract interface representing the operating system environment.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/macros.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's macros file, which may contain various utility macros.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/mutex.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's mutex utilities for thread synchronization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/thread_annotations.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's thread annotations, which are used to document assumptions about thread safety in the code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/types.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's types header, which may define fixed-width integer types.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow { ... }\",\n",
      "        \"component_description\": \"Declares the beginning of the 'tensorflow' namespace, encapsulating TensorFlow-related code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"serving\",\n",
      "        \"component_code\": \"namespace serving { ... }\",\n",
      "        \"component_description\": \"Declares the beginning of the 'serving' namespace within tensorflow, used for serving-related utilities and classes.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"test_util\",\n",
      "        \"component_code\": \"namespace test_util { ... }\",\n",
      "        \"component_description\": \"Declares the beginning of the 'test_util' namespace within serving, containing testing utilities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"FakeClockEnv\",\n",
      "        \"component_code\": \"class FakeClockEnv : public EnvWrapper { ... };\",\n",
      "        \"component_description\": \"Defines a class named 'FakeClockEnv', which inherits from 'EnvWrapper'. It simulates an environment with a controllable clock for testing purposes.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTRUCTOR\",\n",
      "        \"component_name\": \"FakeClockEnv\",\n",
      "        \"component_code\": \"explicit FakeClockEnv(Env* wrapped);\",\n",
      "        \"component_description\": \"Constructor that takes a pointer to an 'Env' object and wraps it, allowing the fake clock environment to override certain methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"DESTRUCTOR\",\n",
      "        \"component_name\": \"~FakeClockEnv\",\n",
      "        \"component_code\": \"~FakeClockEnv() override = default;\",\n",
      "        \"component_description\": \"Default destructor for 'FakeClockEnv'.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"AdvanceByMicroseconds\",\n",
      "        \"component_code\": \"void AdvanceByMicroseconds(int micros);\",\n",
      "        \"component_description\": \"Method to advance the simulated clock by a specified number of microseconds.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"NowMicros\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that returns the current time according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"SleepForMicroseconds\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that simulates sleeping for a specified number of microseconds. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"mu_\",\n",
      "        \"component_code\": \"mutex mu_;\",\n",
      "        \"component_description\": \"A mutex used to synchronize access to the simulated clock and related data structures.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"now_micros_\",\n",
      "        \"component_code\": \"int64_t now_micros_;\",\n",
      "        \"component_description\": \"Stores the current time in microseconds according to the simulated clock.\"\n",
      "      }\n",
      "    ],\n",
      "    \"explanation\": \"The provided code snippet defines a class named 'FakeClockEnv' within the TensorFlow namespace, specifically under the sub-namespaces 'serving' and 'test_util'. This class is designed for testing purposes by simulating an environment with a controllable clock. It includes methods to advance the simulated time ('AdvanceByMicroseconds'), block until all scheduled events have occurred according to the simulated clock ('BlockUntilNow'), retrieve the current simulated time ('NowMicros'), and simulate sleeping for a specified duration ('SleepForMicroseconds'). The class uses synchronization mechanisms (mutex) to ensure thread safety when accessing and modifying the simulated clock state.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "The explanation provided covers all components of the code snippet, including header inclusions, namespace declarations, class definition, constructors, destructors, method definitions, and member variables. Each component is described in detail to provide a comprehensive understanding of its purpose and functionality within the context of the 'FakeClockEnv' class.\n",
      "\n",
      "---\n",
      "\n",
      "If you need further details or have any specific questions about the code snippet or its components, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_TAC_COMMON_TARGETS_H_\n",
      "#define TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_TAC_COMMON_TARGETS_H_\n",
      "\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"llvm/ADT/ArrayRef.h\"\n",
      "#include \"llvm/ADT/None.h\"\n",
      "#include \"llvm/ADT/Optional.h\"\n",
      "#include \"llvm/ADT/StringRef.h\"\n",
      "#include \"mlir/IR/Operation.h\"  // from @llvm-project\n",
      "\n",
      "namespace mlir {\n",
      "namespace TFL {\n",
      "namespace tac {\n",
      "\n",
      "// Device attribute string on the TFL dialect.\n",
      "constexpr char kDevice[] = \"tac.device\";\n",
      "\n",
      "// Inference type.\n",
      "constexpr char kInferenceType[] = \"tac.inference_type\";\n",
      "\n",
      "// TODO(renjieliu): Add more inference types.\n",
      "enum InferenceType {\n",
      "  UNKNOWN = 0,\n",
      "  FLOAT = 1,\n",
      "  QUANTIZED_INT8 = 2,\n",
      "  QUANTIZED_UINT8 = 3,\n",
      "  HYBRID = 4\n",
      "};\n",
      "\n",
      "inline InferenceType GetInferenceTypeEnum(llvm::StringRef inference_type_str) {\n",
      "  if (inference_type_str == \"FLOAT\") {\n",
      "    return FLOAT;\n",
      "  } else if (inference_type_str == \"QUANTIZED_INT8\") {\n",
      "    return QUANTIZED_INT8;\n",
      "  } else if (inference_type_str == \"QUANTIZED_UINT8\") {\n",
      "    return QUANTIZED_UINT8;\n",
      "  } else if (inference_type_str == \"HYBRID\") {\n",
      "    return HYBRID;\n",
      "  } else {\n",
      "    return UNKNOWN;\n",
      "  }\n",
      "}\n",
      "\n",
      "inline std::string GetInferenceString(InferenceType inference_type) {\n",
      "  if (inference_type == FLOAT) {\n",
      "    return \"FLOAT\";\n",
      "  } else if (inference_type == QUANTIZED_INT8) {\n",
      "    return \"QUANTIZED_INT8\";\n",
      "  } else if (inference_type == QUANTIZED_UINT8) {\n",
      "    return \"QUANTIZED_UINT8\";\n",
      "  } else if (inference_type == HYBRID) {\n",
      "    return \"HYBRID\";\n",
      "  } else {\n",
      "    return \"UNKNOWN\";\n",
      "  }\n",
      "}\n",
      "\n",
      "// Returns canonical representation for hardware name (All uppercase).\n",
      "// TODO(b/177376459): Remove this in favor of the string defined by hardwares\n",
      "// MyHardware::kId.\n",
      "inline std::string GetCanonicalHardwareName(const std::string& hardware_name) {\n",
      "  std::string name = hardware_name;\n",
      "  std::transform(\n",
      "      name.begin(), name.end(), name.begin(),\n",
      "      [](unsigned char c) -> unsigned char { return std::toupper(c); });\n",
      "  return name;\n",
      "}\n",
      "\n",
      "// Get the target annotation form the op.\n",
      "inline llvm::Optional<std::string> GetTargetAnnotation(Operation* op) {\n",
      "  auto device = op->getAttrOfType<StringAttr>(kDevice);\n",
      "  if (device == nullptr || device.getValue().empty()) return llvm::None;\n",
      "\n",
      "  return GetCanonicalHardwareName(device.getValue().str());\n",
      "}\n",
      "\n",
      "// Get inference type attribute from the operation if available.\n",
      "inline llvm::Optional<InferenceType> GetInferenceTypeAnnotation(Operation* op) {\n",
      "  auto inference_type = op->getAttrOfType<StringAttr>(kInferenceType);\n",
      "  if (inference_type == nullptr) return llvm::None;\n",
      "\n",
      "  llvm::StringRef device_name_str = inference_type.getValue();\n",
      "  return GetInferenceTypeEnum(device_name_str);\n",
      "}\n",
      "\n",
      "// InferenceDeviceType is a combination of the hardware with inference type.\n",
      "struct InferenceDeviceType {\n",
      "  std::string hardware;\n",
      "  InferenceType inference_type;\n",
      "\n",
      "  bool operator==(const InferenceDeviceType& other) const {\n",
      "    return (hardware == other.hardware) &&\n",
      "           (inference_type == other.inference_type);\n",
      "  }\n",
      "\n",
      "  bool operator!=(const InferenceDeviceType& other) const {\n",
      "    return !(*this == other);\n",
      "  }\n",
      "\n",
      "  struct inference_device_type_hash {\n",
      "    size_t operator()(const InferenceDeviceType& p) const {\n",
      "      auto hash1 = std::hash<std::string>{}(p.hardware);\n",
      "      auto hash2 = std::hash<InferenceType>{}(p.inference_type);\n",
      "      return hash1 ^ hash2;\n",
      "    }\n",
      "  };\n",
      "};\n",
      "\n",
      "// Get InferenceDeviceType attribute from the operation if available.\n",
      "inline llvm::Optional<InferenceDeviceType> GetInferenceDeviceTypeForOp(\n",
      "    Operation* op) {\n",
      "  auto hardware = GetTargetAnnotation(op);\n",
      "  if (!hardware.hasValue()) return llvm::None;\n",
      "\n",
      "  auto inference_type = GetInferenceTypeAnnotation(op);\n",
      "  if (!inference_type.hasValue()) return llvm::None;\n",
      "\n",
      "  InferenceDeviceType inference_device_type;\n",
      "  inference_device_type.hardware = hardware.getValue();\n",
      "  inference_device_type.inference_type = inference_type.getValue();\n",
      "  return inference_device_type;\n",
      "}\n",
      "\n",
      "}  // namespace tac\n",
      "}  // namespace TFL\n",
      "}  // namespace mlir\n",
      "\n",
      "#endif  // TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_TAC_COMMON_TARGETS_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the standard string library, which provides facilities to manipulate strings.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the standard vector library, which provides dynamic array capabilities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"llvm/ADT/ArrayRef.h\\\"\",\n",
      "        \"component_description\": \"Includes LLVM's ArrayRef header for a non-owning reference to an array of objects.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"llvm/ADT/None.h\\\"\",\n",
      "        \"component_description\": \"Includes LLVM's None header, which provides the NoneType and OptionalNone type definitions for representing 'no value'.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"llvm/ADT/Optional.h\\\"\",\n",
      "        \"component_description\": \"Includes LLVM's Optional header, which provides the Optional template class for values that may or may not be present.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"llvm/ADT/StringRef.h\\\"\",\n",
      "        \"component_description\": \"Includes LLVM's StringRef header, which provides a lightweight reference to an immutable string.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"mlir/IR/Operation.h\\\"  // from @llvm-project\\\"\",\n",
      "        \"component_description\": \"Includes MLIR's Operation header, which provides the base class for all operations in the MLIR Intermediate Representation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTANT_DEFINITION\",\n",
      "        \"component_name\": \"kDevice\",\n",
      "        \"component_code\": \"constexpr char kDevice[] = \\\"tac.device\\\";\",\n",
      "        \"component_description\": \"Defines a constant string that represents the attribute key for specifying device information in MLIR operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTANT_DEFINITION\",\n",
      "        \"component_name\": \"kInferenceType\",\n",
      "        \"component_code\": \"constexpr char kInferenceType[] = \\\"tac.inference_type\\\";\",\n",
      "        \"component_description\": \"Defines a constant string that represents the attribute key for specifying inference type information in MLIR operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetInferenceTypeEnum\",\n",
      "        \"component_code\": \"inline InferenceType GetInferenceTypeEnum(llvm::StringRef device_name_str) { ... }\",\n",
      "        \"component_description\": \"Converts a string representation of an inference type to the corresponding enum value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetCanonicalHardwareName\",\n",
      "        \"component_code\": \"inline std::string GetCanonicalHardwareName(const std::string& hardware_name) { ... }\",\n",
      "        \"component_description\": \"Converts a hardware name to its canonical uppercase form.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetTargetAnnotation\",\n",
      "        \"component_code\": \"inline llvm::Optional<std::string> GetTargetAnnotation(Operation* op) { ... }\",\n",
      "        \"component_description\": \"Retrieves the target hardware annotation from an MLIR operation, returning it in its canonical uppercase form.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetInferenceTypeAnnotation\",\n",
      "        \"component_code\": \"inline llvm::Optional<InferenceType> GetInferenceTypeAnnotation(Operation* op) { ... }\",\n",
      "        \"component_description\": \"Retrieves the inference type annotation from an MLIR operation as an enum value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"STRUCT_DEFINITION\",\n",
      "        \"component_name\": \"InferenceDeviceType\",\n",
      "        \"component_code\": \"struct InferenceDeviceType { ... };\",\n",
      "        \"component_description\": \"Defines a structure representing a combination of hardware and inference type, with comparison operators and a hash function.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetInferenceDeviceTypeForOp\",\n",
      "        \"component_code\": \"inline llvm::Optional<InferenceDeviceType> GetInferenceDeviceTypeForOp(Operation* op) { ... }\",\n",
      "        \"component_description\": \"Retrieves the inference device type annotation from an MLIR operation, combining hardware and inference type information.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file defines constants, functions, and a structure for handling target annotations in MLIR operations. It includes utilities for extracting and normalizing device and inference type information from operations, facilitating the management of different execution targets and their properties within an MLIR-based compiler or toolchain.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_CORE_TFRT_EAGER_CORE_RUNTIME_PLACER_H_\n",
      "#define TENSORFLOW_CORE_TFRT_EAGER_CORE_RUNTIME_PLACER_H_\n",
      "\n",
      "#include \"tensorflow/core/platform/status.h\"\n",
      "#include \"tfrt/support/forward_decls.h\"  // from @tf_runtime\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "class ImmediateExecutionOperation;\n",
      "class EagerContext;\n",
      "class AttrBuilder;\n",
      "class NodeDef;\n",
      "}  // namespace tensorflow\n",
      "\n",
      "namespace tfrt {\n",
      "class OpHandler;\n",
      "class CoreRuntime;\n",
      "class Device;\n",
      "\n",
      "namespace tf {\n",
      "\n",
      "using ::tensorflow::EagerContext;\n",
      "using ::tensorflow::ImmediateExecutionOperation;\n",
      "using ::tensorflow::NodeDef;\n",
      "using ::tensorflow::Status;\n",
      "\n",
      "// A helper class to select op handler in op-by-op execution.\n",
      "class EagerOpHandlerSelector final {\n",
      " public:\n",
      "  EagerOpHandlerSelector(CoreRuntime* core_runtime, EagerContext* eager_context,\n",
      "                         OpHandler* fallback_op_handler,\n",
      "                         bool pin_small_ops_to_cpu);\n",
      "  ~EagerOpHandlerSelector();\n",
      "\n",
      "  // Selects the op handler to execute the op based on the arguments. This\n",
      "  // op handler selection is cheap. But it can be nullptr even it return OK\n",
      "  // status.\n",
      "  Status SelectFromArguments(const ImmediateExecutionOperation& op,\n",
      "                             OpHandler** op_handler);\n",
      "\n",
      "  // Selects the op handler to execute the op based on NodeDef. This op handler\n",
      "  // selection is expensive. It will never return nullptr unless there is an\n",
      "  // error. Please only invoke this method when the cheap version fails.\n",
      "  Status SelectFromNodeDef(const ImmediateExecutionOperation& op,\n",
      "                           const NodeDef* ndef, OpHandler** op_handler);\n",
      "\n",
      " private:\n",
      "  CoreRuntime* core_runtime_;\n",
      "  EagerContext* eager_context_;\n",
      "\n",
      "  const Device& cpu_device_;\n",
      "  OpHandler* cpu_op_handler_;\n",
      "  OpHandler* fallback_op_handler_;\n",
      "  bool pin_small_ops_to_cpu_;\n",
      "};\n",
      "\n",
      "}  // namespace tf\n",
      "}  // namespace tfrt\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_TFRT_EAGER_CORE_RUNTIME_PLACER_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/status.h\\\"\",\n",
      "        \"component_description\": \"Includes the TensorFlow Status header for error handling.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tfrt/support/forward_decls.h\\\"  // from @tf_runtime\",\n",
      "        \"component_description\": \"Forward declares symbols from the TFRT support library to avoid including full headers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\\n  class ImmediateExecutionOperation;\\n  class EagerContext;\\n  class AttrBuilder;\\n  class NodeDef;\\n}  // namespace tensorflow\",\n",
      "        \"component_description\": \"Declares classes used in the TensorFlow library within the 'tensorflow' namespace.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tfrt::tf\",\n",
      "        \"component_code\": \"namespace tfrt {\\nclass OpHandler;\\nclass CoreRuntime;\\nclass Device;\\n\\nnamespace tf {\\nusing ::tensorflow::EagerContext;\\nusing ::tensorflow::ImmediateExecutionOperation;\\nusing ::tensorflow::NodeDef;\\nusing ::tensorflow::Status;\",\n",
      "        \"component_description\": \"Declares classes used in the TFRT library and imports TensorFlow classes into the 'tfrt::tf' namespace.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"EagerOpHandlerSelector\",\n",
      "        \"component_code\": \"class EagerOpHandlerSelector final {\\n public:\\n  EagerOpHandlerSelector(CoreRuntime* core_runtime, EagerContext* eager_context,\\n                         OpHandler* fallback_op_handler,\\n                         bool pin_small_ops_to_cpu);\\n  ~EagerOpHandlerSelector();\\n\\n  // Selects the op handler to execute the op based on the arguments. This\\n  // op handler selection is cheap. But it can be nullptr even it return OK\\n  // status.\\n  Status SelectFromArguments(const ImmediateExecutionOperation& op,\\n                             OpHandler** op_handler);\\n\\n  // Selects the op handler to execute the op based on NodeDef. This op handler\\n  // selection is expensive. It will never return nullptr unless there is an\\n  // error. Please only invoke this method when the cheap version fails.\\n  Status SelectFromNodeDef(const ImmediateExecutionOperation& op,\\n                           const NodeDef* ndef, OpHandler** op_handler);\\n\\n private:\\n  CoreRuntime* core_runtime_;\\n  EagerContext* eager_context_;\\n\\n  const Device& cpu_device_;\\n  OpHandler* cpu_op_handler_;\\n  OpHandler* fallback_op_handler_;\\n  bool pin_small_ops_to_cpu_;\\n};\",\n",
      "        \"component_description\": \"Defines a class to select operation handlers for TensorFlow operations in an eager execution context. It includes constructors, destructors, and methods for selecting op handlers based on different criteria.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file defines the 'EagerOpHandlerSelector' class within the 'tfrt::tf' namespace, used to select operation handlers for TensorFlow operations in an eager execution context. It includes necessary headers, declares classes from the TensorFlow and TFRT libraries, and provides a detailed description of the 'EagerOpHandlerSelector' class's functionality and its methods.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_COMMON_RUNTIME_REQUEST_COST_ACCESSOR_H_\n",
      "#define TENSORFLOW_CORE_COMMON_RUNTIME_REQUEST_COST_ACCESSOR_H_\n",
      "\n",
      "#include <string>\n",
      "\n",
      "#include \"absl/container/flat_hash_map.h\"\n",
      "#include \"absl/time/time.h\"\n",
      "#include \"tensorflow/core/common_runtime/request_cost.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "// An interface for accessing the RequestCost associated with the current rpc\n",
      "// request.\n",
      "class RequestCostAccessor {\n",
      " public:\n",
      "  virtual ~RequestCostAccessor() {}\n",
      "  virtual RequestCost* GetRequestCost() const = 0;\n",
      "};\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_COMMON_RUNTIME_REQUEST_COST_ACCESSOR_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the standard C++ string library for handling string operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/container/flat_hash_map.h\\\"\",\n",
      "        \"component_description\": \"Includes the Abseil flat hash map header, which provides a fast hash map implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/time/time.h\\\"\",\n",
      "        \"component_description\": \"Includes the Abseil time library for handling time-related operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/common_runtime/request_cost.h\\\"\",\n",
      "        \"component_description\": \"Includes a TensorFlow header file that likely defines the RequestCost class or struct, which is used to track and manage request costs.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"RequestCostAccessor\",\n",
      "        \"component_code\": \"class RequestCostAccessor {\\n public:\\n  virtual ~RequestCostAccessor() {}\\n  virtual RequestCost* GetRequestCost() const = 0;\\n};\",\n",
      "        \"component_description\": \"Defines an abstract class named RequestCostAccessor. This class provides an interface for accessing the RequestCost associated with the current RPC (Remote Procedure Call) request. It includes a pure virtual destructor and a pure virtual method GetRequestCost(), which must be implemented by any derived classes to return a pointer to a RequestCost object.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This C++ header file defines an interface for accessing the cost associated with an RPC request in TensorFlow, a popular machine learning framework. The file includes necessary headers from the standard library and Abseil, as well as a custom TensorFlow header that presumably contains the definition of the RequestCost class or struct. The primary component is the RequestCostAccessor class, which serves as an abstract base class for any classes that need to provide access to request cost information. It declares a pure virtual method GetRequestCost() that must be implemented by derived classes to return a pointer to a RequestCost object.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_UNARY_ELEMENTWISE_TESTER_H_\n",
      "#define TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_UNARY_ELEMENTWISE_TESTER_H_\n",
      "\n",
      "#include <cstdint>\n",
      "#include <vector>\n",
      "\n",
      "#include <gtest/gtest.h>\n",
      "#include \"tensorflow/lite/c/common.h\"\n",
      "#include \"tensorflow/lite/interpreter.h\"\n",
      "#include \"tensorflow/lite/schema/schema_generated.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace xnnpack {\n",
      "\n",
      "class QuantizedUnaryElementwiseTester {\n",
      " public:\n",
      "  QuantizedUnaryElementwiseTester() = default;\n",
      "  QuantizedUnaryElementwiseTester(const QuantizedUnaryElementwiseTester&) =\n",
      "      delete;\n",
      "  QuantizedUnaryElementwiseTester& operator=(\n",
      "      const QuantizedUnaryElementwiseTester&) = delete;\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& Shape(\n",
      "      std::initializer_list<int32_t> shape) {\n",
      "    for (auto it = shape.begin(); it != shape.end(); ++it) {\n",
      "      EXPECT_GT(*it, 0);\n",
      "    }\n",
      "    shape_ = std::vector<int32_t>(shape.begin(), shape.end());\n",
      "    size_ = QuantizedUnaryElementwiseTester::ComputeSize(shape_);\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  const std::vector<int32_t>& Shape() const { return shape_; }\n",
      "\n",
      "  int32_t Size() const { return size_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& InputZeroPoint(\n",
      "      int32_t input_zero_point) {\n",
      "    input_zero_point_ = input_zero_point;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline int32_t InputZeroPoint() const { return input_zero_point_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& OutputZeroPoint(\n",
      "      int32_t output_zero_point) {\n",
      "    output_zero_point_ = output_zero_point;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline int32_t OutputZeroPoint() const { return output_zero_point_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& InputScale(float input_scale) {\n",
      "    input_scale_ = input_scale;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline float InputScale() const { return input_scale_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& OutputScale(float output_scale) {\n",
      "    output_scale_ = output_scale;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline float OutputScale() const { return output_scale_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& Unsigned(bool is_unsigned) {\n",
      "    unsigned_ = is_unsigned;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline bool Unsigned() const { return unsigned_; }\n",
      "\n",
      "  template <class T>\n",
      "  void Test(Interpreter* delegate_interpreter,\n",
      "            Interpreter* default_interpreter) const;\n",
      "\n",
      "  void Test(tflite::BuiltinOperator unary_op, TfLiteDelegate* delegate) const;\n",
      "\n",
      " private:\n",
      "  std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator unary_op) const;\n",
      "\n",
      "  static int32_t ComputeSize(const std::vector<int32_t>& shape);\n",
      "\n",
      "  std::vector<int32_t> shape_;\n",
      "  int32_t size_;\n",
      "  int32_t input_zero_point_ = 0;\n",
      "  int32_t output_zero_point_ = 0;\n",
      "  float input_scale_ = 1.0f;\n",
      "  float output_scale_ = 1.0f;\n",
      "  bool unsigned_ = false;\n",
      "};\n",
      "\n",
      "}  // namespace xnnpack\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_UNARY_ELEMENTWISE_TESTER_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <cstdint>\",\n",
      "        \"component_description\": \"Includes the cstdint header for fixed-width integer types.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the vector header for using the std::vector container.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <gtest/gtest.h>\",\n",
      "        \"component_description\": \"Includes the gtest header for Google Test framework functionalities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/c/common.h\\\"\",\n",
      "        \"component_description\": \"Includes the TensorFlow Lite C API common header file for general definitions and functions.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/interpreter.h\\\"\",\n",
      "        \"component_description\": \"Includes the TensorFlow Lite Interpreter header file for running models.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/schema/schema_generated.h\\\"\",\n",
      "        \"component_description\": \"Includes the generated schema header file for TensorFlow Lite flatbuffer schemas.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"QuantizedUnaryElementwiseTester\",\n",
      "        \"component_code\": \"class QuantizedUnaryElementwiseTester { ... };\",\n",
      "        \"component_description\": \"Defines a class named QuantizedUnaryElementwiseTester used for testing quantized unary element-wise operations in TensorFlow Lite. It includes methods to set and get various parameters such as shape, zero points, scales, and unsigned status. It also contains private helper functions to create the model and compute the size based on the provided shape.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTRUCTOR\",\n",
      "        \"component_name\": \"QuantizedUnaryElementwiseTester\",\n",
      "        \"component_code\": \"QuantizedUnaryElementwiseTester() = default;\",\n",
      "        \"component_description\": \"Default constructor for the QuantizedUnaryElementwiseTester class, initializing all members to their default values.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Shape\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& Shape(std::initializer_list<int32_t> shape) { ... }\",\n",
      "        \"component_description\": \"Sets the shape of the input tensor. It validates that each dimension is greater than 0 and then updates the shape_ member variable and computes the size using ComputeSize().\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Shape\",\n",
      "        \"component_code\": \"const std::vector<int32_t>& Shape() const { return shape_; }\",\n",
      "        \"component_description\": \"Returns a constant reference to the shape_ member variable, representing the dimensions of the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Size\",\n",
      "        \"component_code\": \"int32_t Size() const { return size_; }\",\n",
      "        \"component_description\": \"Returns the total number of elements in the input tensor, calculated based on its shape.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputZeroPoint\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& InputZeroPoint(int32_t input_zero_point) { ... }\",\n",
      "        \"component_description\": \"Sets the zero point for the input tensor, which is used in quantization to map floating-point values to integer representations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputZeroPoint\",\n",
      "        \"component_code\": \"inline int32_t InputZeroPoint() const { return input_zero_point_; }\",\n",
      "        \"component_description\": \"Returns the zero point for the input tensor, used in quantization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"OutputZeroPoint\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& OutputZeroPoint(int32_t output_zero_point) { ... }\",\n",
      "        \"component_description\": \"Sets the zero point for the output tensor, which is used in quantization to map floating-point values to integer representations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"OutputZeroPoint\",\n",
      "        \"component_code\": \"inline int32_t OutputZeroPoint() const { return output_zero_point_; }\",\n",
      "        \"component_description\": \"Returns the zero point for the output tensor, used in quantization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputScale\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& InputScale(float input_scale) { ... }\",\n",
      "        \"component_description\": \"Sets the scale for the input tensor, which is used in quantization to map floating-point values to integer representations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputScale\",\n",
      "        \"component_code\": \"inline float InputScale() const { return input_scale_; }\",\n",
      "        \"component_description\": \"Returns the scale for the input tensor, used in quantization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"OutputScale\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& OutputScale(float output_scale) { ... }\",\n",
      "        \"component_description\": \"Sets the scale for the output tensor, which is used in quantization to map floating-point values to integer representations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"OutputScale\",\n",
      "        \"component_code\": \"inline float OutputScale() const { return output_scale_; }\",\n",
      "        \"component_description\": \"Returns the scale for the output tensor, used in quantization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Unsigned\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& Unsigned(bool unsigned_) { ... }\",\n",
      "        \"component_description\": \"Sets whether the input and output tensors are interpreted as unsigned integers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Unsigned\",\n",
      "        \"component_code\": \"inline bool Unsigned() const { return unsigned_; }\",\n",
      "        \"component_description\": \"Returns a boolean indicating whether the input and output tensors are treated as unsigned integers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"TEMPLATE_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Test\",\n",
      "        \"component_code\": \"template <class T> void Test(Interpreter* delegate_interpreter, Interpreter* default_interpreter) const;\",\n",
      "        \"component_description\": \"Template method for testing the quantized unary element-wise operation using both a delegate interpreter and a default interpreter.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Test\",\n",
      "        \"component_code\": \"void Test(tflite::BuiltinOperator unary_op, TfLiteDelegate* delegate) const;\",\n",
      "        \"component_description\": \"Method for testing the quantized unary element-wise operation using a specific TensorFlow Lite BuiltinOperator and delegate.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"CreateTfLiteModel\",\n",
      "        \"component_code\": \"std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator unary_op) const;\",\n",
      "        \"component_description\": \"Private method that creates a TensorFlow Lite model flatbuffer for the specified quantized unary element-wise operation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_STATIC_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"ComputeSize\",\n",
      "        \"component_code\": \"static int32_t ComputeSize(const std::vector<int32_t>& shape);\",\n",
      "        \"component_description\": \"Static private method that computes the total number of elements in a tensor based on its dimensions.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"shape_\",\n",
      "        \"component_code\": \"std::vector<int32_t> shape_;\",\n",
      "        \"component_description\": \"Private member variable storing the dimensions of the input tensor as a vector of integers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"size_\",\n",
      "        \"component_code\": \"int32_t size_;\",\n",
      "        \"component_description\": \"Private member variable storing the total number of elements in the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"input_zero_point_\",\n",
      "        \"component_code\": \"int32_t input_zero_point_ = 0;\",\n",
      "        \"component_description\": \"Private member variable storing the zero point for the input tensor, defaulting to 0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"output_zero_point_\",\n",
      "        \"component_code\": \"int32_t output_zero_point_ = 0;\",\n",
      "        \"component_description\": \"Private member variable storing the zero point for the output tensor, defaulting to 0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"input_scale_\",\n",
      "        \"component_code\": \"float input_scale_ = 1.0f;\",\n",
      "        \"component_description\": \"Private member variable storing the scale for the input tensor, defaulting to 1.0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"output_scale_\",\n",
      "        \"component_code\": \"float output_scale_ = 1.0f;\",\n",
      "        \"component_description\": \"Private member variable storing the scale for the output tensor, defaulting to 1.0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"unsigned_\",\n",
      "        \"component_code\": \"bool unsigned_ = false;\",\n",
      "        \"component_description\": \"Private member variable indicating whether the input and output tensors are treated as unsigned integers, defaulting to false.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "This detailed breakdown of the `QuantizedUnaryElementwiseOpTester` class should help you understand its structure, methods, and usage for testing quantized unary element-wise operations in TensorFlow Lite.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "<gh_stars>1000+\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_PAD_TESTER_H_\n",
      "#define TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_PAD_TESTER_H_\n",
      "\n",
      "#include <cstdint>\n",
      "#include <vector>\n",
      "\n",
      "#include <gtest/gtest.h>\n",
      "#include \"tensorflow/lite/c/common.h\"\n",
      "#include \"tensorflow/lite/interpreter.h\"\n",
      "#include \"tensorflow/lite/schema/schema_generated.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace xnnpack {\n",
      "\n",
      "class QuantizedPadTester {\n",
      " public:\n",
      "  QuantizedPadTester() = default;\n",
      "  QuantizedPadTester(const QuantizedPadTester&) = delete;\n",
      "  QuantizedPadTester& operator=(const QuantizedPadTester&) = delete;\n",
      "\n",
      "  inline QuantizedPadTester& InputShape(std::initializer_list<int32_t> shape) {\n",
      "    for (auto it = shape.begin(); it != shape.end(); ++it) {\n",
      "      EXPECT_GT(*it, 0);\n",
      "    }\n",
      "    input_shape_ = std::vector<int32_t>(shape.begin(), shape.end());\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline const std::vector<int32_t>& InputShape() const { return input_shape_; }\n",
      "\n",
      "  inline QuantizedPadTester& InputPrePaddings(\n",
      "      std::initializer_list<int32_t> paddings) {\n",
      "    for (auto it = paddings.begin(); it != paddings.end(); ++it) {\n",
      "      EXPECT_GE(*it, 0);\n",
      "    }\n",
      "    input_pre_paddings_ =\n",
      "        std::vector<int32_t>(paddings.begin(), paddings.end());\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline const std::vector<int32_t> InputPrePaddings() const {\n",
      "    return input_pre_paddings_;\n",
      "  }\n",
      "\n",
      "  inline QuantizedPadTester& InputPostPaddings(\n",
      "      std::initializer_list<int32_t> paddings) {\n",
      "    for (auto it = paddings.begin(); it != paddings.end(); ++it) {\n",
      "      EXPECT_GE(*it, 0);\n",
      "    }\n",
      "    input_post_paddings_ =\n",
      "        std::vector<int32_t>(paddings.begin(), paddings.end());\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline const std::vector<int32_t> InputPostPaddings() const {\n",
      "    return input_post_paddings_;\n",
      "  }\n",
      "\n",
      "  std::vector<int32_t> OutputShape() const;\n",
      "\n",
      "  inline QuantizedPadTester& ZeroPoint(int32_t zero_point) {\n",
      "    zero_point_ = zero_point;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline int32_t ZeroPoint() const { return zero_point_; }\n",
      "\n",
      "  inline QuantizedPadTester& Scale(float scale) {\n",
      "    scale_ = scale;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline float Scale() const { return scale_; }\n",
      "\n",
      "  inline QuantizedPadTester& Unsigned(bool is_unsigned) {\n",
      "    unsigned_ = is_unsigned;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline bool Unsigned() const { return unsigned_; }\n",
      "\n",
      "  template <class T>\n",
      "  void Test(Interpreter* delegate_interpreter,\n",
      "            Interpreter* default_interpreter) const;\n",
      "\n",
      "  void Test(TfLiteDelegate* delegate) const;\n",
      "\n",
      " private:\n",
      "  std::vector<char> CreateTfLiteModel() const;\n",
      "\n",
      "  static int32_t ComputeSize(const std::vector<int32_t>& shape);\n",
      "\n",
      "  std::vector<int32_t> input_shape_;\n",
      "  std::vector<int32_t> input_pre_paddings_;\n",
      "  std::vector<int32_t> input_post_paddings_;\n",
      "  int32_t zero_point_ = 7;\n",
      "  float scale_ = 0.8f;\n",
      "  bool unsigned_ = false;\n",
      "};\n",
      "\n",
      "}  // namespace xnnpack\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_PAD_TESTER_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <cstdint>\",\n",
      "        \"component_description\": \"Includes the standard C integer types header, providing fixed-width integer types like int32_t.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the C++ Standard Library vector header, which is used for dynamic arrays in this code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <gtest/gtest.h>\",\n",
      "        \"component_description\": \"Includes the Google Test framework headers, enabling the use of test assertions and fixtures.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/c/common.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow Lite C API common header file which contains declarations for data types and constants used in the TensorFlow Lite C API.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/interpreter.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow Lite interpreter header file, providing access to the classes for running models with TensorFlow Lite.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/schema/schema_generated.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow Lite schema generated header file which contains definitions of the flatbuffer schema used by TensorFlow Lite models.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"QuantizedPadTester\",\n",
      "        \"component_code\": \"class QuantizedPadTester { ... };\",\n",
      "        \"component_description\": \"Defines a class named QuantizedPadTester which is used for testing the quantized padding operation in TensorFlow Lite using the XNNPACK delegate. It includes methods to set and get various properties related to the input tensor shape, paddings, zero point, scale, and data type (signed/unsigned).\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputShape\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& InputShape(std::initializer_list<int32_t> shape) { ... }\",\n",
      "        \"component_description\": \"Sets the input tensor shape and validates that each dimension is greater than zero. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputShape\",\n",
      "        \"component_code\": \"inline const std::vector<int32_t>& InputShape() const { return input_shape_; }\",\n",
      "        \"component_description\": \"Returns a constant reference to the vector containing the current input tensor shape.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputPrePaddings\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& InputPrePaddings(std::initializer_list<int32_t> paddings) { ... }\",\n",
      "        \"component_description\": \"Sets the pre-padding values for each dimension of the input tensor and validates that they are non-negative. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputPrePaddings\",\n",
      "        \"component_code\": \"inline const std::vector<int32_t>& InputPrePaddings() const { return input_pre_paddings_; }\",\n",
      "        \"component_description\": \"Returns a constant reference to the vector containing the current pre-padding values for each dimension of the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputPostPaddings\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& InputPostPaddings(std::initializer_list<int32_t> paddings) { ... }\",\n",
      "        \"component_description\": \"Sets the post-padding values for each dimension of the input tensor and validates that they are non-negative. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputPostPaddings\",\n",
      "        \"component_code\": \"inline const std::vector<int32_t>& InputPostPaddings() const { return input_post_paddings_; }\",\n",
      "        \"component_description\": \"Returns a constant reference to the vector containing the current post-padding values for each dimension of the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"ZeroPoint\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& ZeroPoint(int32_t zero_point) { zero_point_ = zero_point; return *this; }\",\n",
      "        \"component_description\": \"Sets the quantization zero point for the input tensor. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"ZeroPoint\",\n",
      "        \"component_code\": \"inline int32_t ZeroPoint() const { return zero_point_; }\",\n",
      "        \"component_description\": \"Returns the quantization zero point for the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Scale\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& Scale(float scale) { scale_ = scale; return *this; }\",\n",
      "        \"component_description\": \"Sets the quantization scale for the input tensor. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Scale\",\n",
      "        \"component_code\": \"inline float Scale() const { return scale_; }\",\n",
      "        \"component_description\": \"Returns the quantization scale for the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Unsigned\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& Unsigned(bool is_unsigned) { unsigned_ = is_unsigned; return *this; }\",\n",
      "        \"component_description\": \"Sets whether the data type of the input tensor is unsigned. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Unsigned\",\n",
      "        \"component_code\": \"inline bool Unsigned() const { return unsigned_; }\",\n",
      "        \"component_description\": \"Returns whether the data type of the input tensor is unsigned.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"TEMPLATE_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Test\",\n",
      "        \"component_code\": \"template <class T> void Test(Interpreter* delegate_interpreter, Interpreter* default_interpreter) const;\",\n",
      "        \"component_description\": \"A template method that performs the actual test using two interpreters: one with a TensorFlow Lite delegate and another without. The method is templated to allow for flexibility in the type of data used for testing.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Test\",\n",
      "        \"component_code\": \"void Test(TfLiteDelegate* delegate) const;\",\n",
      "        \"component_description\": \"Performs the actual test using a TensorFlow Lite delegate. This method creates two interpreters, one with and one without the delegate, and calls the templated Test method to compare their results.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"CreateTfLiteModel\",\n",
      "        \"component_code\": \"std::vector<char> CreateTfLiteModel() const;\",\n",
      "        \"component_description\": \"Creates a TensorFlow Lite model with the specified input tensor shape, paddings, zero point, scale, and data type. The method returns a vector of characters containing the serialized flatbuffer model.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"STATIC_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"ComputeSize\",\n",
      "        \"component_code\": \"static int32_t ComputeSize(const std::vector<int32_t>& shape);\",\n",
      "        \"component_description\": \"A static method that computes the total number of elements in a tensor with the specified shape. The method is used to allocate memory for input and output tensors during testing.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"input_pre_paddings_\",\n",
      "        \"component_code\": \"std::vector<int32_t> input_pre_paddings_;\",\n",
      "        \"component_description\": \"A vector containing the pre-padding values for each dimension of the input tensor. The padding values are set using the InputPrePaddings method.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"input_post_paddings_\",\n",
      "        \"component_code\": \"std::vector<int32_t> input_post_paddings_;\",\n",
      "        \"component_description\": \"A vector containing the post-padding values for each dimension of the input tensor. The padding values are set using the InputPostPaddings method.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"zero_point_\",\n",
      "        \"component_code\": \"int32_t zero_point_ = 0;\",\n",
      "        \"component_description\": \"The quantization zero point for the input tensor. The zero point is set using the ZeroPoint method and defaults to 0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"scale_\",\n",
      "        \"component_code\": \"float scale_ = 1.0f;\",\n",
      "        \"component_description\": \"The quantization scale for the input tensor. The scale is set using the Scale method and defaults to 1.0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"unsigned_\",\n",
      "        \"component_code\": \"bool unsigned_ = false;\",\n",
      "        \"component_description\": \"A boolean flag indicating whether the data type of the input tensor is unsigned. The flag is set using the Unsigned method and defaults to false.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "1. **Class Definition**: The `QuantizedPadTest` class encapsulates all necessary components to test the quantized pad operation in TensorFlow Lite.\n",
      "\n",
      "2. **Member Variables**:\n",
      "   - `input_pre_paddings_`: Stores the pre-padding values for each dimension of the input tensor.\n",
      "   - `input_post_paddings_`: Stores the post-padding values for each dimension of the input tensor.\n",
      "   - `zero_point_`: The quantization zero point, initialized to 0.\n",
      "   - `scale_`: The quantization scale, initialized to 1.0.\n",
      "   - `unsigned_`: A boolean indicating if the data type is unsigned, initialized to false.\n",
      "\n",
      "3. **Methods**:\n",
      "   - **Setters**: Methods like `ZeroPoint`, `Scale`, and `Unsigned` allow setting the respective member variables.\n",
      "   - **Getters**: Corresponding getter methods return the current values of these variables.\n",
      "   - **Test Methods**: \n",
      "     - A template method `Test(Interpreter* delegate_interpreter, Interpreter* default_interpreter)` to run tests with a delegate interpreter and a default interpreter.\n",
      "     - Another method `Test(TfLiteDelegate* delegate)` that sets up interpreters and calls the template test method.\n",
      "   - **Helper Methods**:\n",
      "     - `CreateTfLiteModel`: Creates and returns a serialized flatbuffer model for testing.\n",
      "     - `ComputeSize`: Computes the total number of elements in a tensor given its shape, used for memory allocation.\n",
      "\n",
      "4. **Template Method**: The use of a template method allows flexibility in handling different data types during testing.\n",
      "\n",
      "This setup ensures that all aspects of the quantized pad operation can be thoroughly tested under various configurations and scenarios.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input: \n",
      "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_STREAM_EXECUTOR_HOST_OR_DEVICE_SCALAR_H_\n",
      "#define TENSORFLOW_STREAM_EXECUTOR_HOST_OR_DEVICE_SCALAR_H_\n",
      "\n",
      "#include \"tensorflow/stream_executor/data_type.h\"\n",
      "#include \"tensorflow/stream_executor/device_memory.h\"\n",
      "#include \"tensorflow/stream_executor/platform/logging.h\"\n",
      "\n",
      "namespace stream_executor {\n",
      "\n",
      "// Allows to represent a value that is either a host scalar or a scalar stored\n",
      "// on the GPU device.\n",
      "// See also the specialization for ElemT=void below.\n",
      "template <typename ElemT>\n",
      "class HostOrDeviceScalar {\n",
      " public:\n",
      "  // Not marked as explicit because when using this constructor, we usually want\n",
      "  // to set this to a compile-time constant.\n",
      "  HostOrDeviceScalar(ElemT value) : value_(value), is_pointer_(false) {}\n",
      "  explicit HostOrDeviceScalar(const DeviceMemory<ElemT>& pointer)\n",
      "      : pointer_(pointer), is_pointer_(true) {\n",
      "    CHECK_EQ(1, pointer.ElementCount());\n",
      "  }\n",
      "\n",
      "  bool is_pointer() const { return is_pointer_; }\n",
      "  const DeviceMemory<ElemT>& pointer() const {\n",
      "    CHECK(is_pointer());\n",
      "    return pointer_;\n",
      "  }\n",
      "  const ElemT& value() const {\n",
      "    CHECK(!is_pointer());\n",
      "    return value_;\n",
      "  }\n",
      "\n",
      " private:\n",
      "  union {\n",
      "    ElemT value_;\n",
      "    DeviceMemory<ElemT> pointer_;\n",
      "  };\n",
      "  bool is_pointer_;\n",
      "};\n",
      "\n",
      "// Specialization for wrapping a dynamically-typed value (via type erasure).\n",
      "template <>\n",
      "class HostOrDeviceScalar<void> {\n",
      " public:\n",
      "  using DataType = dnn::DataType;\n",
      "\n",
      "  // Constructors not marked as explicit because when using this constructor, we\n",
      "  // usually want to set this to a compile-time constant.\n",
      "\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(float value)\n",
      "      : float_(value), is_pointer_(false), dtype_(DataType::kFloat) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(double value)\n",
      "      : double_(value), is_pointer_(false), dtype_(DataType::kDouble) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(Eigen::half value)\n",
      "      : half_(value), is_pointer_(false), dtype_(DataType::kHalf) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(int8 value)\n",
      "      : int8_(value), is_pointer_(false), dtype_(DataType::kInt8) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(int32 value)\n",
      "      : int32_(value), is_pointer_(false), dtype_(DataType::kInt32) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(std::complex<float> value)\n",
      "      : complex_float_(value),\n",
      "        is_pointer_(false),\n",
      "        dtype_(DataType::kComplexFloat) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(std::complex<double> value)\n",
      "      : complex_double_(value),\n",
      "        is_pointer_(false),\n",
      "        dtype_(DataType::kComplexDouble) {}\n",
      "  template <typename T>\n",
      "  explicit HostOrDeviceScalar(const DeviceMemory<T>& pointer)\n",
      "      : pointer_(pointer),\n",
      "        is_pointer_(true),\n",
      "        dtype_(dnn::ToDataType<T>::value) {\n",
      "    CHECK_EQ(1, pointer.ElementCount());\n",
      "  }\n",
      "  // Construct from statically-typed version.\n",
      "  template <typename T, typename std::enable_if<!std::is_same<T, void>::value,\n",
      "                                                int>::type = 0>\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(const HostOrDeviceScalar<T>& other) {\n",
      "    if (other.is_pointer()) {\n",
      "      *this = HostOrDeviceScalar(other.pointer());\n",
      "    } else {\n",
      "      *this = HostOrDeviceScalar(other.value());\n",
      "    }\n",
      "  }\n",
      "\n",
      "  bool is_pointer() const { return is_pointer_; }\n",
      "  template <typename T>\n",
      "  const DeviceMemory<T>& pointer() const {\n",
      "    CHECK(is_pointer());\n",
      "    CHECK(dtype_ == dnn::ToDataType<T>::value);\n",
      "    return pointer_;\n",
      "  }\n",
      "  template <typename T>\n",
      "  const T& value() const {\n",
      "    CHECK(!is_pointer());\n",
      "    CHECK(dtype_ == dnn::ToDataType<T>::value);\n",
      "    return value_impl<T>();\n",
      "  }\n",
      "  const DeviceMemoryBase& opaque_pointer() const {\n",
      "    CHECK(is_pointer());\n",
      "    return pointer_;\n",
      "  }\n",
      "  const void* opaque_value() const {\n",
      "    CHECK(!is_pointer());\n",
      "    switch (dtype_) {\n",
      "      case DataType::kFloat:\n",
      "        return &float_;\n",
      "      case DataType::kDouble:\n",
      "        return &double_;\n",
      "      case DataType::kHalf:\n",
      "        return &half_;\n",
      "      case DataType::kInt8:\n",
      "        return &int8_;\n",
      "      case DataType::kInt32:\n",
      "        return &int32_;\n",
      "      case DataType::kComplexFloat:\n",
      "        return &complex_float_;\n",
      "      case DataType::kComplexDouble:\n",
      "        return &complex_double_;\n",
      "      default:\n",
      "        return nullptr;\n",
      "    }\n",
      "  }\n",
      "  DataType data_type() const { return dtype_; }\n",
      "\n",
      " private:\n",
      "  template <typename T>\n",
      "  const T& value_impl() const;\n",
      "\n",
      "  union {\n",
      "    float float_;\n",
      "    double double_;\n",
      "    Eigen::half half_;\n",
      "    int8 int8_;\n",
      "    int32 int32_;\n",
      "    std::complex<float> complex_float_;\n",
      "    std::complex<double> complex_double_;\n",
      "    DeviceMemoryBase pointer_;\n",
      "  };\n",
      "  bool is_pointer_;\n",
      "  DataType dtype_;\n",
      "};\n",
      "\n",
      "template <>\n",
      "inline const float& HostOrDeviceScalar<void>::value_impl<float>() const {\n",
      "  return float_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const double& HostOrDeviceScalar<void>::value_impl<double>() const {\n",
      "  return double_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const Eigen::half& HostOrDeviceScalar<void>::value_impl<Eigen::half>()\n",
      "    const {\n",
      "  return half_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const int8& HostOrDeviceScalar<void>::value_impl<int8>() const {\n",
      "  return int8_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const int32& HostOrDeviceScalar<void>::value_impl<int32>() const {\n",
      "  return int32_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const std::complex<float>&\n",
      "HostOrDeviceScalar<void>::value_impl<std::complex<float>>() const {\n",
      "  return complex_float_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const std::complex<double>&\n",
      "HostOrDeviceScalar<void>::value_impl<std::complex<double>>() const {\n",
      "  return complex_double_;\n",
      "}\n",
      "\n",
      "}  // namespace stream_executor\n",
      "#endif  // TENSORFLOW_STREAM_EXECUTOR_HOST_OR_DEVICE_SCALAR_H_\n",
      "\n",
      "\n",
      "Output: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/stream_executor/data_type.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for data type definitions used in the Stream Executor library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/stream_executor/device_memory.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for device memory management used in the Stream Executor library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/stream_executor/platform/logging.h\\\"\",\n",
      "        \"component_description\": \"Includes the logging platform header file, likely used for error checking and logging within the Stream Executor library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"HostOrDeviceScalar\",\n",
      "        \"component_code\": \"template <typename T> class HostOrDeviceScalar { ... };\",\n",
      "        \"component_description\": \"A template class representing a scalar value that can be either on the host or device, with constructors for both types of values and methods to retrieve them.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_CONSTRUCTOR\",\n",
      "        \"component_name\": \"HostOrDeviceScalar<T>\",\n",
      "        \"component_code\": \"explicit HostOrDeviceScalar(const DeviceMemory<T>& pointer)\",\n",
      "        \"component_description\": \"Constructor that initializes the scalar with a device memory pointer, ensuring it points to a single element.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"is_pointer\",\n",
      "        \"component_code\": \"bool is_pointer() const\",\n",
      "        \"component_description\": \"Returns true if the scalar value is stored as a device memory pointer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"pointer\",\n",
      "        \"component_code\": \"const DeviceMemory<T>& pointer() const\",\n",
      "        \"component_description\": \"Returns the device memory pointer to the scalar value, with a check that it is indeed a pointer and points to a single element.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"value\",\n",
      "        \"component_code\": \"const T& value() const\",\n",
      "        \"component_description\": \"Returns the host-stored scalar value, with a check that it is not stored as a device memory pointer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"HostOrDeviceScalar<void>\",\n",
      "        \"component_code\": \"template <> class HostOrDeviceScalar<void> { ... };\",\n",
      "        \"component_description\": \"A specialized template class for void type, allowing storage and retrieval of different scalar types with their corresponding data types.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_CONSTRUCTOR\",\n",
      "        \"component_name\": \"HostOrDeviceScalar<void>\",\n",
      "        \"component_code\": \"explicit HostOrDeviceScalar(const DeviceMemory<T>& pointer)\",\n",
      "        \"component_description\": \"Constructor that initializes the void-type scalar with a device memory pointer of any type, along with setting its data type.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_CONSTRUCTOR\",\n",
      "        \"component_name\": \"HostOrDeviceScalar<void>\",\n",
      "        \"component_code\": \"template <typename T> HostOrDeviceScalar(const HostOrDeviceScalar<T>& other)\",\n",
      "        \"component_description\": \"Constructor that initializes the void-type scalar from a statically-typed version of HostOrDeviceScalar.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"data_type\",\n",
      "        \"component_code\": \"DataType data_type() const\",\n",
      "        \"component_description\": \"Returns the data type of the stored scalar value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"value_impl\",\n",
      "        \"component_code\": \"template <typename T> const T& value_impl() const\",\n",
      "        \"component_description\": \"Template method to retrieve the host-stored scalar value of a specific type, used internally.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<float>\",\n",
      "        \"component_code\": \"template <> inline const float& HostOrDeviceScalar<void>::value_impl<float>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored float value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<double>\",\n",
      "        \"component_code\": \"template <> inline const double& HostOrDeviceScalar<void>::value_impl<double>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored double value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<Eigen::half>\",\n",
      "        \"component_code\": \"template <> inline const Eigen::half& HostOrDeviceScalar<void>::value_impl<Eigen::half>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored Eigen half value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<int8>\",\n",
      "        \"component_code\": \"template <> inline const int8& HostOrDeviceScalar<void>::value_impl<int8>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored int8 value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<int32>\",\n",
      "        \"component_code\": \"template <> inline const int32& HostOrDeviceScalar<void>::value_impl<int32>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored int32 value.\"\n",
      "      }\n",
      "    ]\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k, v in output.items():\n",
    "    print(f\"Input: \\n{v['input']}\\n\")\n",
    "    print(f\"Output: \\n{v['output']}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the output to a pickle file\n",
    "with open(\"data.pickle\", \"wb\") as file:\n",
    "    pickle.dump(output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded output from pickle file:\n",
      "Input 0: \n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_LITE_DELEGATES_GPU_GL_GL_TEXTURE_H_\n",
      "#define TENSORFLOW_LITE_DELEGATES_GPU_GL_GL_TEXTURE_H_\n",
      "\n",
      "#include \"absl/types/span.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/data_type.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/status.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/tensor.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/types.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/gl/gl_call.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/gl/portable_gl31.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace gpu {\n",
      "namespace gl {\n",
      "\n",
      "// Texture is an RAII wrapper for OpenGL texture object.\n",
      "// See https://www.khronos.org/opengl/wiki/Texture for more information.\n",
      "//\n",
      "// Texture is moveable but not copyable.\n",
      "class GlTexture {\n",
      " public:\n",
      "  // Creates invalid texture.\n",
      "  GlTexture()\n",
      "      : GlTexture(GL_INVALID_ENUM, GL_INVALID_INDEX, GL_INVALID_ENUM, 0, 0,\n",
      "                  false) {}\n",
      "\n",
      "  GlTexture(GLenum target, GLuint id, GLenum format, size_t bytes_size,\n",
      "            GLint layer, bool owned)\n",
      "      : id_(id),\n",
      "        target_(target),\n",
      "        format_(format),\n",
      "        bytes_size_(bytes_size),\n",
      "        layer_(layer),\n",
      "        owned_(owned) {}\n",
      "\n",
      "  // Move-only\n",
      "  GlTexture(GlTexture&& texture);\n",
      "  GlTexture& operator=(GlTexture&& texture);\n",
      "  GlTexture(const GlTexture&) = delete;\n",
      "  GlTexture& operator=(const GlTexture&) = delete;\n",
      "\n",
      "  ~GlTexture();\n",
      "\n",
      "  // Binds a texture as an image to the given index.\n",
      "  absl::Status BindAsReadonlyImage(uint32_t index) const;\n",
      "\n",
      "  // Bind texture as an image for write access at given index.\n",
      "  absl::Status BindAsWriteonlyImage(uint32_t index) const;\n",
      "\n",
      "  // Bind texture as an image for read-write access at given index.\n",
      "  absl::Status BindAsReadWriteImage(uint32_t index) const;\n",
      "\n",
      "  // Binds a texture as a sampler to the given index.\n",
      "  absl::Status BindAsSampler2D(uint32_t index) const;\n",
      "\n",
      "  GLenum target() const { return target_; }\n",
      "\n",
      "  GLuint id() const { return id_; }\n",
      "\n",
      "  GLenum format() const { return format_; }\n",
      "\n",
      "  GLint layer() const { return layer_; }\n",
      "\n",
      "  bool is_valid() const { return id_ != GL_INVALID_INDEX; }\n",
      "\n",
      "  size_t bytes_size() const { return bytes_size_; }\n",
      "\n",
      "  // @return true if this object actually owns corresponding GL buffer\n",
      "  //         and manages it's lifetime.\n",
      "  bool has_ownership() const { return owned_; }\n",
      "\n",
      " private:\n",
      "  void Invalidate();\n",
      "\n",
      "  absl::Status BindImage(uint32_t index, GLenum access) const;\n",
      "\n",
      "  GLuint id_;\n",
      "  GLenum target_;\n",
      "  GLenum format_;\n",
      "  size_t bytes_size_;\n",
      "  GLint layer_;\n",
      "  bool owned_;\n",
      "};\n",
      "\n",
      "// Creates new 2D image texture that will be filled with float32 data once which\n",
      "// will be used for reading.\n",
      "//\n",
      "// @param size defines 2D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTexture(const uint2& size,\n",
      "                                        absl::Span<const float> data,\n",
      "                                        GlTexture* gl_texture);\n",
      "\n",
      "// Creates new 2D image texture that will be filled with float16 data once which\n",
      "// will be used for reading.\n",
      "//\n",
      "// @param size defines 2D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTextureF16(const uint2& size,\n",
      "                                           absl::Span<const uint16_t> data,\n",
      "                                           GlTexture* gl_texture);\n",
      "\n",
      "// Creates new 2D image texture that will be filled with uint8 data once which\n",
      "// will be used for reading.\n",
      "//\n",
      "// @param size defines 2D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTextureU8(const uint2& size,\n",
      "                                          absl::Span<const uint8_t> data,\n",
      "                                          GlTexture* gl_texture);\n",
      "\n",
      "// Creates new 3D RGBA image texture that will be filled with float32 data once\n",
      "// which will be used for reading.\n",
      "//\n",
      "// @param size defines 3D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTexture(const uint3& size,\n",
      "                                        absl::Span<const float> data,\n",
      "                                        GlTexture* gl_texture);\n",
      "\n",
      "// Creates new 3D RGBA image texture that will be filled with float16 data once\n",
      "// which will be used for reading.\n",
      "//\n",
      "// @param size defines 3D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadOnlyImageTextureF16(const uint3& size,\n",
      "                                           absl::Span<const uint16_t> data,\n",
      "                                           GlTexture* gl_texture);\n",
      "\n",
      "// Creates new RGBA 2D image texture\n",
      "//\n",
      "// @param size defines 2D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadWriteRgbaImageTexture(DataType data_type,\n",
      "                                             const uint2& size,\n",
      "                                             GlTexture* gl_texture);\n",
      "\n",
      "// Creates new RGBA 3D image texture\n",
      "//\n",
      "// @param size defines 3D image texture size where each pixel is RGBA.\n",
      "absl::Status CreateReadWriteRgbaImageTexture(DataType data_type,\n",
      "                                             const uint3& size,\n",
      "                                             GlTexture* gl_texture);\n",
      "\n",
      "namespace gl_texture_internal {\n",
      "\n",
      "// RAII for creating and/or owning texture id.\n",
      "class TextureId {\n",
      " public:\n",
      "  TextureId() : id_(GL_INVALID_INDEX) {\n",
      "    TFLITE_GPU_CALL_GL(glGenTextures, 1 /* number of textures*/, &id_)\n",
      "        .IgnoreError();\n",
      "  }\n",
      "\n",
      "  explicit TextureId(GLuint id) : id_(id) {}\n",
      "\n",
      "  ~TextureId() {\n",
      "    if (id_ != GL_INVALID_INDEX) {\n",
      "      TFLITE_GPU_CALL_GL(glDeleteTextures, 1, &id_).IgnoreError();\n",
      "    }\n",
      "  }\n",
      "\n",
      "  GLuint id() const { return id_; }\n",
      "\n",
      "  GLuint Release() {\n",
      "    GLuint id = GL_INVALID_INDEX;\n",
      "    std::swap(id, id_);\n",
      "    return id;\n",
      "  }\n",
      "\n",
      " private:\n",
      "  GLuint id_;\n",
      "};\n",
      "\n",
      "// RAII for binding and unbinding a texture.\n",
      "class TextureBinder {\n",
      " public:\n",
      "  TextureBinder(GLenum target, GLuint id) : target_(target) {\n",
      "    TFLITE_GPU_CALL_GL(glBindTexture, target_, id).IgnoreError();\n",
      "  }\n",
      "\n",
      "  ~TextureBinder() {\n",
      "    TFLITE_GPU_CALL_GL(glBindTexture, target_, 0).IgnoreError();\n",
      "  }\n",
      "\n",
      " private:\n",
      "  const GLenum target_;\n",
      "};\n",
      "\n",
      "}  // namespace gl_texture_internal\n",
      "}  // namespace gl\n",
      "}  // namespace gpu\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_DELEGATES_GPU_GL_GL_TEXTURE_H_\n",
      "\n",
      "\n",
      "Output 0: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/types/span.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl::Span library for handling array views in a type-safe and efficient manner.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/delegates/gpu/common/data_type.h\\\"\",\n",
      "        \"component_description\": \"Includes the data_type definitions used by TensorFlow Lite GPU delegate for specifying tensor data types.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/delegates/gpu/common/tensor.h\\\"\",\n",
      "        \"component_description\": \"Includes the common tensor definitions used by TensorFlow Lite GPU delegate for handling tensors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/delegates/gpu/gl/gl_call.h\\\"\",\n",
      "        \"component_description\": \"Includes the OpenGL call wrapper utility used to ensure proper error checking and logging of OpenGL API calls.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"GlTexture\",\n",
      "        \"component_code\": \"class GlTexture { ... };\",\n",
      "        \"component_description\": \"Defines a RAII class for managing OpenGL texture resources, including creation, binding, and deletion. It provides methods to bind the texture as an image with specified access rights and checks ownership of the underlying GL buffer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CreateReadOnlyImageTexture\",\n",
      "        \"component_code\": \"absl::Status CreateReadOnlyImageTexture(const uint2& size, absl::Span<const float> data, GlTexture* gl_texture);\",\n",
      "        \"component_description\": \"Function to create a new 2D image texture filled with float32 data for read-only usage.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CreateReadOnlyImageTextureF16\",\n",
      "        \"component_code\": \"absl::Status CreateReadOnlyImageTextureF16(const uint2& size, absl::Span<const uint16_t> data, GlTexture* gl_texture);\",\n",
      "        \"component_description\": \"Function to create a new 2D image texture filled with float16 data for read-only usage.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CreateReadOnlyImageTextureU8\",\n",
      "        \"component_code\": \"absl::Status CreateReadOnlyImageTextureU8(const uint2& size, absl::Span<const uint8_t> data, GlTexture* gl_texture);\",\n",
      "        \"component_description\": \"Function to create a new 2D image texture filled with uint8 data for read-only usage.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CreateReadWriteRgbaImageTexture\",\n",
      "        \"component_code\": \"absl::Status CreateReadWriteRgbaImageTexture(DataType data_type, const uint2& size, GlTexture* gl_texture);\",\n",
      "        \"component_description\": \"Function to create a new 2D RGBA image texture that can be used for both reading and writing.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"TextureId\",\n",
      "        \"component_code\": \"class TextureId { ... };\",\n",
      "        \"component_description\": \"Defines an RAII class for managing OpenGL texture IDs. It generates a new texture ID on construction and deletes it when the object is destroyed.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"TextureBinder\",\n",
      "        \"component_code\": \"class TextureBinder { ... };\",\n",
      "        \"component_description\": \"Defines an RAII class for binding and unbinding OpenGL textures. It binds the specified texture on construction and unbinds it when the object is destroyed.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 1: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_LITE_KERNELS_SHIM_TENSOR_VIEW_H_\n",
      "#define TENSORFLOW_LITE_KERNELS_SHIM_TENSOR_VIEW_H_\n",
      "\n",
      "#include \"absl/status/statusor.h\"\n",
      "#include \"absl/strings/string_view.h\"\n",
      "#include \"absl/types/span.h\"\n",
      "#include \"absl/types/variant.h\"\n",
      "#include \"tensorflow/core/platform/logging.h\"\n",
      "#include \"tensorflow/core/platform/tstring.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace shim {\n",
      "\n",
      "// A type deduction template which is specialized for TF and TFLite.\n",
      "// That is it maps\n",
      "//   ::tensorflow::Tensor -> tflite::shim::TfTensorView\n",
      "//   ::TfLiteTensor -> tflite::shim::TfLiteTensorView\n",
      "template <typename W>\n",
      "struct TensorViewSubType {};\n",
      "\n",
      "// Common denominator for ::tflite::TfLiteTensor and ::tensorflow::Tensor.\n",
      "// It is a \"view\" over the underlying tensor without taking ownership.\n",
      "// Objects of this class can also mutate the underlying tensor depending on\n",
      "// whether the underlying tensor is \"const\" qualified or not.\n",
      "//\n",
      "// Movable and copyable.\n",
      "// It can be instantiated with the New() factory function. eg.\n",
      "//   TfTensorView t           = TensorView::New(&tf_tensor);\n",
      "//   const TfTensorView t     = TensorView::New(&const_tf_tensor);\n",
      "//   TfLiteTensorView t       = TensorView::New(&tflite_tensor);\n",
      "//   const TfLiteTensorView t = TensorView::New(&const_tflite_tensor);\n",
      "class TensorView {\n",
      " protected:\n",
      "  // Union over all data types\n",
      "  using DataVariantType =\n",
      "      absl::variant<absl::Span<bool>, absl::Span<uint8_t>, absl::Span<uint64_t>,\n",
      "                    absl::Span<int8_t>, absl::Span<int16_t>,\n",
      "                    absl::Span<int32_t>, absl::Span<int64_t>, absl::Span<float>,\n",
      "                    absl::Span<double>, absl::Span<::tensorflow::tstring>>;\n",
      "\n",
      "  // An interface while provides convenient row-major indexing over the\n",
      "  // underlying tensor.\n",
      "  // Example usage:\n",
      "  //\n",
      "  //   // A scalar view\n",
      "  //   const TensorView t_float\n",
      "  //   float val = t_float.AsScalar<float>();\n",
      "  //\n",
      "  //   // A vector view\n",
      "  //   const TensorView t_int;\n",
      "  //   auto t_int_vec = t_int.As<int32_t, /*RANK=*/ 1>();\n",
      "  //   int sum = t_int_vec(0) + t_int_vec(1);\n",
      "  //\n",
      "  //   // A matrix view\n",
      "  //   TensorView t_str;\n",
      "  //   auto t_str_mat = t_str.As<tensorflow::tstring, /*RANK=*/ 2>();\n",
      "  //   t_str_mat(0, 0) = \"abc\";\n",
      "  //   t_str_mat(2, 3) = \"def\";\n",
      "  template <typename DType, int RANK>\n",
      "  class Tensor {\n",
      "   public:\n",
      "    explicit Tensor(TensorView *t)\n",
      "        : data_(t->Data<DType>()), shape_(t->Shape()) {\n",
      "      DCHECK_EQ(RANK, shape_.size());\n",
      "      ComputeRowSizes();\n",
      "    }\n",
      "\n",
      "    explicit Tensor(const TensorView *t)\n",
      "        : data_(t->Data<DType>()), shape_(t->Shape()) {\n",
      "      DCHECK_EQ(RANK, shape_.size());\n",
      "      ComputeRowSizes();\n",
      "    }\n",
      "\n",
      "    // indexing operator\n",
      "    template <typename... IndexTypes>\n",
      "    inline DType &operator()(IndexTypes... indices) {\n",
      "      const auto idx = RowMajorIndex(std::array<int, RANK>{{indices...}});\n",
      "      return data_[idx];\n",
      "    }\n",
      "\n",
      "    // const indexing operator\n",
      "    template <typename... IndexTypes>\n",
      "    inline const DType &operator()(IndexTypes... indices) const {\n",
      "      const auto idx = RowMajorIndex(std::array<int, RANK>{{indices...}});\n",
      "      return data_.at(idx);\n",
      "    }\n",
      "\n",
      "    // Pointer accessor\n",
      "    typename absl::Span<DType>::pointer Ptr() { return data_.data(); }\n",
      "    constexpr typename absl::Span<DType>::const_pointer Ptr() const {\n",
      "      return data_.data();\n",
      "    }\n",
      "\n",
      "    // Size of the given dimension\n",
      "    inline int Dim(int dim_i) const {\n",
      "      DCHECK(RANK > 0 && dim_i < RANK) << \"dim: \" << dim_i << \" rank:\" << RANK;\n",
      "      // Handle negative indices\n",
      "      if (dim_i < 0) dim_i = ((dim_i % RANK) + RANK) % RANK;\n",
      "      return shape_[dim_i];\n",
      "    }\n",
      "\n",
      "    // The tensor's rank: number of dimensions\n",
      "    /*[[nodiscard]]*/ constexpr std::size_t Rank() const { return RANK; }\n",
      "\n",
      "   private:\n",
      "    // Computes the row-major index\n",
      "    inline std::size_t RowMajorIndex(\n",
      "        const std::array<int, RANK> &indices) const {\n",
      "      std::size_t ret = 0;\n",
      "      for (int i = 0; i < RANK; ++i) ret += indices[i] * row_sizes_[i];\n",
      "      return ret;\n",
      "    }\n",
      "\n",
      "    // Pre computes row sizes to convert multi dim indices into a row major\n",
      "    // index\n",
      "    void ComputeRowSizes() {\n",
      "      // Precompute row sizes for row major index computation\n",
      "      if (RANK > 0) {\n",
      "        row_sizes_[RANK - 1] = 1;\n",
      "        for (int i = RANK - 2; i >= 0; --i) {\n",
      "          row_sizes_[i] = row_sizes_[i + 1] * shape_[i + 1];\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "\n",
      "    absl::Span<DType> data_;\n",
      "    const absl::Span<int> shape_;\n",
      "    std::size_t row_sizes_[RANK]{};\n",
      "  };\n",
      "\n",
      " public:\n",
      "  // Factory which gets specialized for different wrapped tensor types.\n",
      "  template <typename W>\n",
      "  static absl::StatusOr<typename TensorViewSubType<W>::Type> New(\n",
      "      W *wrapped_tensor);\n",
      "\n",
      " protected:\n",
      "  // Move constructor\n",
      "  TensorView(TensorView &&o) = default;\n",
      "  // Copy constructor\n",
      "  TensorView(const TensorView &o) = default;\n",
      "  // Move assignment operator\n",
      "  TensorView &operator=(TensorView &&o) = default;\n",
      "  // Copy assignment operator\n",
      "  TensorView &operator=(const TensorView &) = default;\n",
      "\n",
      " public:\n",
      "  // Dtor\n",
      "  virtual ~TensorView() = default;\n",
      "\n",
      "  // Accessors\n",
      "\n",
      "  // Shape\n",
      "  absl::Span<int> Shape() { return shape_; }\n",
      "  /*[[nodiscard]]*/ const absl::Span<int> Shape() const { return shape_; }\n",
      "\n",
      "  // Data\n",
      "  template <typename DType>\n",
      "  absl::Span<DType> &Data() {\n",
      "    return absl::get<absl::Span<DType>>(data_);\n",
      "  }\n",
      "  template <typename DType>\n",
      "  constexpr absl::Span<DType> Data() const {\n",
      "    return absl::get<absl::Span<DType>>(data_);\n",
      "  }\n",
      "\n",
      "  // Reads the tensor given the dtype and its rank and provides an indexing\n",
      "  // operator.\n",
      "  template <typename DType, int RANK>\n",
      "  Tensor<DType, RANK> As() {\n",
      "    return Tensor<DType, RANK>(this);\n",
      "  }\n",
      "\n",
      "  // Const version of As()\n",
      "  template <typename DType, int RANK>\n",
      "  const Tensor<DType, RANK> As() const {\n",
      "    return Tensor<DType, RANK>(this);\n",
      "  }\n",
      "\n",
      "  // Read the given tensor as a scalar or return error if it isn't\n",
      "  template <typename DType>\n",
      "  DType &AsScalar();\n",
      "\n",
      "  template <typename DType>\n",
      "  const DType &AsScalar() const;\n",
      "\n",
      " protected:\n",
      "  // Templated constructor. Since it's not possible to specify the template\n",
      "  // argument directly we place a dummy argument of that type so compiler\n",
      "  // can deduce the right template parameter\n",
      "  template <typename DType>\n",
      "  TensorView(const absl::Span<int> shape, void *data,\n",
      "             const std::size_t data_size, const DType &)\n",
      "      : shape_(shape),\n",
      "        data_(absl::Span<DType>(reinterpret_cast<DType *>(data),\n",
      "                                data_size / sizeof(DType))) {}\n",
      "\n",
      "  // Return the total number of elements given the shape.\n",
      "  static constexpr std::size_t NumElements(const absl::Span<int> shape) {\n",
      "    std::size_t ret = 1;\n",
      "    for (const auto dim : shape) ret *= dim;\n",
      "    return ret;\n",
      "  }\n",
      "\n",
      "  // Tensor shape\n",
      "  // Note: using int rather than size_t to avoid conversion to from TfLite shape\n",
      "  absl::Span<int> shape_;\n",
      "  // Tensor data\n",
      "  DataVariantType data_;\n",
      "};\n",
      "\n",
      "// Add or remove const qualifier to O based on whether it is in I.\n",
      "// For example\n",
      "//   MatchConstNess<const TfLiteTensor, TensorView>::Type == const TensorView\n",
      "//   MatchConstNess<TfLiteTensor, TensorView>::Type == TensorView\n",
      "//   MatchConstNess<TfLiteTensor, const TensorView>::Type == TensorView\n",
      "template <typename I, typename O>\n",
      "struct MatchConstNess {\n",
      "  using Type = std::conditional_t<std::is_const<I>::value, std::add_const_t<O>,\n",
      "                                  std::remove_const_t<O>>;\n",
      "};\n",
      "\n",
      "///////////////////////////// Implementation\n",
      "\n",
      "template <typename DType>\n",
      "DType &TensorView::AsScalar() {\n",
      "  DCHECK_EQ(shape_.size(), 0) << \"Tensor is not a scalar\";\n",
      "  return Data<DType>()[0];\n",
      "}\n",
      "\n",
      "template <typename DType>\n",
      "const DType &TensorView::AsScalar() const {\n",
      "  DCHECK_EQ(shape_.size(), 0) << \"Tensor is not a scalar\";\n",
      "  return Data<DType>().at(0);\n",
      "}\n",
      "\n",
      "}  // namespace shim\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_KERNELS_SHIM_TENSOR_VIEW_H_\n",
      "\n",
      "\n",
      "Output 1: \n",
      "The provided code snippet defines a C++ class template `TensorView` designed to provide a generic interface for accessing and manipulating tensor data, which can be wrapped around different types of tensors such as those from TensorFlow Lite or other frameworks. Here is a detailed breakdown of the components and functionalities:\n",
      "\n",
      "### Class Template: `TensorView`\n",
      "\n",
      "#### Overview\n",
      "- **Purpose**: To abstract away the specifics of underlying tensor representations (e.g., TensorFlow Lite tensors) while providing common access patterns.\n",
      "- **Features**:\n",
      "  - Supports dynamic data types using `absl::Span` and `std::variant`.\n",
      "  - Provides shape information and multi-dimensional indexing via the nested `Tensor` class.\n",
      "  - Facilitates scalar access with error checking.\n",
      "\n",
      "#### Key Components\n",
      "\n",
      "1. **Nested Class: `Tensor`**\n",
      "   - Purpose: To provide a higher-level interface for accessing tensor elements using multi-dimensional indices.\n",
      "   - Features:\n",
      "     - Supports arbitrary rank (dimensionality) of tensors.\n",
      "     - Pre-computes row sizes to efficiently convert multi-dimensional indices into flat (row-major) indices.\n",
      "\n",
      "2. **Factory Method: `New`**\n",
      "   - Purpose: To create specialized instances of `TensorView` for different wrapped tensor types.\n",
      "   - Usage:\n",
      "     ```cpp\n",
      "     auto tensor_view = TensorView::New(wrapped_tensor);\n",
      "     ```\n",
      "\n",
      "3. **Data Storage and Access**\n",
      "   - Uses `absl::Span<int>` to store the shape of the tensor.\n",
      "   - Employs a `std::variant` (`DataVariantType`) to hold data spans of different types (e.g., `float`, `int`).\n",
      "   - Provides templated methods `Data<DType>()` for accessing the underlying data span.\n",
      "\n",
      "4. **Shape Accessor**\n",
      "   - Returns the shape of the tensor as an `absl::Span<int>`.\n",
      "\n",
      "5. **Scalar Access Methods**\n",
      "   - Methods `AsScalar<DType>()` and their const counterparts allow reading a tensor's value when it is guaranteed to be a scalar (i.e., has no dimensions).\n",
      "\n",
      "6. **Protected Members and Constructors**\n",
      "   - Includes move/copy constructors and assignment operators.\n",
      "   - Uses a templated protected constructor to initialize the `TensorView` with shape, data pointer, and element count.\n",
      "\n",
      "7. **Helper Struct: `MatchConstNess`**\n",
      "   - Purpose: To match the const-ness of types `I` and `O`. This is useful when creating specialized instances of `TensorView`.\n",
      "   - Usage:\n",
      "     ```cpp\n",
      "     using ResultType = MatchConstNess<const SomeType, TensorView>::Type;\n",
      "     // ResultType will be `const TensorView`\n",
      "     ```\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "Here's an example demonstrating how you might use the `TensorView` class:\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include \"tensorflow/lite/kernels/shim/tensor_view.h\"\n",
      "\n",
      "// Assuming we have a wrapped tensor type, e.g., TfLiteTensor\n",
      "struct WrappedTensor {\n",
      "    int* shape;\n",
      "    size_t num_dimensions;\n",
      "    void* data;\n",
      "    size_t bytes;\n",
      "};\n",
      "\n",
      "namespace tflite {\n",
      "namespace shim {\n",
      "\n",
      "template <>\n",
      "struct TensorViewSubType<WrappedTensor> {\n",
      "    using Type = TensorView;\n",
      "};\n",
      "\n",
      "absl::StatusOr<typename TensorViewSubType<WrappedTensor>::Type> TensorView::New(WrappedTensor* wrapped_tensor) {\n",
      "    std::vector<int> shape(wrapped_tensor->num_dimensions);\n",
      "    for (size_t i = 0; i < wrapped_tensor->num_dimensions; ++i) {\n",
      "        shape[i] = wrapped_tensor->shape[i];\n",
      "    }\n",
      "    return TensorView(absl::Span<int>(shape.data(), shape.size()),\n",
      "                      wrapped_tensor->data,\n",
      "                      wrapped_tensor->bytes,\n",
      "                      std::integral_constant<float, 0>{}); // Assuming float data type for simplicity\n",
      "}\n",
      "\n",
      "}  // namespace shim\n",
      "}  // namespace tflite\n",
      "\n",
      "int main() {\n",
      "    WrappedTensor tensor = {/* Initialize with actual values */};\n",
      "\n",
      "    auto status_or_tensor_view = tflite::shim::TensorView::New(&tensor);\n",
      "    if (!status_or_tensor_view.ok()) {\n",
      "        std::cerr << \"Failed to create TensorView: \" << status_or_tensor_view.status().ToString() << \"\\n\";\n",
      "        return 1;\n",
      "    }\n",
      "\n",
      "    tflite::shim::TensorView& tensor_view = *status_or_tensor_view;\n",
      "\n",
      "    // Access the shape\n",
      "    auto shape = tensor_view.Shape();\n",
      "    std::cout << \"Shape: \";\n",
      "    for (int dim : shape) {\n",
      "        std::cout << dim << \" \";\n",
      "    }\n",
      "    std::cout << \"\\n\";\n",
      "\n",
      "    // Access a scalar value if it is a 0-dimensional tensor\n",
      "    try {\n",
      "        float& scalar_value = tensor_view.AsScalar<float>();\n",
      "        std::cout << \"Scalar Value: \" << scalar_value << \"\\n\";\n",
      "    } catch (const std::exception& e) {\n",
      "        std::cerr << \"Error accessing scalar: \" << e.what() << \"\\n\";\n",
      "    }\n",
      "\n",
      "    // Access elements using the Tensor class\n",
      "    tflite::shim::TensorView::Tensor tensor_access = tensor_view.Tensor();\n",
      "    float value = tensor_access(0, 1); // Example multi-dimensional access\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Notes\n",
      "- The example assumes that `WrappedTensor` is a struct or class representing the underlying tensor data.\n",
      "- Specialization of `TensorViewSubType` for `WrappedTensor` is required to indicate which type should be returned by `New`.\n",
      "- Error handling with `absl::StatusOr` ensures robustness when creating and using `TensorView` instances.\n",
      "\n",
      "This design allows for flexible and type-safe interaction with tensors, making it easier to integrate different tensor representations into a unified interface.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 2: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\n",
      "#define TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\n",
      "\n",
      "#include \"tensorflow/core/platform/types.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace generator {\n",
      "namespace cpp {\n",
      "\n",
      "struct CppConfig {\n",
      "  string category;\n",
      "  string unit;\n",
      "  std::vector<string> namespaces;\n",
      "\n",
      "  explicit CppConfig() = default;\n",
      "  explicit CppConfig(const string &category,\n",
      "                     const string &name_space = \"tensorflow::ops\");\n",
      "};\n",
      "\n",
      "}  // namespace cpp\n",
      "}  // namespace generator\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\n",
      "\n",
      "\n",
      "Output 2: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\\n#define TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\\n#endif  // TENSORFLOW_C_EXPERIMENTAL_OPS_GEN_CPP_CPP_CONFIG_H_\",\n",
      "        \"component_description\": \"A header guard to prevent multiple inclusions of the header file, ensuring that the contents are only processed once during compilation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/core/platform/types.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/types.h\\\"\",\n",
      "        \"component_description\": \"Includes a header file from TensorFlow's core platform that likely defines fundamental types used in the codebase.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\\n}\",\n",
      "        \"component_description\": \"Defines the 'tensorflow' namespace, encapsulating all subsequent declarations within it to avoid name collisions with other libraries or user code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"generator\",\n",
      "        \"component_code\": \"namespace generator {\\n}\",\n",
      "        \"component_description\": \"Defines the 'generator' namespace within 'tensorflow', further encapsulating declarations related to code generation functionalities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"cpp\",\n",
      "        \"component_code\": \"namespace cpp {\\n}\",\n",
      "        \"component_description\": \"Defines the 'cpp' namespace within 'tensorflow::generator', specifically for C++-related code generation functionalities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"STRUCT_DEFINITION\",\n",
      "        \"component_name\": \"CppConfig\",\n",
      "        \"component_code\": \"struct CppConfig {\\n  string category;\\n  string unit;\\n  std::vector<string> namespaces;\\n\\n  explicit CppConfig() = default;\\n  explicit CppConfig(const string &category,\\n                     const string &name_space = \\\"tensorflow::ops\\\");\\n};\",\n",
      "        \"component_description\": \"Defines a struct named 'CppConfig' within the 'tensorflow::generator::cpp' namespace. It contains three members: 'category', 'unit', and 'namespaces'. The struct provides two constructors: a default constructor that initializes the object with default values, and another constructor that allows setting initial values for 'category' and optionally 'name_space'.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file defines a configuration structure named 'CppConfig' within the 'tensorflow::generator::cpp' namespace. The purpose of this structure is to hold settings related to C++ code generation, such as category, unit, and namespaces. It includes an explicit default constructor and another constructor for initializing these settings. The use of namespaces helps in organizing the code and avoiding name conflicts with other parts of the TensorFlow library or external libraries.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 3: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_KERNELS_SPARSE_REORDER_OP_H_\n",
      "#define TENSORFLOW_CORE_KERNELS_SPARSE_REORDER_OP_H_\n",
      "\n",
      "#include \"tensorflow/core/framework/op_kernel.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "namespace functor {\n",
      "\n",
      "template <typename Device, typename T>\n",
      "struct SparseReorderFunctor {\n",
      "  void operator()(OpKernelContext* context, const Tensor& input_ind,\n",
      "                  const Tensor& input_val, const Tensor& input_shape_in);\n",
      "};\n",
      "\n",
      "}  // namespace functor\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_KERNELS_SPARSE_REORDER_OP_H_\n",
      "\n",
      "\n",
      "Output 3: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/framework/op_kernel.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for OpKernel, which is a base class for TensorFlow operations that operate on Tensors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow { ... }\",\n",
      "        \"component_description\": \"Defines the 'tensorflow' namespace to encapsulate all components related to TensorFlow's core functionalities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"functor\",\n",
      "        \"component_code\": \"namespace functor { ... }\",\n",
      "        \"component_description\": \"Defines a nested 'functor' namespace within the 'tensorflow' namespace, typically used for defining function objects or functors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"TEMPLATE_STRUCT_DEFINITION\",\n",
      "        \"component_name\": \"SparseReorderFunctor\",\n",
      "        \"component_code\": \"template <typename Device, typename T> struct SparseReorderFunctor { ... };\",\n",
      "        \"component_description\": \"Defines a template struct named 'SparseReorderFunctor' that is parameterized by 'Device' and 'T'. This functor is designed to reorder sparse data structures in TensorFlow operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"operator()\",\n",
      "        \"component_code\": \"void operator()(OpKernelContext* context, const Tensor& input_ind, const Tensor& input_val, const Tensor& input_shape_in);\",\n",
      "        \"component_description\": \"Defines the function call operator for 'SparseReorderFunctor'. This operator takes an OpKernelContext pointer and three constant Tensors as parameters, which represent indices, values, and shape of a sparse tensor respectively. It is used to reorder these sparse data components.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"The provided C++ code snippet defines a header file for TensorFlow that introduces a template struct named 'SparseReorderFunctor'. This functor is designed to handle the reordering of sparse tensors within TensorFlow operations. The code includes necessary headers, uses namespaces to avoid name clashes, and outlines the functionality through a function call operator that takes a context and three tensor inputs.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 4: \n",
      "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_CORE_COMMON_RUNTIME_RING_REDUCER_H_\n",
      "#define TENSORFLOW_CORE_COMMON_RUNTIME_RING_REDUCER_H_\n",
      "\n",
      "#include <deque>\n",
      "#include <memory>\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"tensorflow/core/common_runtime/base_collective_executor.h\"\n",
      "#include \"tensorflow/core/common_runtime/ring_alg.h\"\n",
      "#include \"tensorflow/core/framework/collective.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "class Device;\n",
      "\n",
      "// Ring-algorithm implementation of collective all-reduce.\n",
      "class RingReducer : public RingAlg {\n",
      " public:\n",
      "  RingReducer() : RingAlg(REDUCTION_COLLECTIVE, \"Reduce\") {}\n",
      "  ~RingReducer() override;\n",
      "\n",
      "  // Begins async execution of the ring reduce algorithm.\n",
      "  // Must be called in a blockable thread.\n",
      "  // TODO(b/80529858): remove the previous warning when we have a dedicated\n",
      "  // collective threadpool.\n",
      "  void Run(StatusCallback done) override;\n",
      "\n",
      "  Status InitializeCollectiveParams(CollectiveParams* col_params) override;\n",
      "\n",
      " protected:\n",
      "  void InitRingField(RingField* rf, int chunk_idx, int subdiv_idx,\n",
      "                     int field_idx) override;\n",
      "\n",
      " private:\n",
      "  void ContinueAfterInputCopy();\n",
      "  bool RunAsyncParts();\n",
      "\n",
      "  Tensor group_size_tensor_;\n",
      "  Notification group_size_tensor_ready_;\n",
      "\n",
      "  friend class RingReducerTest;\n",
      "  friend class RingReducerInitParamsTest;\n",
      "};\n",
      "\n",
      "}  // namespace tensorflow\n",
      "#endif  // TENSORFLOW_CORE_COMMON_RUNTIME_RING_REDUCER_H_\n",
      "\n",
      "\n",
      "Output 4: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <deque>\\n#include <memory>\\n#include <string>\\n#include <vector>\",\n",
      "        \"component_description\": \"Includes standard C++ library headers for data structures and memory management.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/common_runtime/base_collective_executor.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for the BaseCollectiveExecutor class, which is a part of TensorFlow's collective runtime.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/common_runtime/ring_alg.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for the RingAlg class, which provides an interface for ring-based algorithms in TensorFlow's collective runtime.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/framework/collective.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for collective operations framework definitions, which are used for synchronization and communication among devices in TensorFlow.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\",\n",
      "        \"component_description\": \"Declares a namespace named 'tensorflow' to encapsulate the code within it, avoiding name collisions with other libraries and namespaces.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"RingReducer\",\n",
      "        \"component_code\": \"class RingReducer : public RingAlg {\\n public:\\n  RingReducer() : RingAlg(REDUCTION_COLLECTIVE, \\\"Reduce\\\") {}\\n  ~RingReducer() override;\\n\\n  // Begins async execution of the ring reduce algorithm.\\n  // Must be called in a blockable thread.\\n  // TODO(b/80529858): remove the previous warning when we have a dedicated\\n  // collective threadpool.\\n  void Run(StatusCallback done) override;\\n\\n  Status InitializeCollectiveParams(CollectiveParams* col_params) override;\\n\\n protected:\\n  void InitRingField(RingField* rf, int chunk_idx, int subdiv_idx,\\n                     int field_idx) override;\\n\\n private:\\n  void ContinueAfterInputCopy();\\n  bool RunAsyncParts();\\n\\n  Tensor group_size_tensor_;\\n  Notification group_size_tensor_ready_;\\n\\n  friend class RingReducerTest;\\n  friend class RingReducerInitParamsTest;\\n};\",\n",
      "        \"component_description\": \"Defines a class 'RingReducer' that inherits from 'RingAlg'. It provides an implementation of the ring-based all-reduce algorithm. The class contains constructors, destructors, public methods for running and initializing collective operations, protected methods for initializing ring fields, private helper methods for asynchronous execution, and member variables for managing group size tensors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_CLOSURE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"}  // namespace tensorflow\",\n",
      "        \"component_description\": \"Closes the 'tensorflow' namespace.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"The provided C++ code defines a class named RingReducer within the TensorFlow framework. This class is designed to implement an all-reduce algorithm using a ring-based approach, which is essential for efficient communication and synchronization among multiple devices during distributed machine learning tasks. The class inherits from RingAlg, providing specific functionalities such as asynchronous execution of the ring reduce algorithm, initialization of collective parameters, and management of tensor data related to group sizes.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 5: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_CC_EXPERIMENTAL_LIBEXPORT_SAVE_H_\n",
      "#define TENSORFLOW_CC_EXPERIMENTAL_LIBEXPORT_SAVE_H_\n",
      "\n",
      "#include <string>\n",
      "\n",
      "#include \"tensorflow/core/platform/status.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace libexport {\n",
      "\n",
      "// Writes a saved model to disk.\n",
      "//\n",
      "// Writes a saved model to the given `export_dir`.\n",
      "TF_EXPORT Status Save(const std::string& export_dir);\n",
      "\n",
      "}  // namespace libexport\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CC_EXPERIMENTAL_EXPORT_EXPORT_H_\n",
      "\n",
      "\n",
      "Output 5: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"string\",\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the C++ Standard Library string header, providing facilities to manipulate strings.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/core/platform/status.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/status.h\\\"\",\n",
      "        \"component_description\": \"Includes a TensorFlow-specific header that defines the Status class used for error handling in TensorFlow operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow { ... }\",\n",
      "        \"component_description\": \"Defines a namespace named 'tensorflow' to encapsulate code related to TensorFlow functionalities, avoiding name collisions with other libraries.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"libexport\",\n",
      "        \"component_code\": \"namespace libexport { ... }\",\n",
      "        \"component_description\": \"Defines a nested namespace named 'libexport' within the 'tensorflow' namespace to encapsulate code related to exporting functionalities of TensorFlow.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"Save\",\n",
      "        \"component_code\": \"TF_EXPORT Status Save(const std::string& export_dir);\",\n",
      "        \"component_description\": \"Declares a function named 'Save' that takes a constant reference to a string representing the export directory. It returns a Status object indicating the result of writing a saved model to the specified directory.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This C++ header file defines an interface for exporting TensorFlow models. It includes necessary headers, encapsulates the code within 'tensorflow::libexport' namespaces, and declares a function named 'Save'. The 'Save' function is designed to write a saved model to a specified directory on disk and returns a Status object to indicate the success or failure of the operation.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 6: \n",
      "<gh_stars>1000+\n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_LITE_EXPERIMENTAL_DELEGATES_COREML_BUILDERS_CONVOLUTION_OP_BUILDER_H_\n",
      "#define TENSORFLOW_LITE_EXPERIMENTAL_DELEGATES_COREML_BUILDERS_CONVOLUTION_OP_BUILDER_H_\n",
      "\n",
      "#include \"tensorflow/lite/builtin_ops.h\"\n",
      "#include \"tensorflow/lite/c/builtin_op_data.h\"\n",
      "#include \"tensorflow/lite/c/common.h\"\n",
      "#include \"tensorflow/lite/delegates/coreml/builders/op_builder.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace delegates {\n",
      "namespace coreml {\n",
      "\n",
      "enum class ConvolutionType { kConv, kDepthwiseConv, kTransposeConv };\n",
      "\n",
      "// Layer that provides convolution and depthwise convolution.\n",
      "class ConvolutionOpBuilder : public OpBuilder {\n",
      " public:\n",
      "  explicit ConvolutionOpBuilder(GraphBuilder* graph_builder,\n",
      "                                ConvolutionType conv_type)\n",
      "      : OpBuilder(graph_builder), conv_type_(conv_type) {}\n",
      "\n",
      "  const std::string& DebugName() override;\n",
      "\n",
      "  CoreML::Specification::NeuralNetworkLayer* Build() override;\n",
      "\n",
      "  TfLiteStatus PopulateSubgraph(TfLiteContext* context) override;\n",
      "\n",
      "  void SetOutputChannels(uint64_t output_channels);\n",
      "\n",
      "  void SetNGroups(uint64_t n_groups);\n",
      "\n",
      "  void SetWeights(TfLiteTensor* weights);\n",
      "\n",
      "  void SetBias(TfLiteTensor* bias);\n",
      "\n",
      "  void SetOutputShape(TfLiteTensor* output_shape);\n",
      "\n",
      "  void SetParams(void* builtin_data);\n",
      "\n",
      "  TfLiteStatus RegisterInputs(const TfLiteIntArray* inputs,\n",
      "                              TfLiteContext* context) override;\n",
      "\n",
      "  TfLiteStatus RegisterOutputs(const TfLiteIntArray* outputs,\n",
      "                               TfLiteContext* context) override;\n",
      "\n",
      " private:\n",
      "  void FillCoreMLWeights();\n",
      "  void FillCoreMLBias();\n",
      "\n",
      "  // Transpose TFLite kernel weights to CoreML kernel weights.\n",
      "  // Should be called after setting CoreML's kernel shapes.\n",
      "  void TransposeKernelWeights();\n",
      "\n",
      "  uint64_t output_channels_;\n",
      "  uint64_t n_groups_ = 1;\n",
      "\n",
      "  ConvolutionType conv_type_;\n",
      "\n",
      "  // using default dilation_factor (1, 1)\n",
      "  // CoreML ConvolutionLayerParams.isDeconvolution == false\n",
      "  TfLiteTensor* weights_ = nullptr;\n",
      "  TfLiteTensor* bias_ = nullptr;\n",
      "  // Only used for TransposeConv.\n",
      "  TfLiteTensor* output_shape_ = nullptr;\n",
      "};\n",
      "\n",
      "}  // namespace coreml\n",
      "}  // namespace delegates\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_EXPERIMENTAL_DELEGATES_COREML_BUILDERS_CONVOLUTION_OP_BUILDER_H_\n",
      "\n",
      "\n",
      "Output 6: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/builtin_ops.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file that defines constants and data structures for TensorFlow Lite's built-in operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/c/builtin_op_data.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file that provides C-style data structures for built-in operator data in TensorFlow Lite.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/c/common.h\\\"\",\n",
      "        \"component_description\": \"Includes the common header file for TensorFlow Lite's C API, providing general utilities and definitions.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/delegates/coreml/builders/op_builder.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file that defines the base class for operation builders in TensorFlow Lite's Core ML delegate.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"ENUM_DEFINITION\",\n",
      "        \"component_name\": \"ConvolutionType\",\n",
      "        \"component_code\": \"enum class ConvolutionType { kConv, kDepthwiseConv, kTransposeConv };\",\n",
      "        \"component_description\": \"Defines an enumeration representing different types of convolution operations: standard convolution (kConv), depthwise convolution (kDepthwiseConv), and transposed convolution (kTransposeConv).\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"ConvolutionOpBuilder\",\n",
      "        \"component_code\": \"class ConvolutionOpBuilder : public OpBuilder { ... };\",\n",
      "        \"component_description\": \"Defines a class named ConvolutionOpBuilder that inherits from OpBuilder. This class is responsible for building Core ML layers corresponding to convolution operations in TensorFlow Lite.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTRUCTOR\",\n",
      "        \"component_name\": \"ConvolutionOpBuilder\",\n",
      "        \"component_code\": \"explicit ConvolutionOpBuilder(GraphBuilder* graph_builder, ConvolutionType conv_type) : OpBuilder(graph_builder), conv_type_(conv_type) {}\",\n",
      "        \"component_description\": \"Constructor for the ConvolutionOpBuilder class. Initializes the base class with a pointer to a GraphBuilder and sets the type of convolution operation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"DebugName\",\n",
      "        \"component_code\": \"const std::string& DebugName() override;\",\n",
      "        \"component_description\": \"Pure virtual function that overrides OpBuilder's DebugName method. Returns a string representing the debug name of the operation builder.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"Build\",\n",
      "        \"component_code\": \"CoreML::Specification::NeuralNetworkLayer* Build() override;\",\n",
      "        \"component_description\": \"Pure virtual function that overrides OpBuilder's Build method. Constructs and returns a pointer to the Core ML neural network layer corresponding to the convolution operation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"PopulateSubgraph\",\n",
      "        \"component_code\": \"TfLiteStatus PopulateSubgraph(TfLiteContext* context) override;\",\n",
      "        \"component_description\": \"Pure virtual function that overrides OpBuilder's PopulateSubgraph method. Populates a subgraph in the TensorFlow Lite context with nodes and tensors required for the convolution operation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetOutputChannels\",\n",
      "        \"component_code\": \"void SetOutputChannels(uint64_t output_channels);\",\n",
      "        \"component_description\": \"Function to set the number of output channels for the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetNGroups\",\n",
      "        \"component_code\": \"void SetNGroups(uint64_t n_groups);\",\n",
      "        \"component_description\": \"Function to set the number of groups for grouped convolutions, used in depthwise convolution.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetWeights\",\n",
      "        \"component_code\": \"void SetWeights(TfLiteTensor* weights);\",\n",
      "        \"component_description\": \"Function to set the tensor containing the weights of the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetBias\",\n",
      "        \"component_code\": \"void SetBias(TfLiteTensor* bias);\",\n",
      "        \"component_description\": \"Function to set the tensor containing the biases for the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetOutputShape\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Implicit function (not explicitly shown in code) that would be used to set the output shape of the convolution operation, typically part of the builder's implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"TransposeWeights\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Implicit function (not explicitly shown in code) that would be used to transpose weights if necessary for Core ML compatibility, typically part of the builder's implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"RegisterInputsOutputs\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Implicit function (not explicitly shown in code) that would be used to register inputs and outputs of the convolution layer, typically part of the builder's implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"SetOutputShape\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Implicit function (not explicitly shown in code) that would be used to set the output shape of the convolution operation, typically part of the builder's implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"conv_type_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that holds the type of convolution operation, initialized in the constructor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"output_channels_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that stores the number of output channels for the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"n_groups_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that holds the number of groups used in grouped convolutions, such as depthwise convolution.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"weights_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that stores a pointer to the tensor containing the weights for the convolution layer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"bias_\",\n",
      "        \"component_code\": null,\n",
      "        \"component_description\": \"Member variable (not explicitly shown in code) that stores a pointer to the tensor containing the biases for the convolution layer.\"\n",
      "      }\n",
      "    ],\n",
      "    \"description\": \"This header file defines the ConvolutionOpBuilder class, which is responsible for constructing Core ML layers corresponding to different types of convolution operations in TensorFlow Lite. The builder supports standard convolution, depthwise convolution, and transposed convolution by setting various parameters such as weights, biases, output channels, and groups.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 7: \n",
      "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_FRAMEWORK_RUN_HANDLER_H_\n",
      "#define TENSORFLOW_CORE_FRAMEWORK_RUN_HANDLER_H_\n",
      "\n",
      "#include \"tensorflow/core/lib/core/threadpool.h\"\n",
      "#include \"tensorflow/core/lib/histogram/histogram.h\"\n",
      "#include \"tensorflow/core/platform/context.h\"\n",
      "#include \"tensorflow/core/platform/mutex.h\"\n",
      "#include \"tensorflow/core/platform/thread_annotations.h\"\n",
      "#include \"tensorflow/core/protobuf/config.pb.h\"\n",
      "\n",
      "namespace Eigen {\n",
      "struct ThreadPoolDevice;\n",
      "}\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "class RunHandler;\n",
      "\n",
      "// RunHandlerPool is a fixed size pool of pre-allocated RunHandlers\n",
      "// that can be used for tracking inter-op work for a given Session::Run().\n",
      "// RunHandler(s) in the pool are initially 'inactive'. A RunHandler becomes\n",
      "// 'active' when its unique_ptr is returned by Get() and is being used by a\n",
      "// client. It becomes 'inactive' once more when its unique_ptr gets destroyed.\n",
      "//\n",
      "// Expected usage:\n",
      "//\n",
      "// * Create a single RunHandlerPool (say run_handler_pool_).\n",
      "//\n",
      "// * When a Session::Run() is invoked, obtain a handler by:\n",
      "// auto handler = run_handler_pool_->Get();\n",
      "//\n",
      "// * Use handler for scheduling all inter-op work by:\n",
      "// handler->ScheduleInterOpClosure(closure);\n",
      "//\n",
      "// This class is thread safe.\n",
      "class RunHandlerPool {\n",
      " public:\n",
      "  explicit RunHandlerPool(int num_inter_op_threads);\n",
      "\n",
      "  RunHandlerPool(int num_inter_op_threads, int num_intra_op_threads);\n",
      "  ~RunHandlerPool();\n",
      "\n",
      "  // Returns an inactive RunHandler from the pool.\n",
      "  //\n",
      "  // RunHandlers in RunHandlerPool are initially 'inactive'.\n",
      "  // A RunHandler becomes 'active' when its unique_ptr its returned by Get()\n",
      "  // and is being used by a client.  It becomes 'inactive' once more when the\n",
      "  // unique_ptr is destroyed.\n",
      "  //\n",
      "  // Will block unless there is an inactive handler.\n",
      "  std::unique_ptr<RunHandler> Get(\n",
      "      int64_t step_id = 0, int64_t timeout_in_ms = 0,\n",
      "      const RunOptions::Experimental::RunHandlerPoolOptions& options =\n",
      "          RunOptions::Experimental::RunHandlerPoolOptions());\n",
      "\n",
      "  // Get the priorities for active handlers. The return result is with the same\n",
      "  // order of the active handler list.\n",
      "  std::vector<int64_t> GetActiveHandlerPrioritiesForTesting() const;\n",
      "\n",
      " private:\n",
      "  class Impl;\n",
      "  friend class RunHandler;\n",
      "\n",
      "  std::unique_ptr<Impl> impl_;\n",
      "};\n",
      "\n",
      "// RunHandler can be used to schedule inter/intra-op closures to run on a global\n",
      "// pool shared across all Session::Run(s). The closures are enqueued to a\n",
      "// handler specific queue, from which the work is stolen in a priority order\n",
      "// (time of the Get() call).\n",
      "//\n",
      "// It can only be created via RunHandlerPool::Get().\n",
      "//\n",
      "// This class can be used instead of directly scheduling closures on a global\n",
      "// pool since it maintains a global view across all sessions and optimizes pool\n",
      "// scheduling to improve (median and tail) latency.\n",
      "//\n",
      "// This class is thread safe.\n",
      "class RunHandler {\n",
      " public:\n",
      "  void ScheduleInterOpClosure(std::function<void()> fn);\n",
      "  thread::ThreadPoolInterface* AsIntraThreadPoolInterface();\n",
      "\n",
      "  ~RunHandler();\n",
      "\n",
      " private:\n",
      "  class Impl;\n",
      "  friend class RunHandlerPool::Impl;\n",
      "\n",
      "  explicit RunHandler(Impl* impl);\n",
      "\n",
      "  Impl* impl_;  // NOT OWNED.\n",
      "};\n",
      "\n",
      "namespace internal {\n",
      "\n",
      "// TODO(azaks): Refactor with thread:ThreadPool\n",
      "class RunHandlerEnvironment {\n",
      "  typedef Thread EnvThread;\n",
      "  struct TaskImpl {\n",
      "    std::function<void()> f;\n",
      "    Context context;\n",
      "    uint64 trace_id;\n",
      "  };\n",
      "  Env* const env_;\n",
      "  const ThreadOptions thread_options_;\n",
      "  const string name_;\n",
      "\n",
      " public:\n",
      "  struct Task {\n",
      "    std::unique_ptr<TaskImpl> f;\n",
      "  };\n",
      "\n",
      "  RunHandlerEnvironment(Env* env, const ThreadOptions& thread_options,\n",
      "                        const string& name);\n",
      "\n",
      "  EnvThread* CreateThread(std::function<void()> f,\n",
      "                          const std::string& thread_name);\n",
      "\n",
      "  Task CreateTask(std::function<void()> f);\n",
      "\n",
      "  void ExecuteTask(const Task& t);\n",
      "};\n",
      "\n",
      "typedef typename RunHandlerEnvironment::Task Task;\n",
      "typedef Eigen::RunQueue<Task, 1024> Queue;\n",
      "\n",
      "// To reduce cache misses, we use a doubly-linked list of Waiter structs and\n",
      "// queue them in LIFO order rather than the FIFO order used by a single\n",
      "// condition variable.\n",
      "struct Waiter {\n",
      "  Waiter() {\n",
      "    next = this;\n",
      "    prev = this;\n",
      "  }\n",
      "  condition_variable cv;\n",
      "  mutex mu;\n",
      "  Waiter* next;\n",
      "  Waiter* prev;\n",
      "};\n",
      "\n",
      "class ThreadWorkSource {\n",
      " public:\n",
      "  ThreadWorkSource();\n",
      "\n",
      "  ~ThreadWorkSource();\n",
      "\n",
      "  Task EnqueueTask(Task t, bool is_blocking);\n",
      "\n",
      "  Task PopBlockingTask();\n",
      "\n",
      "  Task PopNonBlockingTask(int start_index, bool search_from_all_queue);\n",
      "\n",
      "  void WaitForWork(int max_sleep_micros);\n",
      "\n",
      "  int TaskQueueSize(bool is_blocking);\n",
      "\n",
      "  int64_t GetTracemeId();\n",
      "\n",
      "  void SetTracemeId(int64_t value);\n",
      "\n",
      "  void SetWaiter(uint64 version, Waiter* waiter, mutex* mutex);\n",
      "\n",
      "  int64_t GetInflightTaskCount(bool is_blocking);\n",
      "\n",
      "  void IncrementInflightTaskCount(bool is_blocking);\n",
      "\n",
      "  void DecrementInflightTaskCount(bool is_blocking);\n",
      "\n",
      "  unsigned NonBlockingWorkShardingFactor();\n",
      "\n",
      "  std::string ToString();\n",
      "\n",
      " private:\n",
      "  struct NonBlockingQueue {\n",
      "    mutex queue_op_mu;\n",
      "    char pad[128];\n",
      "    Queue queue;\n",
      "  };\n",
      "\n",
      "  int32 non_blocking_work_sharding_factor_;\n",
      "  Eigen::MaxSizeVector<NonBlockingQueue*> non_blocking_work_queues_;\n",
      "\n",
      "  std::atomic<int64_t> blocking_inflight_;\n",
      "  std::atomic<int64_t> non_blocking_inflight_;\n",
      "\n",
      "  Queue blocking_work_queue_;\n",
      "  mutex blocking_queue_op_mu_;\n",
      "  char pad_[128];\n",
      "  mutex waiters_mu_;\n",
      "  Waiter queue_waiters_ TF_GUARDED_BY(waiters_mu_);\n",
      "  std::atomic<int64_t> traceme_id_;\n",
      "\n",
      "  mutex run_handler_waiter_mu_;\n",
      "  uint64 version_ TF_GUARDED_BY(run_handler_waiter_mu_);\n",
      "  mutex* sub_thread_pool_waiter_mu_ TF_GUARDED_BY(run_handler_waiter_mu_);\n",
      "  Waiter* sub_thread_pool_waiter_ TF_GUARDED_BY(run_handler_waiter_mu_);\n",
      "};\n",
      "\n",
      "class RunHandlerThreadPool {\n",
      " public:\n",
      "  struct PerThread {\n",
      "    constexpr PerThread() : pool(nullptr), thread_id(-1) {}\n",
      "    RunHandlerThreadPool* pool;  // Parent pool, or null for normal threads.\n",
      "    int thread_id;               // Worker thread index in pool.\n",
      "  };\n",
      "\n",
      "  RunHandlerThreadPool(int num_blocking_threads, int num_non_blocking_threads,\n",
      "                       Env* env, const ThreadOptions& thread_options,\n",
      "                       const string& name,\n",
      "                       Eigen::MaxSizeVector<mutex>* waiters_mu,\n",
      "                       Eigen::MaxSizeVector<Waiter>* queue_waiters);\n",
      "\n",
      "  ~RunHandlerThreadPool();\n",
      "\n",
      "  void Start();\n",
      "\n",
      "  void StartOneThreadForTesting();\n",
      "\n",
      "  void AddWorkToQueue(ThreadWorkSource* tws, bool is_blocking,\n",
      "                      std::function<void()> fn);\n",
      "\n",
      "  // Set work queues from which the thread 'tid' can steal its work.\n",
      "  // The request with start_request_idx will be attempted first. Other requests\n",
      "  // will be attempted in FIFO order based on their arrival time.\n",
      "  void SetThreadWorkSources(\n",
      "      int tid, int start_request_idx, uint64 version,\n",
      "      const Eigen::MaxSizeVector<ThreadWorkSource*>& thread_work_sources);\n",
      "\n",
      "  PerThread* GetPerThread();\n",
      "\n",
      "  int CurrentThreadId() const;\n",
      "\n",
      "  int NumThreads() const;\n",
      "\n",
      "  int NumBlockingThreads() const;\n",
      "\n",
      "  int NumNonBlockingThreads() const;\n",
      "\n",
      "  void WorkerLoop(int thread_id, bool may_steal_blocking_work);\n",
      "\n",
      "  // Search tasks from Requets range searching_range_start to\n",
      "  // searching_range_end. If there is no tasks in the search range and\n",
      "  // may_steal_blocking_work is true, then search from all requests.\n",
      "  Task FindTask(\n",
      "      int searching_range_start, int searching_range_end, int thread_id,\n",
      "      int sub_thread_pool_id, int max_blocking_inflight,\n",
      "      bool may_steal_blocking_work,\n",
      "      const Eigen::MaxSizeVector<ThreadWorkSource*>& thread_work_sources,\n",
      "      bool* task_from_blocking_queue, ThreadWorkSource** tws);\n",
      "\n",
      "  void WaitForWork(bool is_blocking, int thread_id,\n",
      "                   int32_t max_blocking_inflight);\n",
      "\n",
      "  void WaitForWorkInSubThreadPool(bool is_blocking, int sub_thread_pool_id);\n",
      "\n",
      " private:\n",
      "  struct ThreadData {\n",
      "    ThreadData();\n",
      "    mutex mu;\n",
      "    uint64 new_version;\n",
      "    condition_variable sources_not_empty;\n",
      "    std::unique_ptr<Thread> thread;\n",
      "    int current_index;\n",
      "    std::unique_ptr<Eigen::MaxSizeVector<ThreadWorkSource*>>\n",
      "        new_thread_work_sources TF_GUARDED_BY(mu);\n",
      "\n",
      "    uint64 current_version;\n",
      "    // Should only be accessed by one thread.\n",
      "    std::unique_ptr<Eigen::MaxSizeVector<ThreadWorkSource*>>\n",
      "        current_thread_work_sources;\n",
      "\n",
      "    int sub_thread_pool_id;\n",
      "  };\n",
      "\n",
      "  const int num_threads_;\n",
      "  const int num_blocking_threads_;\n",
      "  const int num_non_blocking_threads_;\n",
      "  Eigen::MaxSizeVector<ThreadData> thread_data_;\n",
      "  internal::RunHandlerEnvironment env_;\n",
      "  std::atomic<bool> cancelled_;\n",
      "  string name_;\n",
      "  Eigen::MaxSizeVector<mutex>* waiters_mu_;\n",
      "  Eigen::MaxSizeVector<Waiter>* queue_waiters_;\n",
      "\n",
      "  bool use_sub_thread_pool_;\n",
      "  std::vector<int> num_threads_in_sub_thread_pool_;\n",
      "\n",
      "  // Threads in each sub thread pool will search tasks from the given\n",
      "  // start_request_percentage to end_request_percentage in a round robin\n",
      "  // fashion.\n",
      "  std::vector<double> sub_thread_pool_start_request_percentage_;\n",
      "  std::vector<double> sub_thread_pool_end_request_percentage_;\n",
      "};\n",
      "\n",
      "}  // namespace internal\n",
      "\n",
      "}  // end namespace tensorflow.\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_FRAMEWORK_RUN_HANDLER_H_\n",
      "\n",
      "\n",
      "Output 7: \n",
      "This code snippet defines a part of TensorFlow's framework, specifically around handling and managing thread pools for executing tasks. The key components here are `RunHandlerThreadPool` and its related classes like `ThreadWorkSource`, which are used to manage the distribution and execution of workloads across multiple threads.\n",
      "\n",
      "### Key Classes:\n",
      "\n",
      "1. **ThreadWorkSource**:\n",
      "   - Represents a queue of tasks that can be worked on by threads.\n",
      "   - Maintains separate queues for blocking and non-blocking tasks.\n",
      "   - Provides methods to add, find, and remove tasks from these queues.\n",
      "   \n",
      "2. **RunHandlerThreadPool**:\n",
      "   - Manages a pool of worker threads that execute tasks from `ThreadWorkSource` instances.\n",
      "   - Supports both blocking (synchronous) and non-blocking (asynchronous) workloads.\n",
      "   - Allows threads to steal work from other sources when they become idle, promoting better load balancing.\n",
      "\n",
      "### Key Features:\n",
      "\n",
      "- **Task Queues**: Tasks are categorized into blocking and non-blocking queues based on their nature. Blocking tasks require immediate attention and can block the thread until completion, while non-blocking tasks do not hold up the thread.\n",
      "  \n",
      "- **Thread Pool Management**: The `RunHandlerThreadPool` manages a pool of threads that execute tasks from various sources. It starts threads when needed and stops them gracefully during shutdown.\n",
      "\n",
      "- **Task Stealing**: Threads that finish their assigned tasks can steal work from other threads' queues, which helps in distributing the workload evenly across all available threads and reduces idle time.\n",
      "\n",
      "- **Sub-thread Pools**: For more advanced load balancing, `RunHandlerThreadPool` supports sub-thread pools where threads within a pool search for tasks from specific ranges of sources. This is useful when certain types of tasks are expected to be more common or resource-intensive in different parts of the application.\n",
      "\n",
      "### Usage Scenario:\n",
      "\n",
      "This setup is particularly useful in scenarios where you have a large number of independent, potentially long-running tasks that need to be distributed across multiple CPU cores for parallel execution. By using `RunHandlerThreadPool`, TensorFlow can efficiently manage these tasks, ensuring optimal use of available resources and minimizing idle time.\n",
      "\n",
      "### Example Use Case:\n",
      "\n",
      "Imagine a deep learning model training process where different parts of the model (or even different models) are trained in parallel. Each part could be represented as a `ThreadWorkSource` with its own set of tasks (e.g., forward propagation, backward propagation). The `RunHandlerThreadPool` would manage a pool of threads to execute these tasks efficiently across all available CPU cores.\n",
      "\n",
      "This code is an essential part of TensorFlow's internal architecture and demonstrates sophisticated task management and thread coordination strategies.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 8: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\n",
      "#define TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace profiler {\n",
      "namespace test {\n",
      "// Calls a function on the device to print a string as many times as indicated\n",
      "// by iters.\n",
      "void PrintfKernel(int iters = 1);\n",
      "\n",
      "// Calls an empty kernel (named \"empty\") on the device as many times as\n",
      "// indicated by iters.\n",
      "void EmptyKernel(int iters = 1);\n",
      "\n",
      "// Waits for device activity to complete.\n",
      "void Synchronize();\n",
      "\n",
      "// Copies a few bytes of memory from host to device.\n",
      "void MemCopyH2D();\n",
      "\n",
      "// Copies a few bytes of memory from device to host, asynchronously.\n",
      "void MemCopyH2D_Async();\n",
      "\n",
      "// Copies a few bytes of memory from device to host.\n",
      "void MemCopyD2H();\n",
      "\n",
      "// Returns true if it s possible to copy bytes from device 0 to device 1.\n",
      "bool MemCopyP2PAvailable();\n",
      "\n",
      "// Copies a few bytes of memory from device 0 to device 1.\n",
      "void MemCopyP2PImplicit();\n",
      "\n",
      "// Copies a few bytes of memory from device 0 to device 1.\n",
      "void MemCopyP2PExplicit();\n",
      "\n",
      "}  // namespace test\n",
      "}  // namespace profiler\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\n",
      "\n",
      "\n",
      "Output 8: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\\n#define TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\\n#endif  // TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_CUDA_TEST_H_\",\n",
      "        \"component_description\": \"A header guard to prevent multiple inclusions of the same header file.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow::profiler::test\",\n",
      "        \"component_code\": \"namespace tensorflow {\\nnamespace profiler {\\nnamespace test {\\n...\\n}  // namespace test\\n}  // namespace profiler\\n}  // namespace tensorflow\",\n",
      "        \"component_description\": \"Namespaces to encapsulate and organize the code within the TensorFlow project.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"PrintfKernel\",\n",
      "        \"component_code\": \"void PrintfKernel(int iters = 1);\",\n",
      "        \"component_description\": \"Declares a function that calls a device kernel to print a string multiple times. The default number of iterations is 1.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"EmptyKernel\",\n",
      "        \"component_code\": \"void EmptyKernel(int iters = 1);\",\n",
      "        \"component_description\": \"Declares a function that calls an empty device kernel multiple times. The default number of iterations is 1.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"Synchronize\",\n",
      "        \"component_code\": \"void Synchronize();\",\n",
      "        \"component_description\": \"Declares a function that waits for all previous device operations to complete.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyH2D\",\n",
      "        \"component_code\": \"void MemCopyH2D();\",\n",
      "        \"component_description\": \"Declares a function that copies a small amount of data from the host memory to the device memory.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyH2D_Async\",\n",
      "        \"component_code\": \"void MemCopyH2D_Async();\",\n",
      "        \"component_description\": \"Declares a function that asynchronously copies a small amount of data from the host memory to the device memory.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyD2H\",\n",
      "        \"component_code\": \"void MemCopyD2H();\",\n",
      "        \"component_description\": \"Declares a function that copies a small amount of data from the device memory to the host memory.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyP2PAvailable\",\n",
      "        \"component_code\": \"bool MemCopyP2PAvailable();\",\n",
      "        \"component_description\": \"Declares a function that checks if peer-to-peer memory copy between device 0 and device 1 is possible.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyP2PImplicit\",\n",
      "        \"component_code\": \"void MemCopyP2PImplicit();\",\n",
      "        \"component_description\": \"Declares a function that copies data from device 0 to device 1 using implicit peer-to-peer memory copy.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"MemCopyP2PExplicit\",\n",
      "        \"component_code\": \"void MemCopyP2PExplicit();\",\n",
      "        \"component_description\": \"Declares a function that copies data from device 0 to device 1 using explicit peer-to-peer memory copy.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file declares several functions for testing CUDA operations within the TensorFlow project. It includes functions for printing strings on the GPU, calling empty kernels, synchronizing device activities, and performing various types of memory copies between host and device memories, including peer-to-peer transfers between devices.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 9: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "// Classes for keeping track of on-device state for TPUs.\n",
      "\n",
      "#ifndef TENSORFLOW_COMPILER_XRT_XRT_TPU_DEVICE_H_\n",
      "#define TENSORFLOW_COMPILER_XRT_XRT_TPU_DEVICE_H_\n",
      "\n",
      "#include \"tensorflow/compiler/xla/client/local_client.h\"\n",
      "#include \"tensorflow/core/framework/op_kernel.h\"\n",
      "#include \"tensorflow/core/framework/resource_mgr.h\"\n",
      "#include \"tensorflow/stream_executor/tpu/tpu_node_context.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "// This accessor is used for XLA TPU. It uses the distributed TPU compilation\n",
      "// cache infrastructure which it accesses via the TPU_SYSTEM resource manager.\n",
      "class XRTTpuDeviceAccessor {\n",
      " public:\n",
      "  static Status GetResourceManager(OpKernelContext* ctx, ResourceMgr** rm);\n",
      "\n",
      "  class ScopedRef {\n",
      "   public:\n",
      "    ScopedRef() {}\n",
      "    ~ScopedRef() {}\n",
      "\n",
      "    ScopedRef(const ScopedRef&) = delete;\n",
      "    ScopedRef& operator=(const ScopedRef&) = delete;\n",
      "\n",
      "    // Returns the XLA device properties from the TpuNodeContext object\n",
      "    // protected by this ScopedRef.\n",
      "    xla::Backend* backend() { return node_context_->backend(); }\n",
      "    int device_ordinal() { return ordinal_; }\n",
      "    se::DeviceMemoryAllocator* allocator() {\n",
      "      return backend()->memory_allocator();\n",
      "    }\n",
      "\n",
      "   private:\n",
      "    // XRTTpuDeviceAccessor::InitScopedRef is the only way to initialize\n",
      "    // ScopedRef.\n",
      "    friend class XRTTpuDeviceAccessor;\n",
      "\n",
      "    Status Acquire(int device_ordinal);\n",
      "\n",
      "    Status Acquire(OpKernelContext* ctx);\n",
      "\n",
      "    std::unique_ptr<tpu::TpuNodeContext> node_context_;\n",
      "    int ordinal_ = 0;\n",
      "  };\n",
      "\n",
      "  static Status InitScopedRef(OpKernelContext* ctx, int device_ordinal,\n",
      "                              ScopedRef* scoped_ref);\n",
      "\n",
      "  static Status InitScopedRef(OpKernelContext* ctx, ScopedRef* scoped_ref);\n",
      "};\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_COMPILER_XRT_XRT_TPU_DEVICE_H_\n",
      "\n",
      "\n",
      "Output 9: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/compiler/xla/client/local_client.h\\\"\",\n",
      "        \"component_description\": \"Includes the local_client header file from TensorFlow's XLA client, which provides functionality for interacting with local XLA clients.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/framework/op_kernel.h\\\"\",\n",
      "        \"component_description\": \"Includes the op_kernel header file from TensorFlow's core framework, which contains definitions and utilities for kernel operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/framework/resource_mgr.h\\\"\",\n",
      "        \"component_description\": \"Includes the resource_mgr header file from TensorFlow's core framework, which provides a mechanism for managing resources used by kernel operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/stream_executor/tpu/tpu_node_context.h\\\"\",\n",
      "        \"component_description\": \"Includes the tpu_node_context header file from TensorFlow's stream executor, which provides functionality for managing TPU node contexts.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"XRTTpuDeviceAccessor\",\n",
      "        \"component_code\": \"class XRTTpuDeviceAccessor {\\n public:\\n  static Status GetResourceManager(OpKernelContext* ctx, ResourceMgr** rm);\\n\\n  class ScopedRef {\\n   public:\\n    ScopedRef() {}\\n    ~ScopedRef() {}\\n\\n    ScopedRef(const ScopedRef&) = delete;\\n    ScopedRef& operator=(const ScopedRef&) = delete;\\n\\n    xla::Backend* backend() { return node_context_->backend(); }\\n    int device_ordinal() { return ordinal_; }\\n    se::DeviceMemoryAllocator* allocator() {\\n      return backend()->memory_allocator();\\n    }\\n\\n   private:\\n    friend class XRTTpuDeviceAccessor;\\n\\n    Status Acquire(int device_ordinal);\\n\\n    Status Acquire(OpKernelContext* ctx);\\n\\n    std::unique_ptr<tpu::TpuNodeContext> node_context_;\\n    int ordinal_ = 0;\\n  };\\n\\n  static Status InitScopedRef(OpKernelContext* ctx, int device_ordinal,\\n                              ScopedRef* scoped_ref);\\n\\n  static Status InitScopedRef(OpKernelContext* ctx, ScopedRef* scoped_ref);\\n};\",\n",
      "        \"component_description\": \"Defines the XRTTpuDeviceAccessor class used for managing on-device state for TPUs in TensorFlow. It includes a nested ScopedRef class which manages TPU node contexts and provides access to backend and memory allocator properties. The class also contains static methods for initializing ScopedRef objects and retrieving resource managers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"ScopedRef\",\n",
      "        \"component_code\": \"class ScopedRef {\\n public:\\n  ScopedRef() {}\\n  ~ScopedRef() {}\\n\\n  ScopedRef(const ScopedRef&) = delete;\\n  ScopedRef& operator=(const ScopedRef&) = delete;\\n\\n  xla::Backend* backend() { return node_context_->backend(); }\\n  int device_ordinal() { return ordinal_; }\\n  se::DeviceMemoryAllocator* allocator() {\\n    return backend()->memory_allocator();\\n  }\\n\\n private:\\n  friend class XRTTpuDeviceAccessor;\\n\\n  Status Acquire(int device_ordinal);\\n\\n  Status Acquire(OpKernelContext* ctx);\\n\\n  std::unique_ptr<tpu::TpuNodeContext> node_context_;\\n  int ordinal_ = 0;\\n};\",\n",
      "        \"component_description\": \"Defines the ScopedRef class nested within XRTTpuDeviceAccessor, which manages a unique pointer to a TPU node context and provides accessors for the backend, device ordinal, and memory allocator. It includes private constructors to restrict initialization and provides methods to acquire resources.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetResourceManager\",\n",
      "        \"component_code\": \"static Status GetResourceManager(OpKernelContext* ctx, ResourceMgr** rm);\",\n",
      "        \"component_description\": \"Static method within XRTTpuDeviceAccessor that retrieves the resource manager associated with a given OpKernelContext. It takes an OpKernelContext pointer and a pointer to a ResourceMgr pointer for output.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"Acquire (int)\",\n",
      "        \"component_code\": \"Status Acquire(int device_ordinal);\",\n",
      "        \"component_description\": \"Method within ScopedRef that acquires resources based on the specified device ordinal. It returns a Status object indicating success or failure.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"Acquire (OpKernelContext*)\",\n",
      "        \"component_code\": \"Status Acquire(OpKernelContext* ctx);\",\n",
      "        \"component_description\": \"Method within ScopedRef that acquires resources based on an OpKernelContext. It returns a Status object indicating success or failure.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"InitScopedRef (int)\",\n",
      "        \"component_code\": \"static Status InitScopedRef(OpKernelContext* ctx, int device_ordinal,\\n                              ScopedRef* scoped_ref);\",\n",
      "        \"component_description\": \"Static method within XRTTpuDeviceAccessor that initializes a ScopedRef object with the specified device ordinal. It takes an OpKernelContext pointer and a pointer to a ScopedRef object for output.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"InitScopedRef (OpKernelContext*)\",\n",
      "        \"component_code\": \"static Status InitScopedRef(OpKernelContext* ctx, ScopedRef* scoped_ref);\",\n",
      "        \"component_description\": \"Static method within XRTTpuDeviceAccessor that initializes a ScopedRef object based on an OpKernelContext. It takes an OpKernelContext pointer and a pointer to a ScopedRef object for output.\"\n",
      "      }\n",
      "    ],\n",
      "    \"description\": \"The provided code defines the XRTTpuDeviceAccessor class in TensorFlow, which is used for managing on-device state for TPUs. The class includes nested ScopedRef class that manages TPU node contexts and provides accessors for backend and memory allocator properties. It also contains static methods for initializing ScopedRef objects and retrieving resource managers.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1. **Header Inclusions**:\n",
      "   - `op_kernel.h`: Contains definitions and utilities for kernel operations.\n",
      "   - `resource_mgr.h`: Provides a mechanism for managing resources used by kernel operations.\n",
      "   - `tpu_node_context.h`: Provides functionality for managing TPU node contexts.\n",
      "\n",
      "2. **XRTTpuDeviceAccessor Class**:\n",
      "   - **Nested ScopedRef Class**:\n",
      "     - Manages a unique pointer to a TPU node context.\n",
      "     - Provides accessors for the backend, device ordinal, and memory allocator.\n",
      "     - Includes private constructors to restrict initialization.\n",
      "     - Contains methods `Acquire` to acquire resources based on a device ordinal or an `OpKernelContext`.\n",
      "   - **Static Methods**:\n",
      "     - `GetResourceManager`: Retrieves the resource manager associated with a given `OpKernelContext`.\n",
      "     - `InitScopedRef`: Initializes a `ScopedRef` object based on a device ordinal or an `OpKernelContext`.\n",
      "\n",
      "3. **Methods in ScopedRef**:\n",
      "   - `Acquire(int device_ordinal)`: Acquires resources based on the specified device ordinal.\n",
      "   - `Acquire(OpKernelContext* ctx)`: Acquires resources based on an `OpKernelContext`.\n",
      "\n",
      "This class and its nested class are crucial for managing TPU-specific state and resources within TensorFlow, ensuring efficient resource handling and access to necessary components.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 10: \n",
      "<filename>tensorflow/c/experimental/saved_model/core/signature_def_function_metadata.h\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\n",
      "#define TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\n",
      "\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"tensorflow/c/experimental/saved_model/core/tensor_spec.h\"\n",
      "#include \"tensorflow/core/platform/status.h\"\n",
      "#include \"tensorflow/core/protobuf/struct.pb.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "// SignatureDefParam represents a named Tensor input or output to a\n",
      "// SignatureDefFunction.\n",
      "class SignatureDefParam {\n",
      " public:\n",
      "  SignatureDefParam(std::string name, TensorSpec spec);\n",
      "\n",
      "  const std::string& name() const;\n",
      "\n",
      "  const TensorSpec& spec() const;\n",
      "\n",
      " private:\n",
      "  std::string name_;\n",
      "  TensorSpec spec_;\n",
      "};\n",
      "\n",
      "class SignatureDefFunctionMetadata {\n",
      " public:\n",
      "  SignatureDefFunctionMetadata() = default;\n",
      "  SignatureDefFunctionMetadata(std::vector<SignatureDefParam> arguments,\n",
      "                               std::vector<SignatureDefParam> returns);\n",
      "\n",
      "  const std::vector<SignatureDefParam>& arguments() const;\n",
      "  const std::vector<SignatureDefParam>& returns() const;\n",
      "\n",
      " private:\n",
      "  std::vector<SignatureDefParam> arguments_;\n",
      "  std::vector<SignatureDefParam> returns_;\n",
      "};\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\n",
      "\n",
      "\n",
      "Output 10: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\\n#define TENSORFLOW_C_EXPERIMENTAL_SAVED_MODEL_CORE_SIGNATURE_DEF_FUNCTION_METADATA_H_\",\n",
      "        \"component_description\": \"A header guard to prevent multiple inclusions of the header file.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"string\",\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the C++ Standard Library string header for using std::string.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"vector\",\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the C++ Standard Library vector header for using std::vector.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/c/experimental/saved_model/core/tensor_spec.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/c/experimental/saved_model/core/tensor_spec.h\\\"\",\n",
      "        \"component_description\": \"Includes the TensorSpec header for using the TensorSpec class.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/core/platform/status.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/status.h\\\"\",\n",
      "        \"component_description\": \"Includes the Status header for error handling and status management within TensorFlow.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"tensorflow/core/protobuf/struct.pb.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/protobuf/struct.pb.h\\\"\",\n",
      "        \"component_description\": \"Includes the struct.pb.h header for using protocol buffers related to TensorFlow structures.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DEFINITION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\\n  // ... classes and methods here ...\\n}\",\n",
      "        \"component_description\": \"Defines a namespace named 'tensorflow' to encapsulate the class definitions, avoiding name conflicts.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"SignatureDefParam\",\n",
      "        \"component_code\": \"class SignatureDefParam {\\n public:\\n  SignatureDefParam(std::string name, TensorSpec spec);\\n  const std::string& name() const;\\n  const TensorSpec& spec() const;\\n private:\\n  std::string name_;\\n  TensorSpec spec_;\\n};\",\n",
      "        \"component_description\": \"Defines a class named 'SignatureDefParam' representing a named Tensor input or output to a SignatureDefFunction. It includes a constructor for initialization, and methods to retrieve the name and specification of the tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"SignatureDefFunctionMetadata\",\n",
      "        \"component_code\": \"class SignatureDefFunctionMetadata {\\n public:\\n  SignatureDefFunctionMetadata() = default;\\n  SignatureDefFunctionMetadata(std::vector<SignatureDefParam> arguments, std::vector<SignatureDefParam> returns);\\n  const std::vector<SignatureDefParam>& arguments() const;\\n  const std::vector<SignatureDefParam>& returns() const;\\n private:\\n  std::vector<SignatureDefParam> arguments_;\\n  std::vector<SignatureDefParam> returns_;\\n};\",\n",
      "        \"component_description\": \"Defines a class named 'SignatureDefFunctionMetadata' that holds metadata for a function's signature, including its input and output parameters. It provides constructors for initialization, and methods to retrieve the vectors of input and output parameters.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file defines two classes within the tensorflow namespace: SignatureDefParam and SignatureDefFunctionMetadata. These classes are designed to encapsulate metadata related to function signatures in TensorFlow's SavedModel format, specifically focusing on tensor inputs and outputs. The use of vectors allows for multiple parameters, and the inclusion of protocol buffer-related headers suggests integration with TensorFlow's internal data structures for handling complex data types.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 11: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "/// \\file\n",
      "/// Memory management for TF Lite.\n",
      "#ifndef TENSORFLOW_LITE_ALLOCATION_H_\n",
      "#define TENSORFLOW_LITE_ALLOCATION_H_\n",
      "\n",
      "#include <stddef.h>\n",
      "\n",
      "#include <cstdio>\n",
      "#include <cstdlib>\n",
      "#include <memory>\n",
      "\n",
      "#include \"tensorflow/lite/core/api/error_reporter.h\"\n",
      "\n",
      "namespace tflite {\n",
      "\n",
      "// A memory allocation handle. This could be a mmap or shared memory.\n",
      "class Allocation {\n",
      " public:\n",
      "  virtual ~Allocation() {}\n",
      "\n",
      "  enum class Type {\n",
      "    kMMap,\n",
      "    kFileCopy,\n",
      "    kMemory,\n",
      "  };\n",
      "\n",
      "  // Base pointer of this allocation\n",
      "  virtual const void* base() const = 0;\n",
      "  // Size in bytes of the allocation\n",
      "  virtual size_t bytes() const = 0;\n",
      "  // Whether the allocation is valid\n",
      "  virtual bool valid() const = 0;\n",
      "  // Return the type of the Allocation.\n",
      "  Type type() const { return type_; }\n",
      "\n",
      " protected:\n",
      "  Allocation(ErrorReporter* error_reporter, Type type)\n",
      "      : error_reporter_(error_reporter), type_(type) {}\n",
      "  ErrorReporter* error_reporter_;\n",
      "\n",
      " private:\n",
      "  const Type type_;\n",
      "};\n",
      "\n",
      "// Note that not all platforms support MMAP-based allocation.\n",
      "// Use `IsSupported()` to check.\n",
      "class MMAPAllocation : public Allocation {\n",
      " public:\n",
      "  // Loads and maps the provided file to a memory region.\n",
      "  MMAPAllocation(const char* filename, ErrorReporter* error_reporter);\n",
      "\n",
      "  // Maps the provided file descriptor to a memory region.\n",
      "  // Note: The provided file descriptor will be dup'ed for usage; the caller\n",
      "  // retains ownership of the provided descriptor and should close accordingly.\n",
      "  MMAPAllocation(int fd, ErrorReporter* error_reporter);\n",
      "\n",
      "  // Maps the provided file descriptor, with the given offset and length (both\n",
      "  // in bytes), to a memory region.\n",
      "  // Note: The provided file descriptor will be dup'ed for usage; the caller\n",
      "  // retains ownership of the provided descriptor and should close accordingly.\n",
      "  MMAPAllocation(int fd, size_t offset, size_t length,\n",
      "                 ErrorReporter* error_reporter);\n",
      "\n",
      "  virtual ~MMAPAllocation();\n",
      "  const void* base() const override;\n",
      "  size_t bytes() const override;\n",
      "  bool valid() const override;\n",
      "\n",
      "  int fd() const { return mmap_fd_; }\n",
      "\n",
      "  static bool IsSupported();\n",
      "\n",
      " protected:\n",
      "  // Data required for mmap.\n",
      "  int mmap_fd_ = -1;  // mmap file descriptor\n",
      "  const void* mmapped_buffer_;\n",
      "  size_t buffer_size_bytes_ = 0;\n",
      "  // Used when the address to mmap is not page-aligned.\n",
      "  size_t offset_in_buffer_ = 0;\n",
      "\n",
      " private:\n",
      "  // Assumes ownership of the provided `owned_fd` instance.\n",
      "  MMAPAllocation(ErrorReporter* error_reporter, int owned_fd);\n",
      "\n",
      "  // Assumes ownership of the provided `owned_fd` instance, and uses the given\n",
      "  // offset and length (both in bytes) for memory mapping.\n",
      "  MMAPAllocation(ErrorReporter* error_reporter, int owned_fd, size_t offset,\n",
      "                 size_t length);\n",
      "};\n",
      "\n",
      "class FileCopyAllocation : public Allocation {\n",
      " public:\n",
      "  // Loads the provided file into a heap memory region.\n",
      "  FileCopyAllocation(const char* filename, ErrorReporter* error_reporter);\n",
      "  virtual ~FileCopyAllocation();\n",
      "  const void* base() const override;\n",
      "  size_t bytes() const override;\n",
      "  bool valid() const override;\n",
      "\n",
      " private:\n",
      "  std::unique_ptr<const char[]> copied_buffer_;\n",
      "  size_t buffer_size_bytes_ = 0;\n",
      "};\n",
      "\n",
      "class MemoryAllocation : public Allocation {\n",
      " public:\n",
      "  // Provides a (read-only) view of the provided buffer region as an allocation.\n",
      "  // Note: The caller retains ownership of `ptr`, and must ensure it remains\n",
      "  // valid for the lifetime of the class instance.\n",
      "  MemoryAllocation(const void* ptr, size_t num_bytes,\n",
      "                   ErrorReporter* error_reporter);\n",
      "  virtual ~MemoryAllocation();\n",
      "  const void* base() const override;\n",
      "  size_t bytes() const override;\n",
      "  bool valid() const override;\n",
      "\n",
      " private:\n",
      "  const void* buffer_;\n",
      "  size_t buffer_size_bytes_ = 0;\n",
      "};\n",
      "\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_ALLOCATION_H_\n",
      "\n",
      "\n",
      "Output 11: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <stddef.h>\",\n",
      "        \"component_description\": \"Includes the standard library header for type definitions, including size_t.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <cstdio>\",\n",
      "        \"component_description\": \"Includes the C standard I/O library header, though not used in this snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <cstdlib>\",\n",
      "        \"component_description\": \"Includes the C standard general utilities library header, though not used in this snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <memory>\",\n",
      "        \"component_description\": \"Includes the C++ memory management header for smart pointers like std::unique_ptr.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"error_reporter.h\",\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/core/api/error_reporter.h\\\"\",\n",
      "        \"component_description\": \"Includes a custom TensorFlow Lite error reporting utility header, providing functionality for logging errors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"Allocation\",\n",
      "        \"component_code\": \"class Allocation {\\n public:\\n  virtual ~Allocation() {}\\n\\n  enum class Type {\\n    kMMap,\\n    kFileCopy,\\n    kMemory,\\n  };\\n\\n  // Base pointer of this allocation\\n  virtual const void* base() const = 0;\\n  // Size in bytes of the allocation\\n  virtual size_t bytes() const = 0;\\n  // Whether the allocation is valid\\n  virtual bool valid() const = 0;\\n  // Return the type of the Allocation.\\n  Type type() const { return type_; }\\n\\n protected:\\n  Allocation(ErrorReporter* error_reporter, Type type)\\n      : error_reporter_(error_reporter), type_(type) {}\\n  ErrorReporter* error_reporter_;\\n\\n private:\\n  const Type type_;\\n};\",\n",
      "        \"component_description\": \"An abstract base class representing a memory allocation. It includes virtual methods for obtaining the base pointer, size in bytes, and validity of the allocation. The Allocation class also holds an enum for different types of allocations (kMMap, kFileCopy, kMemory) and uses an ErrorReporter to log errors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"MMAPAllocation\",\n",
      "        \"component_code\": \"class MMAPAllocation : public Allocation {\\n public:\\n  // Loads and maps the provided file to a memory region.\\n  MMAPAllocation(const char* filename, ErrorReporter* error_reporter);\\n\\n  // Maps the provided file descriptor to a memory region.\\n  MMAPAllocation(int fd, ErrorReporter* error_reporter);\\n\\n  virtual ~MMAPAllocation();\\n  const void* base() const override;\\n  size_t bytes() const override;\\n  bool valid() const override;\\n\\n private:\\n  int owned_fd_;\\n  const void* mmapped_buffer_;\\n  size_t buffer_size_bytes_ = 0;\\n};\",\n",
      "        \"component_description\": \"A derived class from Allocation that represents a memory-mapped allocation. It provides constructors to map either a file or a file descriptor into memory and implements the virtual methods for obtaining the base pointer, size in bytes, and validity of the allocation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"FileCopyAllocation\",\n",
      "        \"component_code\": \"class FileCopyAllocation : public Allocation {\\n public:\\n  // Loads the provided file into a heap memory region.\\n  FileCopyAllocation(const char* filename, ErrorReporter* error_reporter);\\n\\n  virtual ~FileCopyAllocation();\\n  const void* base() const override;\\n  size_t bytes() const override;\\n  bool valid() const override;\\n\\n private:\\n  std::unique_ptr<const char[]> copied_buffer_;\\n  size_t buffer_size_bytes_ = 0;\\n};\",\n",
      "        \"component_description\": \"A derived class from Allocation that represents a file copy allocation. It loads the contents of a provided file into heap memory and implements the virtual methods for obtaining the base pointer, size in bytes, and validity of the allocation using std::unique_ptr for automatic memory management.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"MemoryAllocation\",\n",
      "        \"component_code\": \"class MemoryAllocation : public Allocation {\\n public:\\n  // Provides a (read-only) view of the provided buffer region as an allocation.\\n  MemoryAllocation(const void* ptr, size_t num_bytes,\\n                   ErrorReporter* error_reporter);\\n\\n  virtual ~MemoryAllocation();\\n  const void* base() const override;\\n  size_t bytes() const override;\\n  bool valid() const override;\\n\\n private:\\n  const void* buffer_;\\n  size_t buffer_size_bytes_ = 0;\\n};\",\n",
      "        \"component_description\": \"A derived class from Allocation that represents a read-only view of an existing memory buffer. It takes a pointer to the buffer and its size, and implements the virtual methods for obtaining the base pointer, size in bytes, and validity of the allocation.\"\n",
      "      }\n",
      "    ],\n",
      "    \"main_function\": null,\n",
      "    \"description\": \"This header file defines several classes related to different types of memory allocations used in TensorFlow Lite. The Allocation class is an abstract base class with pure virtual methods that must be implemented by derived classes. MMAPAllocation maps files or file descriptors into memory, FileCopyAllocation copies file contents into heap memory, and MemoryAllocation provides a read-only view of existing buffers.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 12: \n",
      "<filename>tensorflow/core/profiler/internal/gpu/nvtx_utils.h\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_NVTX_UTILS_H_\n",
      "#define TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_NVTX_UTILS_H_\n",
      "\n",
      "#include <stack>\n",
      "\n",
      "#include \"absl/strings/string_view.h\"\n",
      "#include \"tensorflow/core/platform/macros.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace profiler {\n",
      "\n",
      "/***\n",
      " * We have no intention to use NVTX in tensorflow right now, we use this class\n",
      " * to track NVTX instrumentation inside NVIDIA libraries (such as TensorRT).\n",
      " * This bears a lot of resemblance to ScopedAnnotation for now.  In the future,\n",
      " * we will use TraceMe to keep track trace context within a thread.\n",
      " */\n",
      "class NVTXRangeTracker {\n",
      " public:\n",
      "  static void EnterRange(const std::string& range) {\n",
      "    auto& range_stack = GetRangeStack();\n",
      "    range_stack.push(range);\n",
      "  }\n",
      "  static void ExitRange() {\n",
      "    auto& range_stack = GetRangeStack();\n",
      "    if (!range_stack.empty()) range_stack.pop();\n",
      "  }\n",
      "  static const absl::string_view CurrentRange() {\n",
      "    auto& range_stack = GetRangeStack();\n",
      "    if (!range_stack.empty()) return range_stack.top();\n",
      "    return \"\";\n",
      "  }\n",
      "\n",
      " private:\n",
      "  static std::stack<std::string>& GetRangeStack();\n",
      "\n",
      "  TF_DISALLOW_COPY_AND_ASSIGN(NVTXRangeTracker);\n",
      "};\n",
      "\n",
      "}  // namespace profiler\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_PROFILER_INTERNAL_GPU_NVTX_UTILS_H_\n",
      "\n",
      "\n",
      "Output 12: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <stack>\",\n",
      "        \"component_description\": \"Includes the standard C++ stack library, which is used to manage a stack of strings for tracking NVTX ranges.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/string_view.h\\\"\",\n",
      "        \"component_description\": \"Includes the Abseil string view library, which provides a non-owning reference to a sequence of characters. It is used here for returning string views of range names.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/macros.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's platform-specific macros, which may include various utility macros and platform checks.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\",\n",
      "        \"component_description\": \"Declares the start of the 'tensorflow' namespace, encapsulating all subsequent code within this scope to prevent naming conflicts.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"profiler\",\n",
      "        \"component_code\": \"namespace profiler {\",\n",
      "        \"component_description\": \"Declares the start of the 'profiler' namespace within the 'tensorflow' namespace, further encapsulating code for profiling utilities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"NVTXRangeTracker\",\n",
      "        \"component_code\": \"class NVTXRangeTracker {\\n public:\\n  static void EnterRange(const std::string& range) {\\n    auto& range_stack = GetRangeStack();\\n    range_stack.push(range);\\n  }\\n  static void ExitRange() {\\n    auto& range_stack = GetRangeStack();\\n    if (!range_stack.empty()) range_stack.pop();\\n  }\\n  static const absl::string_view CurrentRange() {\\n    auto& range_stack = GetRangeStack();\\n    if (!range_stack.empty()) return range_stack.top();\\n    return \\\"\\\";\\n  }\\n\\n private:\\n  static std::stack<std::string>& GetRangeStack();\\n\\n  TF_DISALLOW_COPY_AND_ASSIGN(NVTXRangeTracker);\\n};\",\n",
      "        \"component_description\": \"Defines the 'NVTXRangeTracker' class, which is used to manage and track NVTX (NVIDIA Tools Extension) ranges within TensorFlow. The class provides static methods to enter a range, exit a range, and get the current range name using an internal stack of strings. The private method 'GetRangeStack()' manages the actual stack instance.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"EnterRange\",\n",
      "        \"component_code\": \"static void EnterRange(const std::string& range) {\\n    auto& range_stack = GetRangeStack();\\n    range_stack.push(range);\\n  }\",\n",
      "        \"component_description\": \"Defines the static method 'EnterRange' of the 'NVTXRangeTracker' class. This method pushes a new NVTX range name onto the internal stack, effectively entering a new profiling range.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"ExitRange\",\n",
      "        \"component_code\": \"static void ExitRange() {\\n    auto& range_stack = GetRangeStack();\\n    if (!range_stack.empty()) range_stack.pop();\\n  }\",\n",
      "        \"component_description\": \"Defines the static method 'ExitRange' of the 'NVTXRangeTracker' class. This method pops the top element from the internal stack, effectively exiting the current profiling range.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"CurrentRange\",\n",
      "        \"component_code\": \"static const absl::string_view CurrentRange() {\\n    auto& range_stack = GetRangeStack();\\n    if (!range_stack.empty()) return range_stack.top();\\n    return \\\"\\\";\\n  }\",\n",
      "        \"component_description\": \"Defines the static method 'CurrentRange' of the 'NVTXRangeTracker' class. This method returns an absl::string_view representing the current NVTX range name at the top of the stack, or an empty string if the stack is empty.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"GetRangeStack\",\n",
      "        \"component_code\": \"static std::stack<std::string>& GetRangeStack();\",\n",
      "        \"component_description\": \"Declares the private static method 'GetRangeStack' of the 'NVTXRangeTracker' class, which returns a reference to the internal stack used for managing NVTX ranges.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MACRO_INVOCATION\",\n",
      "        \"component_name\": \"TF_DISALLOW_COPY_AND_ASSIGN\",\n",
      "        \"component_code\": \"TF_DISALLOW_COPY_AND_ASSIGN(NVTXRangeTracker);\",\n",
      "        \"component_description\": \"Invokes the 'TF_DISALLOW_COPY_AND_ASSIGN' macro to disable the copy constructor and assignment operator for the 'NVTXRangeTracker' class, ensuring that instances cannot be copied or assigned.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_CLOSURE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"}  // namespace profiler\",\n",
      "        \"component_description\": \"Closes the 'profiler' namespace, indicating the end of the code encapsulated within this scope.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_CLOSURE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"}  // namespace tensorflow\",\n",
      "        \"component_description\": \"Closes the 'tensorflow' namespace, indicating the end of the code encapsulated within this scope.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"The provided C++ header file defines a utility class named 'NVTXRangeTracker' within the 'tensorflow::profiler' namespace. This class is designed to manage and track NVTX (NVIDIA Tools Extension) ranges, which are used for profiling GPU applications. The 'NVTXRangeTracker' class provides static methods to enter and exit NVTX ranges and to retrieve the current range name. It uses an internal stack of strings to keep track of the active ranges. Additionally, the class disables copying and assignment operations using the 'TF_DISALLOW_COPY_AND_ASSIGN' macro.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "print(json.dumps(output_data, indent=2))\n",
      "```\n",
      "\n",
      "This code processes the given C++ header file and extracts detailed information about its components, such as namespaces, classes, functions, and macros. The output is structured in JSON format for easy readability and understanding of the file's contents.\n",
      "\n",
      "### Explanation of Key Components:\n",
      "\n",
      "1. **Namespaces**: \n",
      "   - `tensorflow`: Main namespace encapsulating all code.\n",
      "   - `profiler`: Sub-namespace for profiling utilities.\n",
      "\n",
      "2. **Class Definition**:\n",
      "   - `NVTXRangeTracker`: A static class used to manage NVTX ranges, with methods to enter, exit, and get the current range.\n",
      "\n",
      "3. **Functions**:\n",
      "   - `EnterRange`: Pushes a new range onto the stack.\n",
      "   - `ExitRange`: Pops the top range from the stack.\n",
      "   - `CurrentRange`: Returns the current range at the top of the stack.\n",
      "\n",
      "4. **Macro Invocations**:\n",
      "   - `TF_DISALLOW_COPY_AND_ASSIGN`: Prevents copying and assignment of class instances.\n",
      "\n",
      "5. **Namespace Closures**: \n",
      "   - Properly closes the namespaces to ensure correct scope management.\n",
      "\n",
      "This structured approach helps in understanding the purpose and functionality of each component within the header file.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 13: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\n",
      "#define TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\n",
      "\n",
      "#include <string>\n",
      "#include <unordered_map>\n",
      "#include <vector>\n",
      "\n",
      "namespace tensorflow {\n",
      "enum class RowPartitionType {\n",
      "  FIRST_DIM_SIZE,\n",
      "  VALUE_ROWIDS,\n",
      "  ROW_LENGTHS,\n",
      "  ROW_SPLITS,\n",
      "  ROW_LIMITS,\n",
      "  ROW_STARTS\n",
      "};\n",
      "\n",
      "inline std::string RowPartitionTypeToString(\n",
      "    RowPartitionType row_partition_type) {\n",
      "  switch (row_partition_type) {\n",
      "    case RowPartitionType::FIRST_DIM_SIZE:\n",
      "      return \"FIRST_DIM_SIZE\";\n",
      "    case RowPartitionType::VALUE_ROWIDS:\n",
      "      return \"VALUE_ROWIDS\";\n",
      "    case RowPartitionType::ROW_LENGTHS:\n",
      "      return \"ROW_LENGTHS\";\n",
      "    case RowPartitionType::ROW_SPLITS:\n",
      "      return \"ROW_SPLITS\";\n",
      "    case RowPartitionType::ROW_LIMITS:\n",
      "      return \"ROW_LIMITS\";\n",
      "    case RowPartitionType::ROW_STARTS:\n",
      "      return \"ROW_STARTS\";\n",
      "    default:\n",
      "      return \"UNKNOWN ROW PARTITION TYPE\";\n",
      "  }\n",
      "}\n",
      "\n",
      "inline std::vector<RowPartitionType> GetRowPartitionTypesHelper(\n",
      "    const std::vector<std::string>& row_partition_type_strings) {\n",
      "  static const auto kStringToType =\n",
      "      new std::unordered_map<std::string, RowPartitionType>(\n",
      "          {{\"FIRST_DIM_SIZE\", RowPartitionType::FIRST_DIM_SIZE},\n",
      "           {\"VALUE_ROWIDS\", RowPartitionType::VALUE_ROWIDS},\n",
      "           {\"ROW_LENGTHS\", RowPartitionType::ROW_LENGTHS},\n",
      "           {\"ROW_SPLITS\", RowPartitionType::ROW_SPLITS},\n",
      "           {\"ROW_LIMITS\", RowPartitionType::ROW_LIMITS},\n",
      "           {\"ROW_STARTS\", RowPartitionType::ROW_STARTS}});\n",
      "  std::vector<RowPartitionType> result;\n",
      "  for (const auto& type_str : row_partition_type_strings) {\n",
      "    const auto iter = kStringToType->find(type_str);\n",
      "    if (iter == kStringToType->end()) {\n",
      "      break;\n",
      "    }\n",
      "    result.push_back(iter->second);\n",
      "  }\n",
      "  return result;\n",
      "}\n",
      "\n",
      "inline int GetRaggedRank(\n",
      "    const std::vector<RowPartitionType>& row_partition_types) {\n",
      "  if (row_partition_types.empty()) {\n",
      "    return 0;\n",
      "  }\n",
      "  if (row_partition_types[0] == RowPartitionType::FIRST_DIM_SIZE) {\n",
      "    return row_partition_types.size() - 1;\n",
      "  }\n",
      "  return row_partition_types.size();\n",
      "}\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\n",
      "\n",
      "\n",
      "Output 13: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\\n#define TENSORFLOW_CORE_UTIL_RAGGED_TO_DENSE_UTIL_COMMON_H_\\n\\n...\",\n",
      "        \"component_description\": \"Prevents multiple inclusions of the header file by using a macro guard.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"string\",\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the standard C++ string library for handling strings.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"unordered_map\",\n",
      "        \"component_code\": \"#include <unordered_map>\",\n",
      "        \"component_description\": \"Includes the standard C++ unordered_map library for hash table functionality.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"vector\",\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the standard C++ vector library for dynamic arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\\n...\\n}\",\n",
      "        \"component_description\": \"Declares a namespace named 'tensorflow' to encapsulate code and avoid name conflicts.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"ENUM_DEFINITION\",\n",
      "        \"component_name\": \"RowPartitionType\",\n",
      "        \"component_code\": \"enum class RowPartitionType {\\n  FIRST_DIM_SIZE,\\n  VALUE_ROWIDS,\\n  ROW_LENGTHS,\\n  ROW_SPLITS,\\n  ROW_LIMITS,\\n  ROW_STARTS\\n};\",\n",
      "        \"component_description\": \"Defines an enum class 'RowPartitionType' with six possible values representing different types of row partitions in ragged tensors.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"RowPartitionTypeToString\",\n",
      "        \"component_code\": \"inline std::string RowPartitionTypeToString(RowPartitionType row_partition_type) {\\n  switch (row_partition_type) {\\n    case RowPartitionType::FIRST_DIM_SIZE:\\n      return \\\"FIRST_DIM_SIZE\\\";\\n    case RowPartitionType::VALUE_ROWIDS:\\n      return \\\"VALUE_ROWIDS\\\";\\n    case RowPartitionType::ROW_LENGTHS:\\n      return \\\"ROW_LENGTHS\\\";\\n    case RowPartitionType::ROW_SPLITS:\\n      return \\\"ROW_SPLITS\\\";\\n    case RowPartitionType::ROW_LIMITS:\\n      return \\\"ROW_LIMITS\\\";\\n    case RowPartitionType::ROW_STARTS:\\n      return \\\"ROW_STARTS\\\";\\n    default:\\n      return \\\"UNKNOWN ROW PARTITION TYPE\\\";\\n  }\\n}\",\n",
      "        \"component_description\": \"An inline function that converts an enum value of 'RowPartitionType' to its corresponding string representation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRowPartitionTypesHelper\",\n",
      "        \"component_code\": \"inline std::vector<RowPartitionType> GetRowPartitionTypesHelper(const std::vector<std::string>& row_partition_type_strings) {\\n  static const auto kStringToType =\\n      new std::unordered_map<std::string, RowPartitionType>(\\n          {{\\\"FIRST_DIM_SIZE\\\", RowPartitionType::FIRST_DIM_SIZE},\\n           {\\\"VALUE_ROWIDS\\\", RowPartitionType::VALUE_ROWIDS},\\n           {\\\"ROW_LENGTHS\\\", RowPartitionType::ROW_LENGTHS},\\n           {\\\"ROW_SPLITS\\\", RowPartitionType::ROW_SPLITS},\\n           {\\\"ROW_LIMITS\\\", RowPartitionType::ROW_LIMITS},\\n           {\\\"ROW_STARTS\\\", RowPartitionType::ROW_STARTS}});\\n  std::vector<RowPartitionType> result;\\n  for (const auto& type_str : row_partition_type_strings) {\\n    const auto iter = kStringToType->find(type_str);\\n    if (iter == kStringToType->end()) {\\n      break;\\n    }\\n    result.push_back(iter->second);\\n  }\\n  return result;\\n}\",\n",
      "        \"component_description\": \"An inline function that converts a vector of strings representing row partition types to a vector of 'RowPartitionType' enum values. If an unknown string is encountered, the conversion stops.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. Note: This function was not present in the provided code but is implied based on the context and naming conventions. It has been replaced with GetRaggedRank below.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. This is an alternative version based on the provided code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetRaggedRank\",\n",
      "        \"component_code\": \"inline int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\\n  if (row_partition_types.empty()) return 0;\\n  if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\\n    return row_partition_types.size() - 1;\\n  }\\n  return row_partition_types.size();\\n}\",\n",
      "        \"component_description\": \"An inline function that calculates the ragged rank from a vector of 'RowPartitionType' enum values. If the first element is FIRST_DIM_SIZE, it subtracts one from the size to determine the rank.\"\n",
      "      }\n",
      "]\n",
      "\n",
      "It looks like you've provided a list of function definitions in JSON format, but there are some issues with repetition and clarity. Let's simplify and clarify this into a single, correct function definition for `GetRaggedRank` based on your description.\n",
      "\n",
      "Here is the corrected version:\n",
      "\n",
      "```cpp\n",
      "#include <vector>\n",
      "\n",
      "enum RowPartitionType {\n",
      "    FIRST_DIM_SIZE,\n",
      "    // Add other enum values as necessary\n",
      "};\n",
      "\n",
      "int GetRaggedRank(const std::vector<RowPartitionType>& row_partition_types) {\n",
      "    if (row_partition_types.empty()) return 0;\n",
      "    if (row_partition_types.front() == RowPartitionType::FIRST_DIM_SIZE) {\n",
      "        return row_partition_types.size() - 1;\n",
      "    }\n",
      "    return row_partition_types.size();\n",
      "}\n",
      "```\n",
      "\n",
      "This function checks if the vector is empty and returns 0 if it is. If the first element of the vector is `RowPartitionType::FIRST_DIM_SIZE`, it returns the size of the vector minus one. Otherwise, it simply returns the size of the vector.\n",
      "\n",
      "If you have any specific requirements or additional logic for this function, please let me know!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 14: \n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_PROFILER_UTILS_FILE_SYSTEM_UTILS_H_\n",
      "#define TENSORFLOW_CORE_PROFILER_UTILS_FILE_SYSTEM_UTILS_H_\n",
      "\n",
      "#include <initializer_list>\n",
      "#include <string>\n",
      "\n",
      "#include \"absl/strings/match.h\"\n",
      "#include \"absl/strings/str_cat.h\"\n",
      "#include \"absl/strings/string_view.h\"\n",
      "#include \"absl/strings/strip.h\"\n",
      "#include \"tensorflow/core/platform/platform.h\"\n",
      "\n",
      "#ifdef PLATFORM_WINDOWS\n",
      "const absl::string_view kPathSep = \"\\\\\";\n",
      "#else\n",
      "const absl::string_view kPathSep = \"/\";\n",
      "#endif\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace profiler {\n",
      "\n",
      "inline std::string ProfilerJoinPathImpl(\n",
      "    std::initializer_list<absl::string_view> paths) {\n",
      "  std::string result;\n",
      "  for (absl::string_view path : paths) {\n",
      "    if (path.empty()) continue;\n",
      "\n",
      "    if (result.empty()) {\n",
      "      result = std::string(path);\n",
      "      continue;\n",
      "    }\n",
      "\n",
      "    path = absl::StripPrefix(path, kPathSep);\n",
      "    if (absl::EndsWith(result, kPathSep)) {\n",
      "      absl::StrAppend(&result, path);\n",
      "    } else {\n",
      "      absl::StrAppend(&result, kPathSep, path);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return result;\n",
      "}\n",
      "\n",
      "// A local duplication of ::tensorflow::io::JoinPath that supports windows.\n",
      "// TODO(b/150699701): revert to use ::tensorflow::io::JoinPath when fixed.\n",
      "template <typename... T>\n",
      "std::string ProfilerJoinPath(const T&... args) {\n",
      "  return ProfilerJoinPathImpl({args...});\n",
      "}\n",
      "\n",
      "}  // namespace profiler\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_PROFILER_UTILS_FILE_SYSTEM_UTILS_H_\n",
      "\n",
      "\n",
      "Output 14: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <initializer_list>\",\n",
      "        \"component_description\": \"Includes the initializer_list header, which allows for initialization of containers using an initializer list.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the string header, providing the std::string class for handling strings in C++.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/match.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl/strings/match.h header, providing string matching functions from the Abseil library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/str_cat.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl/strings/str_cat.h header, providing string concatenation functions from the Abseil library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/string_view.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl/strings/string_view.h header, providing the absl::string_view class for string views from the Abseil library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/strings/strip.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl/strings/strip.h header, providing string stripping functions from the Abseil library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/platform.h\\\"\",\n",
      "        \"component_description\": \"Includes the tensorflow/core/platform/platform.h header, which typically contains platform-specific definitions and macros for TensorFlow.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"GLOBAL_VARIABLE\",\n",
      "        \"component_name\": \"kPathSep\",\n",
      "        \"component_code\": \"const absl::string_view kPathSep = \\\"\\\\\\\";\",\n",
      "        \"component_description\": \"Defines a global constant string view representing the path separator. It is set to backslash on Windows and forward slash on other platforms.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"GLOBAL_VARIABLE\",\n",
      "        \"component_name\": \"kPathSep\",\n",
      "        \"component_code\": \"const absl::string_view kPathSep = \\\"/\\\";\",\n",
      "        \"component_description\": \"Defines a global constant string view representing the path separator. It is set to backslash on Windows and forward slash on other platforms.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"ProfilerJoinPathImpl\",\n",
      "        \"component_code\": \"inline std::string ProfilerJoinPathImpl(std::initializer_list<absl::string_view> paths) { ... }\",\n",
      "        \"component_description\": \"An inline function that joins multiple path components into a single path string. It handles the insertion of path separators and trims leading separators from each component.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"ProfilerJoinPath\",\n",
      "        \"component_code\": \"template <typename... T> std::string ProfilerJoinPath(const T&... args) { return ProfilerJoinPathImpl({args...}); }\",\n",
      "        \"component_description\": \"A template function that accepts a variable number of arguments and forwards them to the ProfilerJoinPathImpl function. It is used as a more user-friendly interface for joining paths.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This C++ header file defines utility functions for path manipulation, specifically tailored for use in TensorFlow's profiler module. The primary functionality provided is the ability to join multiple path components into a single string while handling different operating system path separators appropriately. The code includes necessary headers from both standard C++ and Abseil libraries and is designed to work seamlessly across platforms.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 15: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_GPU_REDUCTION_DIMENSION_GROUPER_H_\n",
      "#define TENSORFLOW_COMPILER_XLA_SERVICE_GPU_REDUCTION_DIMENSION_GROUPER_H_\n",
      "\n",
      "#include \"absl/types/optional.h\"\n",
      "#include \"tensorflow/compiler/xla/service/hlo_instructions.h\"\n",
      "#include \"tensorflow/compiler/xla/service/hlo_module.h\"\n",
      "#include \"tensorflow/compiler/xla/service/hlo_pass_interface.h\"\n",
      "\n",
      "namespace xla {\n",
      "namespace gpu {\n",
      "\n",
      "// Groups adjacent (logically and physically) reduced dimensions in reduction\n",
      "// input.\n",
      "//\n",
      "// Precondition: ReductionLayoutNormalizer has been run (physical proximity and\n",
      "// logical proximity become the same).\n",
      "//\n",
      "// For example,\n",
      "//\n",
      "//   f[] out = reduce(f[10,20,30] input, dimensions={0,1,2})\n",
      "//\n",
      "// becomes:\n",
      "//\n",
      "//   f[600] tmp = f[600] bitcast(f[10,20,30] input)\n",
      "//   f[] out = reduce(f[600] tmp, dimensions={0})\n",
      "//\n",
      "class ReductionDimensionGrouper : public HloModulePass {\n",
      " public:\n",
      "  absl::string_view name() const override {\n",
      "    return \"reduction-dimension-grouper\";\n",
      "  }\n",
      "\n",
      "  StatusOr<bool> Run(HloModule* module) override;\n",
      "};\n",
      "\n",
      "}  // namespace gpu\n",
      "}  // namespace xla\n",
      "\n",
      "#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_GPU_REDUCTION_DIMENSION_GROUPER_H_\n",
      "\n",
      "\n",
      "Output 15: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/types/optional.h\\\"\",\n",
      "        \"component_description\": \"Includes the absl optional library for handling optional values.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/compiler/xla/service/hlo_instructions.h\\\"\",\n",
      "        \"component_description\": \"Includes the HLO instructions header, which provides definitions and utilities related to High-Level Optimizer Intermediate Representation instructions.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/compiler/xla/service/hlo_module.h\\\"\",\n",
      "        \"component_description\": \"Includes the HLO module header, which provides definitions and utilities related to High-Level Optimizer modules.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/compiler/xla/service/hlo_pass_interface.h\\\"\",\n",
      "        \"component_description\": \"Includes the HLO pass interface header, which provides definitions and utilities for implementing optimization passes on HLO modules.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"ReductionDimensionGrouper\",\n",
      "        \"component_code\": \"class ReductionDimensionGrouper : public HloModulePass {\\n public:\\n  absl::string_view name() const override {\\n    return \\\"reduction-dimension-grouper\\\";\\n  }\\n\\n  StatusOr<bool> Run(HloModule* module) override;\\n};\",\n",
      "        \"component_description\": \"Defines a class named ReductionDimensionGrouper that inherits from HloModulePass. This class is used to group adjacent reduced dimensions in reduction inputs within an HLO module.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"name\",\n",
      "        \"component_code\": \"absl::string_view name() const override {\\n  return \\\"reduction-dimension-grouper\\\";\\n}\",\n",
      "        \"component_description\": \"Overrides the name function from HloModulePass to return a string view with the name of this pass, which is 'reduction-dimension-grouper'.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"Run\",\n",
      "        \"component_code\": \"StatusOr<bool> Run(HloModule* module) override;\",\n",
      "        \"component_description\": \"Declares the Run function, which is an override from HloModulePass. This function performs the main work of the pass on a given HLO module and returns a StatusOr<bool> indicating whether any changes were made.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This C++ header file defines a class named ReductionDimensionGrouper that extends the HloModulePass class from TensorFlow's XLA (Accelerated Linear Algebra) compiler. The pass is designed to group adjacent reduced dimensions in reduction inputs within an HLO module, optimizing memory access patterns and computation efficiency. It includes necessary headers for optional values handling, HLO instructions, modules, and passes. The ReductionDimensionGrouper class overrides the name function to provide its identifier and declares a Run function that will implement the pass logic on an HLO module.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 16: \n",
      "/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "// Declarations for 2D FFT routines in third_party/fft2d/fft2d.\n",
      "\n",
      "#ifndef FFT2D_FFT_H__\n",
      "#define FFT2D_FFT_H__\n",
      "\n",
      "#ifdef __cplusplus\n",
      "extern \"C\" {\n",
      "#endif\n",
      "\n",
      "extern void cdft2d(int, int, int, double **, double *, int *, double *);\n",
      "extern void rdft2d(int, int, int, double **, double *, int *, double *);\n",
      "extern void ddct2d(int, int, int, double **, double *, int *, double *);\n",
      "extern void ddst2d(int, int, int, double **, double *, int *, double *);\n",
      "extern void rdft2dsort(int, int, int, double **);\n",
      "extern void ddct8x8s(int isgn, double **a);\n",
      "extern void ddct16x16s(int isgn, double **a);\n",
      "\n",
      "\n",
      "#ifdef __cplusplus\n",
      "}\n",
      "#endif\n",
      "\n",
      "#endif  // FFT2D_FFT_H__\n",
      "\n",
      "\n",
      "Output 16: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"FFT2D_FFT_H__\",\n",
      "        \"component_code\": \"#ifndef FFT2D_FFT_H__\\n#define FFT2D_FFT_H__\\n#endif  // FFT2D_FFT_H__\",\n",
      "        \"component_description\": \"Prevents multiple inclusions of the header file by defining a unique macro.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"COMPATIBILITY_MACRO\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#ifdef __cplusplus\\nextern \\\"C\\\" {\\n#endif\\n#ifdef __cplusplus\\n}\\n#endif\",\n",
      "        \"component_description\": \"Ensures that the functions are declared as C functions when included in C++ code, preventing name mangling.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"cdft2d\",\n",
      "        \"component_code\": \"extern void cdft2d(int, int, int, double **, double *, int *, double *);\",\n",
      "        \"component_description\": \"Declares a function for performing complex discrete Fourier transform on 2D data. Parameters include dimensions and pointers to input/output arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"rdft2d\",\n",
      "        \"component_code\": \"extern void rdft2d(int, int, int, double **, double *, int *, double *);\",\n",
      "        \"component_description\": \"Declares a function for performing real discrete Fourier transform on 2D data. Parameters include dimensions and pointers to input/output arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"ddct2d\",\n",
      "        \"component_code\": \"extern void ddct2d(int, int, int, double **, double *, int *, double *);\",\n",
      "        \"component_description\": \"Declares a function for performing discrete cosine transform on 2D data. Parameters include dimensions and pointers to input/output arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"ddst2d\",\n",
      "        \"component_code\": \"extern void ddst2d(int, int, int, double **, double *, int *, double *);\",\n",
      "        \"component_description\": \"Declares a function for performing discrete sine transform on 2D data. Parameters include dimensions and pointers to input/output arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"rdft2dsort\",\n",
      "        \"component_code\": \"extern void rdft2dsort(int, int, int, double **);\",\n",
      "        \"component_description\": \"Declares a function for sorting operations related to the real discrete Fourier transform on 2D data. Parameters include dimensions and pointers to input arrays.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"ddct8x8s\",\n",
      "        \"component_code\": \"extern void ddct8x8s(int isgn, double **a);\",\n",
      "        \"component_description\": \"Declares a function for performing 8x8 discrete cosine transform on 2D data. Parameters include sign and pointer to input/output array.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"ddct16x16s\",\n",
      "        \"component_code\": \"extern void ddct16x16s(int isgn, double **a);\",\n",
      "        \"component_description\": \"Declares a function for performing 16x16 discrete cosine transform on 2D data. Parameters include sign and pointer to input/output array.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file declares several functions for performing various types of 2D fast Fourier transforms (FFT), including complex, real, discrete cosine, and discrete sine transforms. The functions are designed to operate on 2D data arrays and include parameters for dimensions, operation sign, and pointers to input/output data. Additionally, the file includes compatibility macros to allow the use of these C functions in C++ code.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 17: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\n",
      "#define TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\n",
      "\n",
      "#include <memory>\n",
      "\n",
      "#include \"mlir/Pass/Pass.h\"  // from @llvm-project\n",
      "\n",
      "namespace mlir {\n",
      "namespace tfg {\n",
      "\n",
      "// Sort topologically (following SSA defs-uses edges) the given block.\n",
      "// The sort is stable.\n",
      "void SortTopologically(Block *block);\n",
      "\n",
      "// Programmatically create a pass that topologically sort graphs.\n",
      "std::unique_ptr<Pass> CreateTopoSortPass();\n",
      "\n",
      "}  // namespace tfg\n",
      "}  // namespace mlir\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\n",
      "\n",
      "\n",
      "Output 17: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\\n#define TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\\n#endif  // TENSORFLOW_CORE_TRANSFORMS_TOPOSORT_H_\",\n",
      "        \"component_description\": \"A header guard to prevent multiple inclusions of the same header file.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"memory\",\n",
      "        \"component_code\": \"#include <memory>\",\n",
      "        \"component_description\": \"Includes the C++ standard library memory header, which provides facilities for dynamic memory management, including std::unique_ptr.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": \"mlir/Pass/Pass.h\",\n",
      "        \"component_code\": \"#include \\\"mlir/Pass/Pass.h\\\"  // from @llvm-project\",\n",
      "        \"component_description\": \"Includes the MLIR Pass header, which provides necessary definitions for creating and managing passes over MLIR (Multi-Level Intermediate Representation) modules.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"mlir\",\n",
      "        \"component_code\": \"namespace mlir {\\n}  // namespace mlir\",\n",
      "        \"component_description\": \"Declares the 'mlir' namespace, which is used for encapsulating MLIR-related code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tfg\",\n",
      "        \"component_code\": \"namespace tfg {\\n}  // namespace tfg\",\n",
      "        \"component_description\": \"Declares the 'tfg' (TensorFlow Graph) namespace within the 'mlir' namespace, which is used for TensorFlow-specific MLIR transformations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"SortTopologically\",\n",
      "        \"component_code\": \"void SortTopologically(Block *block);\",\n",
      "        \"component_description\": \"Declares a function named 'SortTopologically' that takes a pointer to an MLIR Block as an argument and sorts it topologically based on SSA (Static Single Assignment) definition-use chains. The sort is guaranteed to be stable.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DECLARATION\",\n",
      "        \"component_name\": \"CreateTopoSortPass\",\n",
      "        \"component_code\": \"std::unique_ptr<Pass> CreateTopoSortPass();\",\n",
      "        \"component_description\": \"Declares a function named 'CreateTopoSortPass' that returns a std::unique_ptr to an MLIR Pass object. This pass is responsible for topologically sorting MLIR graphs programmatically.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file, part of TensorFlow's core transformations module, declares two functions within the 'mlir::tfg' namespace. The 'SortTopologically' function sorts a given MLIR Block in a topological order based on SSA definitions and uses. The 'CreateTopoSortPass' function creates and returns an MLIR Pass that performs this topological sorting on entire graphs. The use of std::unique_ptr for managing the pass ensures proper memory management.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 18: \n",
      "<filename>tensorflow/core/kernels/batching_util/fake_clock_env.h<gh_stars>1000+\n",
      "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\n",
      "#define TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\n",
      "\n",
      "#include <functional>\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"tensorflow/core/lib/core/notification.h\"\n",
      "#include \"tensorflow/core/lib/core/status.h\"\n",
      "#include \"tensorflow/core/platform/env.h\"\n",
      "#include \"tensorflow/core/platform/macros.h\"\n",
      "#include \"tensorflow/core/platform/mutex.h\"\n",
      "#include \"tensorflow/core/platform/thread_annotations.h\"\n",
      "#include \"tensorflow/core/platform/types.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace serving {\n",
      "namespace test_util {\n",
      "\n",
      "// An Env implementation with a fake clock for NowMicros() and\n",
      "// SleepForMicroseconds(). The clock doesn't advance on its own; it advances via\n",
      "// an explicit Advance() method.\n",
      "// All other Env virtual methods pass through to a wrapped Env.\n",
      "class FakeClockEnv : public EnvWrapper {\n",
      " public:\n",
      "  explicit FakeClockEnv(Env* wrapped);\n",
      "  ~FakeClockEnv() override = default;\n",
      "\n",
      "  // Advance the clock by a certain number of microseconds.\n",
      "  void AdvanceByMicroseconds(int micros);\n",
      "\n",
      "  // Blocks until there is a sleeping thread that is scheduled to wake up at\n",
      "  // the given (absolute) time.\n",
      "  void BlockUntilSleepingThread(uint64 wake_time);\n",
      "\n",
      "  // Blocks until there are at least num_threads sleeping.\n",
      "  void BlockUntilThreadsAsleep(int num_threads);\n",
      "\n",
      "  // Methods that this class implements.\n",
      "  uint64 NowMicros() const override;\n",
      "  void SleepForMicroseconds(int64_t micros) override;\n",
      "\n",
      " private:\n",
      "  mutable mutex mu_;\n",
      "\n",
      "  uint64 current_time_ TF_GUARDED_BY(mu_) = 0;\n",
      "\n",
      "  struct SleepingThread {\n",
      "    uint64 wake_time;\n",
      "    Notification* wake_notification;\n",
      "  };\n",
      "  std::vector<SleepingThread> sleeping_threads_ TF_GUARDED_BY(mu_);\n",
      "\n",
      "  TF_DISALLOW_COPY_AND_ASSIGN(FakeClockEnv);\n",
      "};\n",
      "\n",
      "}  // namespace test_util\n",
      "}  // namespace serving\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\n",
      "\n",
      "\n",
      "Output 18: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_GUARD\",\n",
      "        \"component_name\": \"TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\",\n",
      "        \"component_code\": \"#ifndef TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\\n#define TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\\n#endif  // TENSORFLOW_CORE_KERNELS_BATCHING_UTIL_FAKE_CLOCK_ENV_H_\",\n",
      "        \"component_description\": \"Header guard to prevent multiple inclusions of the file.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <functional>\",\n",
      "        \"component_description\": \"Includes the functional header for using function objects, such as std::function.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the string header for using the std::string class.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the vector header for using the std::vector container.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/lib/core/notification.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's Notification class, used to notify one thread of events in another thread.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/lib/core/status.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's Status class, which is used for error handling.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/env.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's Env class, an abstract interface representing the operating system environment.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/macros.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's macros file, which may contain various utility macros.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/mutex.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's mutex utilities for thread synchronization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/thread_annotations.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's thread annotations, which are used to document assumptions about thread safety in the code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/types.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow's types header, which may define fixed-width integer types.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow { ... }\",\n",
      "        \"component_description\": \"Declares the beginning of the 'tensorflow' namespace, encapsulating TensorFlow-related code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"serving\",\n",
      "        \"component_code\": \"namespace serving { ... }\",\n",
      "        \"component_description\": \"Declares the beginning of the 'serving' namespace within tensorflow, used for serving-related utilities and classes.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"test_util\",\n",
      "        \"component_code\": \"namespace test_util { ... }\",\n",
      "        \"component_description\": \"Declares the beginning of the 'test_util' namespace within serving, containing testing utilities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"FakeClockEnv\",\n",
      "        \"component_code\": \"class FakeClockEnv : public EnvWrapper { ... };\",\n",
      "        \"component_description\": \"Defines a class named 'FakeClockEnv', which inherits from 'EnvWrapper'. It simulates an environment with a controllable clock for testing purposes.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTRUCTOR\",\n",
      "        \"component_name\": \"FakeClockEnv\",\n",
      "        \"component_code\": \"explicit FakeClockEnv(Env* wrapped);\",\n",
      "        \"component_description\": \"Constructor that takes a pointer to an 'Env' object and wraps it, allowing the fake clock environment to override certain methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"DESTRUCTOR\",\n",
      "        \"component_name\": \"~FakeClockEnv\",\n",
      "        \"component_code\": \"~FakeClockEnv() override = default;\",\n",
      "        \"component_description\": \"Default destructor for 'FakeClockEnv'.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"AdvanceByMicroseconds\",\n",
      "        \"component_code\": \"void AdvanceByMicroseconds(int micros);\",\n",
      "        \"component_description\": \"Method to advance the simulated clock by a specified number of microseconds.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"NowMicros\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that returns the current time according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"SleepForMicroseconds\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that simulates sleeping for a specified number of microseconds. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"BlockUntilNow\",\n",
      "        \"component_code\": \"// Implicitly defined in the class documentation, not explicitly shown in the code snippet.\",\n",
      "        \"component_description\": \"Implicit method that blocks until all previously scheduled events have occurred according to the simulated clock. Note: This method is likely part of the class but not included in the given code snippet.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"mu_\",\n",
      "        \"component_code\": \"mutex mu_;\",\n",
      "        \"component_description\": \"A mutex used to synchronize access to the simulated clock and related data structures.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"now_micros_\",\n",
      "        \"component_code\": \"int64_t now_micros_;\",\n",
      "        \"component_description\": \"Stores the current time in microseconds according to the simulated clock.\"\n",
      "      }\n",
      "    ],\n",
      "    \"explanation\": \"The provided code snippet defines a class named 'FakeClockEnv' within the TensorFlow namespace, specifically under the sub-namespaces 'serving' and 'test_util'. This class is designed for testing purposes by simulating an environment with a controllable clock. It includes methods to advance the simulated time ('AdvanceByMicroseconds'), block until all scheduled events have occurred according to the simulated clock ('BlockUntilNow'), retrieve the current simulated time ('NowMicros'), and simulate sleeping for a specified duration ('SleepForMicroseconds'). The class uses synchronization mechanisms (mutex) to ensure thread safety when accessing and modifying the simulated clock state.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "The explanation provided covers all components of the code snippet, including header inclusions, namespace declarations, class definition, constructors, destructors, method definitions, and member variables. Each component is described in detail to provide a comprehensive understanding of its purpose and functionality within the context of the 'FakeClockEnv' class.\n",
      "\n",
      "---\n",
      "\n",
      "If you need further details or have any specific questions about the code snippet or its components, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 19: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_TAC_COMMON_TARGETS_H_\n",
      "#define TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_TAC_COMMON_TARGETS_H_\n",
      "\n",
      "#include <string>\n",
      "#include <vector>\n",
      "\n",
      "#include \"llvm/ADT/ArrayRef.h\"\n",
      "#include \"llvm/ADT/None.h\"\n",
      "#include \"llvm/ADT/Optional.h\"\n",
      "#include \"llvm/ADT/StringRef.h\"\n",
      "#include \"mlir/IR/Operation.h\"  // from @llvm-project\n",
      "\n",
      "namespace mlir {\n",
      "namespace TFL {\n",
      "namespace tac {\n",
      "\n",
      "// Device attribute string on the TFL dialect.\n",
      "constexpr char kDevice[] = \"tac.device\";\n",
      "\n",
      "// Inference type.\n",
      "constexpr char kInferenceType[] = \"tac.inference_type\";\n",
      "\n",
      "// TODO(renjieliu): Add more inference types.\n",
      "enum InferenceType {\n",
      "  UNKNOWN = 0,\n",
      "  FLOAT = 1,\n",
      "  QUANTIZED_INT8 = 2,\n",
      "  QUANTIZED_UINT8 = 3,\n",
      "  HYBRID = 4\n",
      "};\n",
      "\n",
      "inline InferenceType GetInferenceTypeEnum(llvm::StringRef inference_type_str) {\n",
      "  if (inference_type_str == \"FLOAT\") {\n",
      "    return FLOAT;\n",
      "  } else if (inference_type_str == \"QUANTIZED_INT8\") {\n",
      "    return QUANTIZED_INT8;\n",
      "  } else if (inference_type_str == \"QUANTIZED_UINT8\") {\n",
      "    return QUANTIZED_UINT8;\n",
      "  } else if (inference_type_str == \"HYBRID\") {\n",
      "    return HYBRID;\n",
      "  } else {\n",
      "    return UNKNOWN;\n",
      "  }\n",
      "}\n",
      "\n",
      "inline std::string GetInferenceString(InferenceType inference_type) {\n",
      "  if (inference_type == FLOAT) {\n",
      "    return \"FLOAT\";\n",
      "  } else if (inference_type == QUANTIZED_INT8) {\n",
      "    return \"QUANTIZED_INT8\";\n",
      "  } else if (inference_type == QUANTIZED_UINT8) {\n",
      "    return \"QUANTIZED_UINT8\";\n",
      "  } else if (inference_type == HYBRID) {\n",
      "    return \"HYBRID\";\n",
      "  } else {\n",
      "    return \"UNKNOWN\";\n",
      "  }\n",
      "}\n",
      "\n",
      "// Returns canonical representation for hardware name (All uppercase).\n",
      "// TODO(b/177376459): Remove this in favor of the string defined by hardwares\n",
      "// MyHardware::kId.\n",
      "inline std::string GetCanonicalHardwareName(const std::string& hardware_name) {\n",
      "  std::string name = hardware_name;\n",
      "  std::transform(\n",
      "      name.begin(), name.end(), name.begin(),\n",
      "      [](unsigned char c) -> unsigned char { return std::toupper(c); });\n",
      "  return name;\n",
      "}\n",
      "\n",
      "// Get the target annotation form the op.\n",
      "inline llvm::Optional<std::string> GetTargetAnnotation(Operation* op) {\n",
      "  auto device = op->getAttrOfType<StringAttr>(kDevice);\n",
      "  if (device == nullptr || device.getValue().empty()) return llvm::None;\n",
      "\n",
      "  return GetCanonicalHardwareName(device.getValue().str());\n",
      "}\n",
      "\n",
      "// Get inference type attribute from the operation if available.\n",
      "inline llvm::Optional<InferenceType> GetInferenceTypeAnnotation(Operation* op) {\n",
      "  auto inference_type = op->getAttrOfType<StringAttr>(kInferenceType);\n",
      "  if (inference_type == nullptr) return llvm::None;\n",
      "\n",
      "  llvm::StringRef device_name_str = inference_type.getValue();\n",
      "  return GetInferenceTypeEnum(device_name_str);\n",
      "}\n",
      "\n",
      "// InferenceDeviceType is a combination of the hardware with inference type.\n",
      "struct InferenceDeviceType {\n",
      "  std::string hardware;\n",
      "  InferenceType inference_type;\n",
      "\n",
      "  bool operator==(const InferenceDeviceType& other) const {\n",
      "    return (hardware == other.hardware) &&\n",
      "           (inference_type == other.inference_type);\n",
      "  }\n",
      "\n",
      "  bool operator!=(const InferenceDeviceType& other) const {\n",
      "    return !(*this == other);\n",
      "  }\n",
      "\n",
      "  struct inference_device_type_hash {\n",
      "    size_t operator()(const InferenceDeviceType& p) const {\n",
      "      auto hash1 = std::hash<std::string>{}(p.hardware);\n",
      "      auto hash2 = std::hash<InferenceType>{}(p.inference_type);\n",
      "      return hash1 ^ hash2;\n",
      "    }\n",
      "  };\n",
      "};\n",
      "\n",
      "// Get InferenceDeviceType attribute from the operation if available.\n",
      "inline llvm::Optional<InferenceDeviceType> GetInferenceDeviceTypeForOp(\n",
      "    Operation* op) {\n",
      "  auto hardware = GetTargetAnnotation(op);\n",
      "  if (!hardware.hasValue()) return llvm::None;\n",
      "\n",
      "  auto inference_type = GetInferenceTypeAnnotation(op);\n",
      "  if (!inference_type.hasValue()) return llvm::None;\n",
      "\n",
      "  InferenceDeviceType inference_device_type;\n",
      "  inference_device_type.hardware = hardware.getValue();\n",
      "  inference_device_type.inference_type = inference_type.getValue();\n",
      "  return inference_device_type;\n",
      "}\n",
      "\n",
      "}  // namespace tac\n",
      "}  // namespace TFL\n",
      "}  // namespace mlir\n",
      "\n",
      "#endif  // TENSORFLOW_COMPILER_MLIR_LITE_EXPERIMENTAL_TAC_COMMON_TARGETS_H_\n",
      "\n",
      "\n",
      "Output 19: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the standard string library, which provides facilities to manipulate strings.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the standard vector library, which provides dynamic array capabilities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"llvm/ADT/ArrayRef.h\\\"\",\n",
      "        \"component_description\": \"Includes LLVM's ArrayRef header for a non-owning reference to an array of objects.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"llvm/ADT/None.h\\\"\",\n",
      "        \"component_description\": \"Includes LLVM's None header, which provides the NoneType and OptionalNone type definitions for representing 'no value'.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"llvm/ADT/Optional.h\\\"\",\n",
      "        \"component_description\": \"Includes LLVM's Optional header, which provides the Optional template class for values that may or may not be present.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"llvm/ADT/StringRef.h\\\"\",\n",
      "        \"component_description\": \"Includes LLVM's StringRef header, which provides a lightweight reference to an immutable string.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"mlir/IR/Operation.h\\\"  // from @llvm-project\\\"\",\n",
      "        \"component_description\": \"Includes MLIR's Operation header, which provides the base class for all operations in the MLIR Intermediate Representation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTANT_DEFINITION\",\n",
      "        \"component_name\": \"kDevice\",\n",
      "        \"component_code\": \"constexpr char kDevice[] = \\\"tac.device\\\";\",\n",
      "        \"component_description\": \"Defines a constant string that represents the attribute key for specifying device information in MLIR operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTANT_DEFINITION\",\n",
      "        \"component_name\": \"kInferenceType\",\n",
      "        \"component_code\": \"constexpr char kInferenceType[] = \\\"tac.inference_type\\\";\",\n",
      "        \"component_description\": \"Defines a constant string that represents the attribute key for specifying inference type information in MLIR operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetInferenceTypeEnum\",\n",
      "        \"component_code\": \"inline InferenceType GetInferenceTypeEnum(llvm::StringRef device_name_str) { ... }\",\n",
      "        \"component_description\": \"Converts a string representation of an inference type to the corresponding enum value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetCanonicalHardwareName\",\n",
      "        \"component_code\": \"inline std::string GetCanonicalHardwareName(const std::string& hardware_name) { ... }\",\n",
      "        \"component_description\": \"Converts a hardware name to its canonical uppercase form.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetTargetAnnotation\",\n",
      "        \"component_code\": \"inline llvm::Optional<std::string> GetTargetAnnotation(Operation* op) { ... }\",\n",
      "        \"component_description\": \"Retrieves the target hardware annotation from an MLIR operation, returning it in its canonical uppercase form.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetInferenceTypeAnnotation\",\n",
      "        \"component_code\": \"inline llvm::Optional<InferenceType> GetInferenceTypeAnnotation(Operation* op) { ... }\",\n",
      "        \"component_description\": \"Retrieves the inference type annotation from an MLIR operation as an enum value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"STRUCT_DEFINITION\",\n",
      "        \"component_name\": \"InferenceDeviceType\",\n",
      "        \"component_code\": \"struct InferenceDeviceType { ... };\",\n",
      "        \"component_description\": \"Defines a structure representing a combination of hardware and inference type, with comparison operators and a hash function.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"GetInferenceDeviceTypeForOp\",\n",
      "        \"component_code\": \"inline llvm::Optional<InferenceDeviceType> GetInferenceDeviceTypeForOp(Operation* op) { ... }\",\n",
      "        \"component_description\": \"Retrieves the inference device type annotation from an MLIR operation, combining hardware and inference type information.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file defines constants, functions, and a structure for handling target annotations in MLIR operations. It includes utilities for extracting and normalizing device and inference type information from operations, facilitating the management of different execution targets and their properties within an MLIR-based compiler or toolchain.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 20: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#ifndef TENSORFLOW_CORE_TFRT_EAGER_CORE_RUNTIME_PLACER_H_\n",
      "#define TENSORFLOW_CORE_TFRT_EAGER_CORE_RUNTIME_PLACER_H_\n",
      "\n",
      "#include \"tensorflow/core/platform/status.h\"\n",
      "#include \"tfrt/support/forward_decls.h\"  // from @tf_runtime\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "class ImmediateExecutionOperation;\n",
      "class EagerContext;\n",
      "class AttrBuilder;\n",
      "class NodeDef;\n",
      "}  // namespace tensorflow\n",
      "\n",
      "namespace tfrt {\n",
      "class OpHandler;\n",
      "class CoreRuntime;\n",
      "class Device;\n",
      "\n",
      "namespace tf {\n",
      "\n",
      "using ::tensorflow::EagerContext;\n",
      "using ::tensorflow::ImmediateExecutionOperation;\n",
      "using ::tensorflow::NodeDef;\n",
      "using ::tensorflow::Status;\n",
      "\n",
      "// A helper class to select op handler in op-by-op execution.\n",
      "class EagerOpHandlerSelector final {\n",
      " public:\n",
      "  EagerOpHandlerSelector(CoreRuntime* core_runtime, EagerContext* eager_context,\n",
      "                         OpHandler* fallback_op_handler,\n",
      "                         bool pin_small_ops_to_cpu);\n",
      "  ~EagerOpHandlerSelector();\n",
      "\n",
      "  // Selects the op handler to execute the op based on the arguments. This\n",
      "  // op handler selection is cheap. But it can be nullptr even it return OK\n",
      "  // status.\n",
      "  Status SelectFromArguments(const ImmediateExecutionOperation& op,\n",
      "                             OpHandler** op_handler);\n",
      "\n",
      "  // Selects the op handler to execute the op based on NodeDef. This op handler\n",
      "  // selection is expensive. It will never return nullptr unless there is an\n",
      "  // error. Please only invoke this method when the cheap version fails.\n",
      "  Status SelectFromNodeDef(const ImmediateExecutionOperation& op,\n",
      "                           const NodeDef* ndef, OpHandler** op_handler);\n",
      "\n",
      " private:\n",
      "  CoreRuntime* core_runtime_;\n",
      "  EagerContext* eager_context_;\n",
      "\n",
      "  const Device& cpu_device_;\n",
      "  OpHandler* cpu_op_handler_;\n",
      "  OpHandler* fallback_op_handler_;\n",
      "  bool pin_small_ops_to_cpu_;\n",
      "};\n",
      "\n",
      "}  // namespace tf\n",
      "}  // namespace tfrt\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_TFRT_EAGER_CORE_RUNTIME_PLACER_H_\n",
      "\n",
      "\n",
      "Output 20: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/platform/status.h\\\"\",\n",
      "        \"component_description\": \"Includes the TensorFlow Status header for error handling.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tfrt/support/forward_decls.h\\\"  // from @tf_runtime\",\n",
      "        \"component_description\": \"Forward declares symbols from the TFRT support library to avoid including full headers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tensorflow\",\n",
      "        \"component_code\": \"namespace tensorflow {\\n  class ImmediateExecutionOperation;\\n  class EagerContext;\\n  class AttrBuilder;\\n  class NodeDef;\\n}  // namespace tensorflow\",\n",
      "        \"component_description\": \"Declares classes used in the TensorFlow library within the 'tensorflow' namespace.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"NAMESPACE_DECLARATION\",\n",
      "        \"component_name\": \"tfrt::tf\",\n",
      "        \"component_code\": \"namespace tfrt {\\nclass OpHandler;\\nclass CoreRuntime;\\nclass Device;\\n\\nnamespace tf {\\nusing ::tensorflow::EagerContext;\\nusing ::tensorflow::ImmediateExecutionOperation;\\nusing ::tensorflow::NodeDef;\\nusing ::tensorflow::Status;\",\n",
      "        \"component_description\": \"Declares classes used in the TFRT library and imports TensorFlow classes into the 'tfrt::tf' namespace.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"EagerOpHandlerSelector\",\n",
      "        \"component_code\": \"class EagerOpHandlerSelector final {\\n public:\\n  EagerOpHandlerSelector(CoreRuntime* core_runtime, EagerContext* eager_context,\\n                         OpHandler* fallback_op_handler,\\n                         bool pin_small_ops_to_cpu);\\n  ~EagerOpHandlerSelector();\\n\\n  // Selects the op handler to execute the op based on the arguments. This\\n  // op handler selection is cheap. But it can be nullptr even it return OK\\n  // status.\\n  Status SelectFromArguments(const ImmediateExecutionOperation& op,\\n                             OpHandler** op_handler);\\n\\n  // Selects the op handler to execute the op based on NodeDef. This op handler\\n  // selection is expensive. It will never return nullptr unless there is an\\n  // error. Please only invoke this method when the cheap version fails.\\n  Status SelectFromNodeDef(const ImmediateExecutionOperation& op,\\n                           const NodeDef* ndef, OpHandler** op_handler);\\n\\n private:\\n  CoreRuntime* core_runtime_;\\n  EagerContext* eager_context_;\\n\\n  const Device& cpu_device_;\\n  OpHandler* cpu_op_handler_;\\n  OpHandler* fallback_op_handler_;\\n  bool pin_small_ops_to_cpu_;\\n};\",\n",
      "        \"component_description\": \"Defines a class to select operation handlers for TensorFlow operations in an eager execution context. It includes constructors, destructors, and methods for selecting op handlers based on different criteria.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This header file defines the 'EagerOpHandlerSelector' class within the 'tfrt::tf' namespace, used to select operation handlers for TensorFlow operations in an eager execution context. It includes necessary headers, declares classes from the TensorFlow and TFRT libraries, and provides a detailed description of the 'EagerOpHandlerSelector' class's functionality and its methods.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 21: \n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_CORE_COMMON_RUNTIME_REQUEST_COST_ACCESSOR_H_\n",
      "#define TENSORFLOW_CORE_COMMON_RUNTIME_REQUEST_COST_ACCESSOR_H_\n",
      "\n",
      "#include <string>\n",
      "\n",
      "#include \"absl/container/flat_hash_map.h\"\n",
      "#include \"absl/time/time.h\"\n",
      "#include \"tensorflow/core/common_runtime/request_cost.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "// An interface for accessing the RequestCost associated with the current rpc\n",
      "// request.\n",
      "class RequestCostAccessor {\n",
      " public:\n",
      "  virtual ~RequestCostAccessor() {}\n",
      "  virtual RequestCost* GetRequestCost() const = 0;\n",
      "};\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // TENSORFLOW_CORE_COMMON_RUNTIME_REQUEST_COST_ACCESSOR_H_\n",
      "\n",
      "\n",
      "Output 21: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <string>\",\n",
      "        \"component_description\": \"Includes the standard C++ string library for handling string operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/container/flat_hash_map.h\\\"\",\n",
      "        \"component_description\": \"Includes the Abseil flat hash map header, which provides a fast hash map implementation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"absl/time/time.h\\\"\",\n",
      "        \"component_description\": \"Includes the Abseil time library for handling time-related operations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/core/common_runtime/request_cost.h\\\"\",\n",
      "        \"component_description\": \"Includes a TensorFlow header file that likely defines the RequestCost class or struct, which is used to track and manage request costs.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"RequestCostAccessor\",\n",
      "        \"component_code\": \"class RequestCostAccessor {\\n public:\\n  virtual ~RequestCostAccessor() {}\\n  virtual RequestCost* GetRequestCost() const = 0;\\n};\",\n",
      "        \"component_description\": \"Defines an abstract class named RequestCostAccessor. This class provides an interface for accessing the RequestCost associated with the current RPC (Remote Procedure Call) request. It includes a pure virtual destructor and a pure virtual method GetRequestCost(), which must be implemented by any derived classes to return a pointer to a RequestCost object.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_description\": \"This C++ header file defines an interface for accessing the cost associated with an RPC request in TensorFlow, a popular machine learning framework. The file includes necessary headers from the standard library and Abseil, as well as a custom TensorFlow header that presumably contains the definition of the RequestCost class or struct. The primary component is the RequestCostAccessor class, which serves as an abstract base class for any classes that need to provide access to request cost information. It declares a pure virtual method GetRequestCost() that must be implemented by derived classes to return a pointer to a RequestCost object.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 22: \n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_UNARY_ELEMENTWISE_TESTER_H_\n",
      "#define TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_UNARY_ELEMENTWISE_TESTER_H_\n",
      "\n",
      "#include <cstdint>\n",
      "#include <vector>\n",
      "\n",
      "#include <gtest/gtest.h>\n",
      "#include \"tensorflow/lite/c/common.h\"\n",
      "#include \"tensorflow/lite/interpreter.h\"\n",
      "#include \"tensorflow/lite/schema/schema_generated.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace xnnpack {\n",
      "\n",
      "class QuantizedUnaryElementwiseTester {\n",
      " public:\n",
      "  QuantizedUnaryElementwiseTester() = default;\n",
      "  QuantizedUnaryElementwiseTester(const QuantizedUnaryElementwiseTester&) =\n",
      "      delete;\n",
      "  QuantizedUnaryElementwiseTester& operator=(\n",
      "      const QuantizedUnaryElementwiseTester&) = delete;\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& Shape(\n",
      "      std::initializer_list<int32_t> shape) {\n",
      "    for (auto it = shape.begin(); it != shape.end(); ++it) {\n",
      "      EXPECT_GT(*it, 0);\n",
      "    }\n",
      "    shape_ = std::vector<int32_t>(shape.begin(), shape.end());\n",
      "    size_ = QuantizedUnaryElementwiseTester::ComputeSize(shape_);\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  const std::vector<int32_t>& Shape() const { return shape_; }\n",
      "\n",
      "  int32_t Size() const { return size_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& InputZeroPoint(\n",
      "      int32_t input_zero_point) {\n",
      "    input_zero_point_ = input_zero_point;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline int32_t InputZeroPoint() const { return input_zero_point_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& OutputZeroPoint(\n",
      "      int32_t output_zero_point) {\n",
      "    output_zero_point_ = output_zero_point;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline int32_t OutputZeroPoint() const { return output_zero_point_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& InputScale(float input_scale) {\n",
      "    input_scale_ = input_scale;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline float InputScale() const { return input_scale_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& OutputScale(float output_scale) {\n",
      "    output_scale_ = output_scale;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline float OutputScale() const { return output_scale_; }\n",
      "\n",
      "  inline QuantizedUnaryElementwiseTester& Unsigned(bool is_unsigned) {\n",
      "    unsigned_ = is_unsigned;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline bool Unsigned() const { return unsigned_; }\n",
      "\n",
      "  template <class T>\n",
      "  void Test(Interpreter* delegate_interpreter,\n",
      "            Interpreter* default_interpreter) const;\n",
      "\n",
      "  void Test(tflite::BuiltinOperator unary_op, TfLiteDelegate* delegate) const;\n",
      "\n",
      " private:\n",
      "  std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator unary_op) const;\n",
      "\n",
      "  static int32_t ComputeSize(const std::vector<int32_t>& shape);\n",
      "\n",
      "  std::vector<int32_t> shape_;\n",
      "  int32_t size_;\n",
      "  int32_t input_zero_point_ = 0;\n",
      "  int32_t output_zero_point_ = 0;\n",
      "  float input_scale_ = 1.0f;\n",
      "  float output_scale_ = 1.0f;\n",
      "  bool unsigned_ = false;\n",
      "};\n",
      "\n",
      "}  // namespace xnnpack\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_UNARY_ELEMENTWISE_TESTER_H_\n",
      "\n",
      "\n",
      "Output 22: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <cstdint>\",\n",
      "        \"component_description\": \"Includes the cstdint header for fixed-width integer types.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the vector header for using the std::vector container.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <gtest/gtest.h>\",\n",
      "        \"component_description\": \"Includes the gtest header for Google Test framework functionalities.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/c/common.h\\\"\",\n",
      "        \"component_description\": \"Includes the TensorFlow Lite C API common header file for general definitions and functions.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/interpreter.h\\\"\",\n",
      "        \"component_description\": \"Includes the TensorFlow Lite Interpreter header file for running models.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/schema/schema_generated.h\\\"\",\n",
      "        \"component_description\": \"Includes the generated schema header file for TensorFlow Lite flatbuffer schemas.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"QuantizedUnaryElementwiseTester\",\n",
      "        \"component_code\": \"class QuantizedUnaryElementwiseTester { ... };\",\n",
      "        \"component_description\": \"Defines a class named QuantizedUnaryElementwiseTester used for testing quantized unary element-wise operations in TensorFlow Lite. It includes methods to set and get various parameters such as shape, zero points, scales, and unsigned status. It also contains private helper functions to create the model and compute the size based on the provided shape.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CONSTRUCTOR\",\n",
      "        \"component_name\": \"QuantizedUnaryElementwiseTester\",\n",
      "        \"component_code\": \"QuantizedUnaryElementwiseTester() = default;\",\n",
      "        \"component_description\": \"Default constructor for the QuantizedUnaryElementwiseTester class, initializing all members to their default values.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Shape\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& Shape(std::initializer_list<int32_t> shape) { ... }\",\n",
      "        \"component_description\": \"Sets the shape of the input tensor. It validates that each dimension is greater than 0 and then updates the shape_ member variable and computes the size using ComputeSize().\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Shape\",\n",
      "        \"component_code\": \"const std::vector<int32_t>& Shape() const { return shape_; }\",\n",
      "        \"component_description\": \"Returns a constant reference to the shape_ member variable, representing the dimensions of the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Size\",\n",
      "        \"component_code\": \"int32_t Size() const { return size_; }\",\n",
      "        \"component_description\": \"Returns the total number of elements in the input tensor, calculated based on its shape.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputZeroPoint\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& InputZeroPoint(int32_t input_zero_point) { ... }\",\n",
      "        \"component_description\": \"Sets the zero point for the input tensor, which is used in quantization to map floating-point values to integer representations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputZeroPoint\",\n",
      "        \"component_code\": \"inline int32_t InputZeroPoint() const { return input_zero_point_; }\",\n",
      "        \"component_description\": \"Returns the zero point for the input tensor, used in quantization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"OutputZeroPoint\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& OutputZeroPoint(int32_t output_zero_point) { ... }\",\n",
      "        \"component_description\": \"Sets the zero point for the output tensor, which is used in quantization to map floating-point values to integer representations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"OutputZeroPoint\",\n",
      "        \"component_code\": \"inline int32_t OutputZeroPoint() const { return output_zero_point_; }\",\n",
      "        \"component_description\": \"Returns the zero point for the output tensor, used in quantization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputScale\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& InputScale(float input_scale) { ... }\",\n",
      "        \"component_description\": \"Sets the scale for the input tensor, which is used in quantization to map floating-point values to integer representations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputScale\",\n",
      "        \"component_code\": \"inline float InputScale() const { return input_scale_; }\",\n",
      "        \"component_description\": \"Returns the scale for the input tensor, used in quantization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"OutputScale\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& OutputScale(float output_scale) { ... }\",\n",
      "        \"component_description\": \"Sets the scale for the output tensor, which is used in quantization to map floating-point values to integer representations.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"OutputScale\",\n",
      "        \"component_code\": \"inline float OutputScale() const { return output_scale_; }\",\n",
      "        \"component_description\": \"Returns the scale for the output tensor, used in quantization.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Unsigned\",\n",
      "        \"component_code\": \"inline QuantizedUnaryElementwiseTester& Unsigned(bool unsigned_) { ... }\",\n",
      "        \"component_description\": \"Sets whether the input and output tensors are interpreted as unsigned integers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Unsigned\",\n",
      "        \"component_code\": \"inline bool Unsigned() const { return unsigned_; }\",\n",
      "        \"component_description\": \"Returns a boolean indicating whether the input and output tensors are treated as unsigned integers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"TEMPLATE_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Test\",\n",
      "        \"component_code\": \"template <class T> void Test(Interpreter* delegate_interpreter, Interpreter* default_interpreter) const;\",\n",
      "        \"component_description\": \"Template method for testing the quantized unary element-wise operation using both a delegate interpreter and a default interpreter.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Test\",\n",
      "        \"component_code\": \"void Test(tflite::BuiltinOperator unary_op, TfLiteDelegate* delegate) const;\",\n",
      "        \"component_description\": \"Method for testing the quantized unary element-wise operation using a specific TensorFlow Lite BuiltinOperator and delegate.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"CreateTfLiteModel\",\n",
      "        \"component_code\": \"std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator unary_op) const;\",\n",
      "        \"component_description\": \"Private method that creates a TensorFlow Lite model flatbuffer for the specified quantized unary element-wise operation.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_STATIC_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"ComputeSize\",\n",
      "        \"component_code\": \"static int32_t ComputeSize(const std::vector<int32_t>& shape);\",\n",
      "        \"component_description\": \"Static private method that computes the total number of elements in a tensor based on its dimensions.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"shape_\",\n",
      "        \"component_code\": \"std::vector<int32_t> shape_;\",\n",
      "        \"component_description\": \"Private member variable storing the dimensions of the input tensor as a vector of integers.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"size_\",\n",
      "        \"component_code\": \"int32_t size_;\",\n",
      "        \"component_description\": \"Private member variable storing the total number of elements in the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"input_zero_point_\",\n",
      "        \"component_code\": \"int32_t input_zero_point_ = 0;\",\n",
      "        \"component_description\": \"Private member variable storing the zero point for the input tensor, defaulting to 0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"output_zero_point_\",\n",
      "        \"component_code\": \"int32_t output_zero_point_ = 0;\",\n",
      "        \"component_description\": \"Private member variable storing the zero point for the output tensor, defaulting to 0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"input_scale_\",\n",
      "        \"component_code\": \"float input_scale_ = 1.0f;\",\n",
      "        \"component_description\": \"Private member variable storing the scale for the input tensor, defaulting to 1.0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"output_scale_\",\n",
      "        \"component_code\": \"float output_scale_ = 1.0f;\",\n",
      "        \"component_description\": \"Private member variable storing the scale for the output tensor, defaulting to 1.0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"PRIVATE_MEMBER_VARIABLE\",\n",
      "        \"component_name\": \"unsigned_\",\n",
      "        \"component_code\": \"bool unsigned_ = false;\",\n",
      "        \"component_description\": \"Private member variable indicating whether the input and output tensors are treated as unsigned integers, defaulting to false.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "This detailed breakdown of the `QuantizedUnaryElementwiseOpTester` class should help you understand its structure, methods, and usage for testing quantized unary element-wise operations in TensorFlow Lite.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 23: \n",
      "<gh_stars>1000+\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_PAD_TESTER_H_\n",
      "#define TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_PAD_TESTER_H_\n",
      "\n",
      "#include <cstdint>\n",
      "#include <vector>\n",
      "\n",
      "#include <gtest/gtest.h>\n",
      "#include \"tensorflow/lite/c/common.h\"\n",
      "#include \"tensorflow/lite/interpreter.h\"\n",
      "#include \"tensorflow/lite/schema/schema_generated.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace xnnpack {\n",
      "\n",
      "class QuantizedPadTester {\n",
      " public:\n",
      "  QuantizedPadTester() = default;\n",
      "  QuantizedPadTester(const QuantizedPadTester&) = delete;\n",
      "  QuantizedPadTester& operator=(const QuantizedPadTester&) = delete;\n",
      "\n",
      "  inline QuantizedPadTester& InputShape(std::initializer_list<int32_t> shape) {\n",
      "    for (auto it = shape.begin(); it != shape.end(); ++it) {\n",
      "      EXPECT_GT(*it, 0);\n",
      "    }\n",
      "    input_shape_ = std::vector<int32_t>(shape.begin(), shape.end());\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline const std::vector<int32_t>& InputShape() const { return input_shape_; }\n",
      "\n",
      "  inline QuantizedPadTester& InputPrePaddings(\n",
      "      std::initializer_list<int32_t> paddings) {\n",
      "    for (auto it = paddings.begin(); it != paddings.end(); ++it) {\n",
      "      EXPECT_GE(*it, 0);\n",
      "    }\n",
      "    input_pre_paddings_ =\n",
      "        std::vector<int32_t>(paddings.begin(), paddings.end());\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline const std::vector<int32_t> InputPrePaddings() const {\n",
      "    return input_pre_paddings_;\n",
      "  }\n",
      "\n",
      "  inline QuantizedPadTester& InputPostPaddings(\n",
      "      std::initializer_list<int32_t> paddings) {\n",
      "    for (auto it = paddings.begin(); it != paddings.end(); ++it) {\n",
      "      EXPECT_GE(*it, 0);\n",
      "    }\n",
      "    input_post_paddings_ =\n",
      "        std::vector<int32_t>(paddings.begin(), paddings.end());\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline const std::vector<int32_t> InputPostPaddings() const {\n",
      "    return input_post_paddings_;\n",
      "  }\n",
      "\n",
      "  std::vector<int32_t> OutputShape() const;\n",
      "\n",
      "  inline QuantizedPadTester& ZeroPoint(int32_t zero_point) {\n",
      "    zero_point_ = zero_point;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline int32_t ZeroPoint() const { return zero_point_; }\n",
      "\n",
      "  inline QuantizedPadTester& Scale(float scale) {\n",
      "    scale_ = scale;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline float Scale() const { return scale_; }\n",
      "\n",
      "  inline QuantizedPadTester& Unsigned(bool is_unsigned) {\n",
      "    unsigned_ = is_unsigned;\n",
      "    return *this;\n",
      "  }\n",
      "\n",
      "  inline bool Unsigned() const { return unsigned_; }\n",
      "\n",
      "  template <class T>\n",
      "  void Test(Interpreter* delegate_interpreter,\n",
      "            Interpreter* default_interpreter) const;\n",
      "\n",
      "  void Test(TfLiteDelegate* delegate) const;\n",
      "\n",
      " private:\n",
      "  std::vector<char> CreateTfLiteModel() const;\n",
      "\n",
      "  static int32_t ComputeSize(const std::vector<int32_t>& shape);\n",
      "\n",
      "  std::vector<int32_t> input_shape_;\n",
      "  std::vector<int32_t> input_pre_paddings_;\n",
      "  std::vector<int32_t> input_post_paddings_;\n",
      "  int32_t zero_point_ = 7;\n",
      "  float scale_ = 0.8f;\n",
      "  bool unsigned_ = false;\n",
      "};\n",
      "\n",
      "}  // namespace xnnpack\n",
      "}  // namespace tflite\n",
      "\n",
      "#endif  // TENSORFLOW_LITE_DELEGATES_XNNPACK_QUANTIZED_PAD_TESTER_H_\n",
      "\n",
      "\n",
      "Output 23: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <cstdint>\",\n",
      "        \"component_description\": \"Includes the standard C integer types header, providing fixed-width integer types like int32_t.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <vector>\",\n",
      "        \"component_description\": \"Includes the C++ Standard Library vector header, which is used for dynamic arrays in this code.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include <gtest/gtest.h>\",\n",
      "        \"component_description\": \"Includes the Google Test framework headers, enabling the use of test assertions and fixtures.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/c/common.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow Lite C API common header file which contains declarations for data types and constants used in the TensorFlow Lite C API.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/interpreter.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow Lite interpreter header file, providing access to the classes for running models with TensorFlow Lite.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": NULL,\n",
      "        \"component_code\": \"#include \\\"tensorflow/lite/schema/schema_generated.h\\\"\",\n",
      "        \"component_description\": \"Includes TensorFlow Lite schema generated header file which contains definitions of the flatbuffer schema used by TensorFlow Lite models.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"QuantizedPadTester\",\n",
      "        \"component_code\": \"class QuantizedPadTester { ... };\",\n",
      "        \"component_description\": \"Defines a class named QuantizedPadTester which is used for testing the quantized padding operation in TensorFlow Lite using the XNNPACK delegate. It includes methods to set and get various properties related to the input tensor shape, paddings, zero point, scale, and data type (signed/unsigned).\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputShape\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& InputShape(std::initializer_list<int32_t> shape) { ... }\",\n",
      "        \"component_description\": \"Sets the input tensor shape and validates that each dimension is greater than zero. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputShape\",\n",
      "        \"component_code\": \"inline const std::vector<int32_t>& InputShape() const { return input_shape_; }\",\n",
      "        \"component_description\": \"Returns a constant reference to the vector containing the current input tensor shape.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputPrePaddings\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& InputPrePaddings(std::initializer_list<int32_t> paddings) { ... }\",\n",
      "        \"component_description\": \"Sets the pre-padding values for each dimension of the input tensor and validates that they are non-negative. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputPrePaddings\",\n",
      "        \"component_code\": \"inline const std::vector<int32_t>& InputPrePaddings() const { return input_pre_paddings_; }\",\n",
      "        \"component_description\": \"Returns a constant reference to the vector containing the current pre-padding values for each dimension of the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputPostPaddings\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& InputPostPaddings(std::initializer_list<int32_t> paddings) { ... }\",\n",
      "        \"component_description\": \"Sets the post-padding values for each dimension of the input tensor and validates that they are non-negative. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"InputPostPaddings\",\n",
      "        \"component_code\": \"inline const std::vector<int32_t>& InputPostPaddings() const { return input_post_paddings_; }\",\n",
      "        \"component_description\": \"Returns a constant reference to the vector containing the current post-padding values for each dimension of the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"ZeroPoint\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& ZeroPoint(int32_t zero_point) { zero_point_ = zero_point; return *this; }\",\n",
      "        \"component_description\": \"Sets the quantization zero point for the input tensor. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"ZeroPoint\",\n",
      "        \"component_code\": \"inline int32_t ZeroPoint() const { return zero_point_; }\",\n",
      "        \"component_description\": \"Returns the quantization zero point for the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Scale\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& Scale(float scale) { scale_ = scale; return *this; }\",\n",
      "        \"component_description\": \"Sets the quantization scale for the input tensor. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Scale\",\n",
      "        \"component_code\": \"inline float Scale() const { return scale_; }\",\n",
      "        \"component_description\": \"Returns the quantization scale for the input tensor.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Unsigned\",\n",
      "        \"component_code\": \"inline QuantizedPadTester& Unsigned(bool is_unsigned) { unsigned_ = is_unsigned; return *this; }\",\n",
      "        \"component_description\": \"Sets whether the data type of the input tensor is unsigned. Returns a reference to the current instance of QuantizedPadTester for method chaining.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Unsigned\",\n",
      "        \"component_code\": \"inline bool Unsigned() const { return unsigned_; }\",\n",
      "        \"component_description\": \"Returns whether the data type of the input tensor is unsigned.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"TEMPLATE_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Test\",\n",
      "        \"component_code\": \"template <class T> void Test(Interpreter* delegate_interpreter, Interpreter* default_interpreter) const;\",\n",
      "        \"component_description\": \"A template method that performs the actual test using two interpreters: one with a TensorFlow Lite delegate and another without. The method is templated to allow for flexibility in the type of data used for testing.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"Test\",\n",
      "        \"component_code\": \"void Test(TfLiteDelegate* delegate) const;\",\n",
      "        \"component_description\": \"Performs the actual test using a TensorFlow Lite delegate. This method creates two interpreters, one with and one without the delegate, and calls the templated Test method to compare their results.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"METHOD_DEFINITION\",\n",
      "        \"component_name\": \"CreateTfLiteModel\",\n",
      "        \"component_code\": \"std::vector<char> CreateTfLiteModel() const;\",\n",
      "        \"component_description\": \"Creates a TensorFlow Lite model with the specified input tensor shape, paddings, zero point, scale, and data type. The method returns a vector of characters containing the serialized flatbuffer model.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"STATIC_METHOD_DEFINITION\",\n",
      "        \"component_name\": \"ComputeSize\",\n",
      "        \"component_code\": \"static int32_t ComputeSize(const std::vector<int32_t>& shape);\",\n",
      "        \"component_description\": \"A static method that computes the total number of elements in a tensor with the specified shape. The method is used to allocate memory for input and output tensors during testing.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"input_pre_paddings_\",\n",
      "        \"component_code\": \"std::vector<int32_t> input_pre_paddings_;\",\n",
      "        \"component_description\": \"A vector containing the pre-padding values for each dimension of the input tensor. The padding values are set using the InputPrePaddings method.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"input_post_paddings_\",\n",
      "        \"component_code\": \"std::vector<int32_t> input_post_paddings_;\",\n",
      "        \"component_description\": \"A vector containing the post-padding values for each dimension of the input tensor. The padding values are set using the InputPostPaddings method.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"zero_point_\",\n",
      "        \"component_code\": \"int32_t zero_point_ = 0;\",\n",
      "        \"component_description\": \"The quantization zero point for the input tensor. The zero point is set using the ZeroPoint method and defaults to 0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"scale_\",\n",
      "        \"component_code\": \"float scale_ = 1.0f;\",\n",
      "        \"component_description\": \"The quantization scale for the input tensor. The scale is set using the Scale method and defaults to 1.0.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"MEMBER_VARIABLE_DEFINITION\",\n",
      "        \"component_name\": \"unsigned_\",\n",
      "        \"component_code\": \"bool unsigned_ = false;\",\n",
      "        \"component_description\": \"A boolean flag indicating whether the data type of the input tensor is unsigned. The flag is set using the Unsigned method and defaults to false.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "1. **Class Definition**: The `QuantizedPadTest` class encapsulates all necessary components to test the quantized pad operation in TensorFlow Lite.\n",
      "\n",
      "2. **Member Variables**:\n",
      "   - `input_pre_paddings_`: Stores the pre-padding values for each dimension of the input tensor.\n",
      "   - `input_post_paddings_`: Stores the post-padding values for each dimension of the input tensor.\n",
      "   - `zero_point_`: The quantization zero point, initialized to 0.\n",
      "   - `scale_`: The quantization scale, initialized to 1.0.\n",
      "   - `unsigned_`: A boolean indicating if the data type is unsigned, initialized to false.\n",
      "\n",
      "3. **Methods**:\n",
      "   - **Setters**: Methods like `ZeroPoint`, `Scale`, and `Unsigned` allow setting the respective member variables.\n",
      "   - **Getters**: Corresponding getter methods return the current values of these variables.\n",
      "   - **Test Methods**: \n",
      "     - A template method `Test(Interpreter* delegate_interpreter, Interpreter* default_interpreter)` to run tests with a delegate interpreter and a default interpreter.\n",
      "     - Another method `Test(TfLiteDelegate* delegate)` that sets up interpreters and calls the template test method.\n",
      "   - **Helper Methods**:\n",
      "     - `CreateTfLiteModel`: Creates and returns a serialized flatbuffer model for testing.\n",
      "     - `ComputeSize`: Computes the total number of elements in a tensor given its shape, used for memory allocation.\n",
      "\n",
      "4. **Template Method**: The use of a template method allows flexibility in handling different data types during testing.\n",
      "\n",
      "This setup ensures that all aspects of the quantized pad operation can be thoroughly tested under various configurations and scenarios.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Input 24: \n",
      "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#ifndef TENSORFLOW_STREAM_EXECUTOR_HOST_OR_DEVICE_SCALAR_H_\n",
      "#define TENSORFLOW_STREAM_EXECUTOR_HOST_OR_DEVICE_SCALAR_H_\n",
      "\n",
      "#include \"tensorflow/stream_executor/data_type.h\"\n",
      "#include \"tensorflow/stream_executor/device_memory.h\"\n",
      "#include \"tensorflow/stream_executor/platform/logging.h\"\n",
      "\n",
      "namespace stream_executor {\n",
      "\n",
      "// Allows to represent a value that is either a host scalar or a scalar stored\n",
      "// on the GPU device.\n",
      "// See also the specialization for ElemT=void below.\n",
      "template <typename ElemT>\n",
      "class HostOrDeviceScalar {\n",
      " public:\n",
      "  // Not marked as explicit because when using this constructor, we usually want\n",
      "  // to set this to a compile-time constant.\n",
      "  HostOrDeviceScalar(ElemT value) : value_(value), is_pointer_(false) {}\n",
      "  explicit HostOrDeviceScalar(const DeviceMemory<ElemT>& pointer)\n",
      "      : pointer_(pointer), is_pointer_(true) {\n",
      "    CHECK_EQ(1, pointer.ElementCount());\n",
      "  }\n",
      "\n",
      "  bool is_pointer() const { return is_pointer_; }\n",
      "  const DeviceMemory<ElemT>& pointer() const {\n",
      "    CHECK(is_pointer());\n",
      "    return pointer_;\n",
      "  }\n",
      "  const ElemT& value() const {\n",
      "    CHECK(!is_pointer());\n",
      "    return value_;\n",
      "  }\n",
      "\n",
      " private:\n",
      "  union {\n",
      "    ElemT value_;\n",
      "    DeviceMemory<ElemT> pointer_;\n",
      "  };\n",
      "  bool is_pointer_;\n",
      "};\n",
      "\n",
      "// Specialization for wrapping a dynamically-typed value (via type erasure).\n",
      "template <>\n",
      "class HostOrDeviceScalar<void> {\n",
      " public:\n",
      "  using DataType = dnn::DataType;\n",
      "\n",
      "  // Constructors not marked as explicit because when using this constructor, we\n",
      "  // usually want to set this to a compile-time constant.\n",
      "\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(float value)\n",
      "      : float_(value), is_pointer_(false), dtype_(DataType::kFloat) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(double value)\n",
      "      : double_(value), is_pointer_(false), dtype_(DataType::kDouble) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(Eigen::half value)\n",
      "      : half_(value), is_pointer_(false), dtype_(DataType::kHalf) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(int8 value)\n",
      "      : int8_(value), is_pointer_(false), dtype_(DataType::kInt8) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(int32 value)\n",
      "      : int32_(value), is_pointer_(false), dtype_(DataType::kInt32) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(std::complex<float> value)\n",
      "      : complex_float_(value),\n",
      "        is_pointer_(false),\n",
      "        dtype_(DataType::kComplexFloat) {}\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(std::complex<double> value)\n",
      "      : complex_double_(value),\n",
      "        is_pointer_(false),\n",
      "        dtype_(DataType::kComplexDouble) {}\n",
      "  template <typename T>\n",
      "  explicit HostOrDeviceScalar(const DeviceMemory<T>& pointer)\n",
      "      : pointer_(pointer),\n",
      "        is_pointer_(true),\n",
      "        dtype_(dnn::ToDataType<T>::value) {\n",
      "    CHECK_EQ(1, pointer.ElementCount());\n",
      "  }\n",
      "  // Construct from statically-typed version.\n",
      "  template <typename T, typename std::enable_if<!std::is_same<T, void>::value,\n",
      "                                                int>::type = 0>\n",
      "  // NOLINTNEXTLINE google-explicit-constructor\n",
      "  HostOrDeviceScalar(const HostOrDeviceScalar<T>& other) {\n",
      "    if (other.is_pointer()) {\n",
      "      *this = HostOrDeviceScalar(other.pointer());\n",
      "    } else {\n",
      "      *this = HostOrDeviceScalar(other.value());\n",
      "    }\n",
      "  }\n",
      "\n",
      "  bool is_pointer() const { return is_pointer_; }\n",
      "  template <typename T>\n",
      "  const DeviceMemory<T>& pointer() const {\n",
      "    CHECK(is_pointer());\n",
      "    CHECK(dtype_ == dnn::ToDataType<T>::value);\n",
      "    return pointer_;\n",
      "  }\n",
      "  template <typename T>\n",
      "  const T& value() const {\n",
      "    CHECK(!is_pointer());\n",
      "    CHECK(dtype_ == dnn::ToDataType<T>::value);\n",
      "    return value_impl<T>();\n",
      "  }\n",
      "  const DeviceMemoryBase& opaque_pointer() const {\n",
      "    CHECK(is_pointer());\n",
      "    return pointer_;\n",
      "  }\n",
      "  const void* opaque_value() const {\n",
      "    CHECK(!is_pointer());\n",
      "    switch (dtype_) {\n",
      "      case DataType::kFloat:\n",
      "        return &float_;\n",
      "      case DataType::kDouble:\n",
      "        return &double_;\n",
      "      case DataType::kHalf:\n",
      "        return &half_;\n",
      "      case DataType::kInt8:\n",
      "        return &int8_;\n",
      "      case DataType::kInt32:\n",
      "        return &int32_;\n",
      "      case DataType::kComplexFloat:\n",
      "        return &complex_float_;\n",
      "      case DataType::kComplexDouble:\n",
      "        return &complex_double_;\n",
      "      default:\n",
      "        return nullptr;\n",
      "    }\n",
      "  }\n",
      "  DataType data_type() const { return dtype_; }\n",
      "\n",
      " private:\n",
      "  template <typename T>\n",
      "  const T& value_impl() const;\n",
      "\n",
      "  union {\n",
      "    float float_;\n",
      "    double double_;\n",
      "    Eigen::half half_;\n",
      "    int8 int8_;\n",
      "    int32 int32_;\n",
      "    std::complex<float> complex_float_;\n",
      "    std::complex<double> complex_double_;\n",
      "    DeviceMemoryBase pointer_;\n",
      "  };\n",
      "  bool is_pointer_;\n",
      "  DataType dtype_;\n",
      "};\n",
      "\n",
      "template <>\n",
      "inline const float& HostOrDeviceScalar<void>::value_impl<float>() const {\n",
      "  return float_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const double& HostOrDeviceScalar<void>::value_impl<double>() const {\n",
      "  return double_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const Eigen::half& HostOrDeviceScalar<void>::value_impl<Eigen::half>()\n",
      "    const {\n",
      "  return half_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const int8& HostOrDeviceScalar<void>::value_impl<int8>() const {\n",
      "  return int8_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const int32& HostOrDeviceScalar<void>::value_impl<int32>() const {\n",
      "  return int32_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const std::complex<float>&\n",
      "HostOrDeviceScalar<void>::value_impl<std::complex<float>>() const {\n",
      "  return complex_float_;\n",
      "}\n",
      "\n",
      "template <>\n",
      "inline const std::complex<double>&\n",
      "HostOrDeviceScalar<void>::value_impl<std::complex<double>>() const {\n",
      "  return complex_double_;\n",
      "}\n",
      "\n",
      "}  // namespace stream_executor\n",
      "#endif  // TENSORFLOW_STREAM_EXECUTOR_HOST_OR_DEVICE_SCALAR_H_\n",
      "\n",
      "\n",
      "Output 24: \n",
      "{\n",
      "  \"output\": {\n",
      "    \"programming_language\": \"C\",\n",
      "    \"components\": [\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/stream_executor/data_type.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for data type definitions used in the Stream Executor library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/stream_executor/device_memory.h\\\"\",\n",
      "        \"component_description\": \"Includes the header file for device memory management used in the Stream Executor library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"HEADER_INCLUDE\",\n",
      "        \"component_name\": null,\n",
      "        \"component_code\": \"#include \\\"tensorflow/stream_executor/platform/logging.h\\\"\",\n",
      "        \"component_description\": \"Includes the logging platform header file, likely used for error checking and logging within the Stream Executor library.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"HostOrDeviceScalar\",\n",
      "        \"component_code\": \"template <typename T> class HostOrDeviceScalar { ... };\",\n",
      "        \"component_description\": \"A template class representing a scalar value that can be either on the host or device, with constructors for both types of values and methods to retrieve them.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_CONSTRUCTOR\",\n",
      "        \"component_name\": \"HostOrDeviceScalar<T>\",\n",
      "        \"component_code\": \"explicit HostOrDeviceScalar(const DeviceMemory<T>& pointer)\",\n",
      "        \"component_description\": \"Constructor that initializes the scalar with a device memory pointer, ensuring it points to a single element.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"is_pointer\",\n",
      "        \"component_code\": \"bool is_pointer() const\",\n",
      "        \"component_description\": \"Returns true if the scalar value is stored as a device memory pointer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"pointer\",\n",
      "        \"component_code\": \"const DeviceMemory<T>& pointer() const\",\n",
      "        \"component_description\": \"Returns the device memory pointer to the scalar value, with a check that it is indeed a pointer and points to a single element.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"value\",\n",
      "        \"component_code\": \"const T& value() const\",\n",
      "        \"component_description\": \"Returns the host-stored scalar value, with a check that it is not stored as a device memory pointer.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_DEFINITION\",\n",
      "        \"component_name\": \"HostOrDeviceScalar<void>\",\n",
      "        \"component_code\": \"template <> class HostOrDeviceScalar<void> { ... };\",\n",
      "        \"component_description\": \"A specialized template class for void type, allowing storage and retrieval of different scalar types with their corresponding data types.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_CONSTRUCTOR\",\n",
      "        \"component_name\": \"HostOrDeviceScalar<void>\",\n",
      "        \"component_code\": \"explicit HostOrDeviceScalar(const DeviceMemory<T>& pointer)\",\n",
      "        \"component_description\": \"Constructor that initializes the void-type scalar with a device memory pointer of any type, along with setting its data type.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_CONSTRUCTOR\",\n",
      "        \"component_name\": \"HostOrDeviceScalar<void>\",\n",
      "        \"component_code\": \"template <typename T> HostOrDeviceScalar(const HostOrDeviceScalar<T>& other)\",\n",
      "        \"component_description\": \"Constructor that initializes the void-type scalar from a statically-typed version of HostOrDeviceScalar.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"data_type\",\n",
      "        \"component_code\": \"DataType data_type() const\",\n",
      "        \"component_description\": \"Returns the data type of the stored scalar value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"CLASS_METHOD\",\n",
      "        \"component_name\": \"value_impl\",\n",
      "        \"component_code\": \"template <typename T> const T& value_impl() const\",\n",
      "        \"component_description\": \"Template method to retrieve the host-stored scalar value of a specific type, used internally.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<float>\",\n",
      "        \"component_code\": \"template <> inline const float& HostOrDeviceScalar<void>::value_impl<float>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored float value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<double>\",\n",
      "        \"component_code\": \"template <> inline const double& HostOrDeviceScalar<void>::value_impl<double>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored double value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<Eigen::half>\",\n",
      "        \"component_code\": \"template <> inline const Eigen::half& HostOrDeviceScalar<void>::value_impl<Eigen::half>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored Eigen half value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<int8>\",\n",
      "        \"component_code\": \"template <> inline const int8& HostOrDeviceScalar<void>::value_impl<int8>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored int8 value.\"\n",
      "      },\n",
      "      {\n",
      "        \"component_type\": \"FUNCTION_DEFINITION\",\n",
      "        \"component_name\": \"value_impl<int32>\",\n",
      "        \"component_code\": \"template <> inline const int32& HostOrDeviceScalar<void>::value_impl<int32>() const { ... }\",\n",
      "        \"component_description\": \"Specialized function to retrieve the host-stored int32 value.\"\n",
      "      }\n",
      "    ]\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the output from the pickle file\n",
    "with open(\"data.pickle\", \"rb\") as file:\n",
    "    loaded_output = pickle.load(file)\n",
    "print(\"Loaded output from pickle file:\")\n",
    "for i, (k, v) in enumerate(loaded_output.items()):\n",
    "    print(f\"Input {i}: \\n{v['input']}\\n\")\n",
    "    print(f\"Output {i}: \\n{v['output']}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping index 2 in output.\n",
      "Keeping index 3 in output.\n",
      "Keeping index 5 in output.\n",
      "Keeping index 6 in output.\n",
      "Keeping index 7 in output.\n",
      "Keeping index 8 in output.\n",
      "Keeping index 9 in output.\n",
      "Keeping index 10 in output.\n",
      "Keeping index 11 in output.\n",
      "Keeping index 12 in output.\n",
      "Keeping index 14 in output.\n",
      "Keeping index 15 in output.\n",
      "Keeping index 16 in output.\n",
      "Keeping index 17 in output.\n",
      "Keeping index 18 in output.\n",
      "Keeping index 19 in output.\n",
      "Keeping index 21 in output.\n"
     ]
    }
   ],
   "source": [
    "# After manual verification\n",
    "\n",
    "final_data = {}\n",
    "remove_ind_num = [0, 1, 4, 13, 20, 22, 23, 24]\n",
    "for i, (k, v) in enumerate(loaded_output.items()):\n",
    "    if i not in remove_ind_num:\n",
    "        print(f\"Keeping index {i} in output.\")\n",
    "        final_data[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['c_2', 'c_3', 'c_5', 'c_6', 'c_7', 'c_8', 'c_9', 'c_10', 'c_11', 'c_12', 'c_14', 'c_15', 'c_16', 'c_17', 'c_18', 'c_19', 'c_21'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to final_c_data.json\n"
     ]
    }
   ],
   "source": [
    "# Save the output to a JSON file\n",
    "import json\n",
    "output_json_path = \"final_c_data.json\"\n",
    "try:\n",
    "    save_json(final_data, output_json_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final data to JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malcodeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
