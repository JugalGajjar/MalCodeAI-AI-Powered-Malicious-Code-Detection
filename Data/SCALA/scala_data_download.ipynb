{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "# First, download the dataset with caching\n",
    "def download_starcoderdata_scala(save_directory, split=\"train\", download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS):\n",
    "    try:\n",
    "        ds = load_dataset(\n",
    "            \"bigcode/starcoderdata\",\n",
    "            data_dir=\"scala\",\n",
    "            split=split,\n",
    "            cache_dir=save_directory,\n",
    "            download_mode=download_mode,\n",
    "        )\n",
    "\n",
    "        # Save the dataset properly for later reloading\n",
    "        output_path = f\"{save_directory}/scala_{split}_dataset\"\n",
    "        ds.save_to_disk(output_path)\n",
    "\n",
    "        print(f\"Dataset 'bigcode/starcoderdata' (SCALA, {split}) successfully downloaded and saved to '{output_path}'.\")\n",
    "        return ds\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset 'bigcode/starcoderdata' (SCALA, {split}): {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5716d5d2c5a4eef9fb6983de9109ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a0be020a4347d7aba5884533d3e03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00005.parquet:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25260fef1424da7a088fab7552a1b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00005.parquet:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448ea2f15c944a7e936a9d44f2633586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00005.parquet:   0%|          | 0.00/352M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0948051e89d34c83aa34d4b321e588ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00005.parquet:   0%|          | 0.00/356M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667525172c86437590f2bf9e0641f709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00004-of-00005.parquet:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e035c217fe6443d68166e36bcf0519a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830a29e5e851449c9147f4420e729e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/1355788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'bigcode/starcoderdata' (SCALA, train) successfully downloaded and saved to './/scala_train_dataset'.\n"
     ]
    }
   ],
   "source": [
    "ds = download_starcoderdata_scala(save_directory=\"./\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Later, load the dataset from the saved location\n",
    "dataset = load_from_disk(\"./scala_train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id', 'content'],\n",
       "    num_rows: 1355788\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_stars_repo_path': 'src/main/scala/com/wavesplatform/Explorer.scala', 'max_stars_repo_name': 'raisex/g4', 'max_stars_count': 0, 'id': '0', 'content': 'package com.wavesplatform\\n\\nimport java.io.File\\nimport java.nio.ByteBuffer\\nimport java.util\\n\\nimport com.typesafe.config.ConfigFactory\\nimport com.wavesplatform.database.{Keys, LevelDBWriter}\\nimport com.wavesplatform.db.openDB\\nimport com.wavesplatform.settings.{WavesSettings, loadConfig}\\nimport com.wavesplatform.state.{ByteStr, EitherExt2}\\nimport com.wavesplatform.utils.{Base58, Base64}\\nimport org.slf4j.bridge.SLF4JBridgeHandler\\nimport scorex.account.{Address, AddressScheme}\\nimport scorex.utils.ScorexLogging\\n\\nimport scala.collection.JavaConverters._\\nimport scala.util.Try\\n\\nobject Explorer extends ScorexLogging {\\n  case class Stats(entryCount: Long, totalKeySize: Long, totalValueSize: Long)\\n\\n  private val keys = Array(\\n    \"version\",\\n    \"height\",\\n    \"score\",\\n    \"block-at-height\",\\n    \"height-of\",\\n    \"waves-balance-history\",\\n    \"waves-balance\",\\n    \"assets-for-address\",\\n    \"asset-balance-history\",\\n    \"asset-balance\",\\n    \"asset-info-history\",\\n    \"asset-info\",\\n    \"lease-balance-history\",\\n    \"lease-balance\",\\n    \"lease-status-history\",\\n    \"lease-status\",\\n    \"filled-volume-and-fee-history\",\\n    \"filled-volume-and-fee\",\\n    \"transaction-info\",\\n    \"address-transaction-history\",\\n    \"address-transaction-ids-at-height\",\\n    \"changed-addresses\",\\n    \"transaction-ids-at-height\",\\n    \"address-id-of-alias\",\\n    \"last-address-id\",\\n    \"address-to-id\",\\n    \"id-of-address\",\\n    \"address-script-history\",\\n    \"address-script\",\\n    \"approved-features\",\\n    \"activated-features\",\\n    \"data-key-chunk-count\",\\n    \"data-key-chunk\",\\n    \"data-history\",\\n    \"data\",\\n    \"sponsorship-history\",\\n    \"sponsorship\",\\n    \"addresses-for-waves-seq-nr\",\\n    \"addresses-for-waves\",\\n    \"addresses-for-asset-seq-nr\",\\n    \"addresses-for-asset\",\\n    \"address-transaction-ids-seq-nr\",\\n    \"address-transaction-ids\"\\n  )\\n\\n  def main(args: Array[String]): Unit = {\\n    SLF4JBridgeHandler.removeHandlersForRootLogger()\\n    SLF4JBridgeHandler.install()\\n\\n    val configFilename = Try(args(0)).toOption.getOrElse(\"TN-testnet.conf\")\\n\\n    val settings = WavesSettings.fromConfig(loadConfig(ConfigFactory.parseFile(new File(configFilename))))\\n    AddressScheme.current = new AddressScheme {\\n      override val chainId: Byte = settings.blockchainSettings.addressSchemeCharacter.toByte\\n    }\\n\\n    log.info(s\"Data directory: ${settings.dataDirectory}\")\\n\\n    val db     = openDB(settings.dataDirectory)\\n    val reader = new LevelDBWriter(db, settings.blockchainSettings.functionalitySettings)\\n\\n    val blockchainHeight = reader.height\\n    log.info(s\"Blockchain height is $blockchainHeight\")\\n    try {\\n\\n      val flag = args(1).toUpperCase\\n\\n      flag match {\\n        case \"B\" =>\\n          val maybeBlockId = Base58.decode(args(2)).toOption.map(ByteStr.apply)\\n          if (maybeBlockId.isDefined) {\\n            val kBlockHeight     = Keys.heightOf(maybeBlockId.get)\\n            val blockHeightBytes = db.get(kBlockHeight.keyBytes)\\n            val maybeBlockHeight = kBlockHeight.parse(blockHeightBytes)\\n            maybeBlockHeight.foreach { h =>\\n              val kBlock     = Keys.blockBytes(h)\\n              val blockBytes = db.get(kBlock.keyBytes)\\n              log.info(s\"BlockId=${maybeBlockId.get} at h=$h: ${Base64.encode(blockBytes)}\")\\n            }\\n          } else log.error(\"No block ID was provided\")\\n\\n        case \"O\" =>\\n          val orderId = Base58.decode(args(2)).toOption.map(ByteStr.apply)\\n          if (orderId.isDefined) {\\n            val kVolumeAndFee = Keys.filledVolumeAndFee(orderId.get)(blockchainHeight)\\n            val bytes1        = db.get(kVolumeAndFee.keyBytes)\\n            val v             = kVolumeAndFee.parse(bytes1)\\n            log.info(s\"OrderId = ${Base58.encode(orderId.get.arr)}: Volume = ${v.volume}, Fee = ${v.fee}\")\\n\\n            val kVolumeAndFeeHistory = Keys.filledVolumeAndFeeHistory(orderId.get)\\n            val bytes2               = db.get(kVolumeAndFeeHistory.keyBytes)\\n            val value2               = kVolumeAndFeeHistory.parse(bytes2)\\n            val value2Str            = value2.mkString(\"[\", \", \", \"]\")\\n            log.info(s\"OrderId = ${Base58.encode(orderId.get.arr)}: History = $value2Str\")\\n            value2.foreach { h =>\\n              val k = Keys.filledVolumeAndFee(orderId.get)(h)\\n              val v = k.parse(db.get(k.keyBytes))\\n              log.info(s\"\\\\t h = $h: Volume = ${v.volume}, Fee = ${v.fee}\")\\n            }\\n          } else log.error(\"No order ID was provided\")\\n\\n        case \"A\" =>\\n          val address   = Address.fromString(args(2)).explicitGet()\\n          val aid       = Keys.addressId(address)\\n          val addressId = aid.parse(db.get(aid.keyBytes)).get\\n          log.info(s\"Address id = $addressId\")\\n\\n          val kwbh = Keys.wavesBalanceHistory(addressId)\\n          val wbh  = kwbh.parse(db.get(kwbh.keyBytes))\\n\\n          val balances = wbh.map { h =>\\n            val k = Keys.wavesBalance(addressId)(h)\\n            h -> k.parse(db.get(k.keyBytes))\\n          }\\n          balances.foreach(b => log.info(s\"h = ${b._1}: balance = ${b._2}\"))\\n\\n        case \"AC\" =>\\n          val lastAddressId = Keys.lastAddressId.parse(db.get(Keys.lastAddressId.keyBytes))\\n          log.info(s\"Last address id: $lastAddressId\")\\n\\n        case \"AD\" =>\\n          val result        = new util.HashMap[Address, java.lang.Integer]()\\n          val lastAddressId = Keys.lastAddressId.parse(db.get(Keys.lastAddressId.keyBytes))\\n          for (id <- BigInt(1) to lastAddressId.getOrElse(BigInt(0))) {\\n            val k       = Keys.idToAddress(id)\\n            val address = k.parse(db.get(k.keyBytes))\\n            result.compute(address,\\n                           (_, prev) =>\\n                             prev match {\\n                               case null    => 1\\n                               case notNull => 1 + notNull\\n                           })\\n          }\\n\\n          for ((k, v) <- result.asScala if v > 1) {\\n            log.info(s\"$k,$v\")\\n          }\\n\\n        case \"AA\" =>\\n          val secondaryId = args(3)\\n\\n          val address   = Address.fromString(args(2)).explicitGet()\\n          val asset     = ByteStr.decodeBase58(secondaryId).get\\n          val ai        = Keys.addressId(address)\\n          val addressId = ai.parse(db.get(ai.keyBytes)).get\\n          log.info(s\"Address ID = $addressId\")\\n\\n          val kabh = Keys.assetBalanceHistory(addressId, asset)\\n          val abh  = kabh.parse(db.get(kabh.keyBytes))\\n\\n          val balances = abh.map { h =>\\n            val k = Keys.assetBalance(addressId, asset)(h)\\n            h -> k.parse(db.get(k.keyBytes))\\n          }\\n          balances.foreach(b => log.info(s\"h = ${b._1}: balance = ${b._2}\"))\\n\\n        case \"S\" =>\\n          log.info(\"Collecting DB stats\")\\n          val iterator = db.iterator()\\n          val result   = new util.HashMap[Short, Stats]\\n          iterator.seekToFirst()\\n          while (iterator.hasNext) {\\n            val entry     = iterator.next()\\n            val keyPrefix = ByteBuffer.wrap(entry.getKey).getShort\\n            result.compute(\\n              keyPrefix,\\n              (_, maybePrev) =>\\n                maybePrev match {\\n                  case null => Stats(1, entry.getKey.length, entry.getValue.length)\\n                  case prev => Stats(prev.entryCount + 1, prev.totalKeySize + entry.getKey.length, prev.totalValueSize + entry.getValue.length)\\n              }\\n            )\\n          }\\n          iterator.close()\\n\\n          log.info(\"key-space,entry-count,total-key-size,total-value-size\")\\n          for ((prefix, stats) <- result.asScala) {\\n            log.info(s\"${keys(prefix)},${stats.entryCount},${stats.totalKeySize},${stats.totalValueSize}\")\\n          }\\n      }\\n    } finally db.close()\\n  }\\n}\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "package com.wavesplatform\n",
      "\n",
      "import java.io.File\n",
      "import java.nio.ByteBuffer\n",
      "import java.util\n",
      "\n",
      "import com.typesafe.config.ConfigFactory\n",
      "import com.wavesplatform.database.{Keys, LevelDBWriter}\n",
      "import com.wavesplatform.db.openDB\n",
      "import com.wavesplatform.settings.{WavesSettings, loadConfig}\n",
      "import com.wavesplatform.state.{ByteStr, EitherExt2}\n",
      "import com.wavesplatform.utils.{Base58, Base64}\n",
      "import org.slf4j.bridge.SLF4JBridgeHandler\n",
      "import scorex.account.{Address, AddressScheme}\n",
      "import scorex.utils.ScorexLogging\n",
      "\n",
      "import scala.collection.JavaConverters._\n",
      "import scala.util.Try\n",
      "\n",
      "object Explorer extends ScorexLogging {\n",
      "  case class Stats(entryCount: Long, totalKeySize: Long, totalValueSize: Long)\n",
      "\n",
      "  private val keys = Array(\n",
      "    \"version\",\n",
      "    \"height\",\n",
      "    \"score\",\n",
      "    \"block-at-height\",\n",
      "    \"height-of\",\n",
      "    \"waves-balance-history\",\n",
      "    \"waves-balance\",\n",
      "    \"assets-for-address\",\n",
      "    \"asset-balance-history\",\n",
      "    \"asset-balance\",\n",
      "    \"asset-info-history\",\n",
      "    \"asset-info\",\n",
      "    \"lease-balance-history\",\n",
      "    \"lease-balance\",\n",
      "    \"lease-status-history\",\n",
      "    \"lease-status\",\n",
      "    \"filled-volume-and-fee-history\",\n",
      "    \"filled-volume-and-fee\",\n",
      "    \"transaction-info\",\n",
      "    \"address-transaction-history\",\n",
      "    \"address-transaction-ids-at-height\",\n",
      "    \"changed-addresses\",\n",
      "    \"transaction-ids-at-height\",\n",
      "    \"address-id-of-alias\",\n",
      "    \"last-address-id\",\n",
      "    \"address-to-id\",\n",
      "    \"id-of-address\",\n",
      "    \"address-script-history\",\n",
      "    \"address-script\",\n",
      "    \"approved-features\",\n",
      "    \"activated-features\",\n",
      "    \"data-key-chunk-count\",\n",
      "    \"data-key-chunk\",\n",
      "    \"data-history\",\n",
      "    \"data\",\n",
      "    \"sponsorship-history\",\n",
      "    \"sponsorship\",\n",
      "    \"addresses-for-waves-seq-nr\",\n",
      "    \"addresses-for-waves\",\n",
      "    \"addresses-for-asset-seq-nr\",\n",
      "    \"addresses-for-asset\",\n",
      "    \"address-transaction-ids-seq-nr\",\n",
      "    \"address-transaction-ids\"\n",
      "  )\n",
      "\n",
      "  def main(args: Array[String]): Unit = {\n",
      "    SLF4JBridgeHandler.removeHandlersForRootLogger()\n",
      "    SLF4JBridgeHandler.install()\n",
      "\n",
      "    val configFilename = Try(args(0)).toOption.getOrElse(\"TN-testnet.conf\")\n",
      "\n",
      "    val settings = WavesSettings.fromConfig(loadConfig(ConfigFactory.parseFile(new File(configFilename))))\n",
      "    AddressScheme.current = new AddressScheme {\n",
      "      override val chainId: Byte = settings.blockchainSettings.addressSchemeCharacter.toByte\n",
      "    }\n",
      "\n",
      "    log.info(s\"Data directory: ${settings.dataDirectory}\")\n",
      "\n",
      "    val db     = openDB(settings.dataDirectory)\n",
      "    val reader = new LevelDBWriter(db, settings.blockchainSettings.functionalitySettings)\n",
      "\n",
      "    val blockchainHeight = reader.height\n",
      "    log.info(s\"Blockchain height is $blockchainHeight\")\n",
      "    try {\n",
      "\n",
      "      val flag = args(1).toUpperCase\n",
      "\n",
      "      flag match {\n",
      "        case \"B\" =>\n",
      "          val maybeBlockId = Base58.decode(args(2)).toOption.map(ByteStr.apply)\n",
      "          if (maybeBlockId.isDefined) {\n",
      "            val kBlockHeight     = Keys.heightOf(maybeBlockId.get)\n",
      "            val blockHeightBytes = db.get(kBlockHeight.keyBytes)\n",
      "            val maybeBlockHeight = kBlockHeight.parse(blockHeightBytes)\n",
      "            maybeBlockHeight.foreach { h =>\n",
      "              val kBlock     = Keys.blockBytes(h)\n",
      "              val blockBytes = db.get(kBlock.keyBytes)\n",
      "              log.info(s\"BlockId=${maybeBlockId.get} at h=$h: ${Base64.encode(blockBytes)}\")\n",
      "            }\n",
      "          } else log.error(\"No block ID was provided\")\n",
      "\n",
      "        case \"O\" =>\n",
      "          val orderId = Base58.decode(args(2)).toOption.map(ByteStr.apply)\n",
      "          if (orderId.isDefined) {\n",
      "            val kVolumeAndFee = Keys.filledVolumeAndFee(orderId.get)(blockchainHeight)\n",
      "            val bytes1        = db.get(kVolumeAndFee.keyBytes)\n",
      "            val v             = kVolumeAndFee.parse(bytes1)\n",
      "            log.info(s\"OrderId = ${Base58.encode(orderId.get.arr)}: Volume = ${v.volume}, Fee = ${v.fee}\")\n",
      "\n",
      "            val kVolumeAndFeeHistory = Keys.filledVolumeAndFeeHistory(orderId.get)\n",
      "            val bytes2               = db.get(kVolumeAndFeeHistory.keyBytes)\n",
      "            val value2               = kVolumeAndFeeHistory.parse(bytes2)\n",
      "            val value2Str            = value2.mkString(\"[\", \", \", \"]\")\n",
      "            log.info(s\"OrderId = ${Base58.encode(orderId.get.arr)}: History = $value2Str\")\n",
      "            value2.foreach { h =>\n",
      "              val k = Keys.filledVolumeAndFee(orderId.get)(h)\n",
      "              val v = k.parse(db.get(k.keyBytes))\n",
      "              log.info(s\"\\t h = $h: Volume = ${v.volume}, Fee = ${v.fee}\")\n",
      "            }\n",
      "          } else log.error(\"No order ID was provided\")\n",
      "\n",
      "        case \"A\" =>\n",
      "          val address   = Address.fromString(args(2)).explicitGet()\n",
      "          val aid       = Keys.addressId(address)\n",
      "          val addressId = aid.parse(db.get(aid.keyBytes)).get\n",
      "          log.info(s\"Address id = $addressId\")\n",
      "\n",
      "          val kwbh = Keys.wavesBalanceHistory(addressId)\n",
      "          val wbh  = kwbh.parse(db.get(kwbh.keyBytes))\n",
      "\n",
      "          val balances = wbh.map { h =>\n",
      "            val k = Keys.wavesBalance(addressId)(h)\n",
      "            h -> k.parse(db.get(k.keyBytes))\n",
      "          }\n",
      "          balances.foreach(b => log.info(s\"h = ${b._1}: balance = ${b._2}\"))\n",
      "\n",
      "        case \"AC\" =>\n",
      "          val lastAddressId = Keys.lastAddressId.parse(db.get(Keys.lastAddressId.keyBytes))\n",
      "          log.info(s\"Last address id: $lastAddressId\")\n",
      "\n",
      "        case \"AD\" =>\n",
      "          val result        = new util.HashMap[Address, java.lang.Integer]()\n",
      "          val lastAddressId = Keys.lastAddressId.parse(db.get(Keys.lastAddressId.keyBytes))\n",
      "          for (id <- BigInt(1) to lastAddressId.getOrElse(BigInt(0))) {\n",
      "            val k       = Keys.idToAddress(id)\n",
      "            val address = k.parse(db.get(k.keyBytes))\n",
      "            result.compute(address,\n",
      "                           (_, prev) =>\n",
      "                             prev match {\n",
      "                               case null    => 1\n",
      "                               case notNull => 1 + notNull\n",
      "                           })\n",
      "          }\n",
      "\n",
      "          for ((k, v) <- result.asScala if v > 1) {\n",
      "            log.info(s\"$k,$v\")\n",
      "          }\n",
      "\n",
      "        case \"AA\" =>\n",
      "          val secondaryId = args(3)\n",
      "\n",
      "          val address   = Address.fromString(args(2)).explicitGet()\n",
      "          val asset     = ByteStr.decodeBase58(secondaryId).get\n",
      "          val ai        = Keys.addressId(address)\n",
      "          val addressId = ai.parse(db.get(ai.keyBytes)).get\n",
      "          log.info(s\"Address ID = $addressId\")\n",
      "\n",
      "          val kabh = Keys.assetBalanceHistory(addressId, asset)\n",
      "          val abh  = kabh.parse(db.get(kabh.keyBytes))\n",
      "\n",
      "          val balances = abh.map { h =>\n",
      "            val k = Keys.assetBalance(addressId, asset)(h)\n",
      "            h -> k.parse(db.get(k.keyBytes))\n",
      "          }\n",
      "          balances.foreach(b => log.info(s\"h = ${b._1}: balance = ${b._2}\"))\n",
      "\n",
      "        case \"S\" =>\n",
      "          log.info(\"Collecting DB stats\")\n",
      "          val iterator = db.iterator()\n",
      "          val result   = new util.HashMap[Short, Stats]\n",
      "          iterator.seekToFirst()\n",
      "          while (iterator.hasNext) {\n",
      "            val entry     = iterator.next()\n",
      "            val keyPrefix = ByteBuffer.wrap(entry.getKey).getShort\n",
      "            result.compute(\n",
      "              keyPrefix,\n",
      "              (_, maybePrev) =>\n",
      "                maybePrev match {\n",
      "                  case null => Stats(1, entry.getKey.length, entry.getValue.length)\n",
      "                  case prev => Stats(prev.entryCount + 1, prev.totalKeySize + entry.getKey.length, prev.totalValueSize + entry.getValue.length)\n",
      "              }\n",
      "            )\n",
      "          }\n",
      "          iterator.close()\n",
      "\n",
      "          log.info(\"key-space,entry-count,total-key-size,total-value-size\")\n",
      "          for ((prefix, stats) <- result.asScala) {\n",
      "            log.info(s\"${keys(prefix)},${stats.entryCount},${stats.totalKeySize},${stats.totalValueSize}\")\n",
      "          }\n",
      "      }\n",
      "    } finally db.close()\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 1:\n",
      "package helpers\n",
      "\n",
      "import org.specs2.mutable._\n",
      "import com.ruimo.recoeng.RecoEngApi\n",
      "import com.ruimo.recoeng.json.{JsonResponseHeader, OnSalesJsonResponse, SalesItem, TransactionMode, TransactionSalesMode, SortOrder, JsonRequestPaging, Desc, Asc, ScoredItem}\n",
      "import com.ruimo.recoeng.json.RecommendByItemJsonResponse\n",
      "import play.api.libs.json.{JsSuccess, JsResult}\n",
      "import models.LoginSession\n",
      "import org.mockito.Mockito.mock\n",
      "import models.PersistedTransaction\n",
      "import models.TransactionLogItem\n",
      "import models.TransactionLogCoupon\n",
      "import models.TransactionLogHeader\n",
      "import models.TransactionType\n",
      "import models.Address\n",
      "import models.ItemName\n",
      "import helpers.Helper._\n",
      "import com.ruimo.scoins.Scoping._\n",
      "\n",
      "class RecommendEngineSpec extends Specification {\n",
      "  \"Recommend engine\" should {\n",
      "    \"Can send transaction\" in {\n",
      "      val api: RecoEngApi = new RecoEngApi {\n",
      "        def onSales(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          transactionMode: TransactionMode,\n",
      "          transactionTime: Long,\n",
      "          userCode: String,\n",
      "          salesItems: Seq[SalesItem]\n",
      "        ): JsResult[OnSalesJsonResponse] = {\n",
      "          transactionMode === TransactionSalesMode\n",
      "          transactionTime === 23456L\n",
      "          userCode === \"12345\"\n",
      "          salesItems.size === 3\n",
      "          val set = salesItems.toSet\n",
      "          set.contains(SalesItem(\"555\", \"8192\", 3)) must beTrue\n",
      "          set.contains(SalesItem(\"555\", \"8193\", 5)) must beTrue\n",
      "          set.contains(SalesItem(\"666\", \"8194\", 1)) must beTrue\n",
      "\n",
      "          JsSuccess(\n",
      "            OnSalesJsonResponse(\n",
      "              JsonResponseHeader(sequenceNumber = \"1234\", statusCode = \"OK\", message = \"msg\")\n",
      "            )\n",
      "          )\n",
      "        }\n",
      "\n",
      "        def recommendByItem(\n",
      "          requestTime: Long = System.currentTimeMillis,\n",
      "          sequenceNumber: Long,\n",
      "          salesItems: Seq[SalesItem],\n",
      "          sort: SortOrder = Desc(\"score\"),\n",
      "          paging: JsonRequestPaging\n",
      "        ): JsResult[RecommendByItemJsonResponse] = null\n",
      "      }\n",
      "\n",
      "      val login = mock(classOf[LoginSession])\n",
      "      val tran: PersistedTransaction = PersistedTransaction(\n",
      "        header = TransactionLogHeader(\n",
      "          id = None,\n",
      "          userId = 12345L,\n",
      "          transactionTime = 23456L,\n",
      "          currencyId = 111L,\n",
      "          totalAmount = BigDecimal(1234),\n",
      "          taxAmount = BigDecimal(20),\n",
      "          transactionType = TransactionType.NORMAL\n",
      "        ),\n",
      "        tranSiteLog = Map(),\n",
      "        siteTable = Seq(),\n",
      "        shippingTable = Map(),\n",
      "        taxTable = Map(),\n",
      "        itemTable = Map(\n",
      "          555L -> Seq(\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 888L,\n",
      "              itemId = 8192L,\n",
      "              itemPriceHistoryId = 444L,\n",
      "              quantity = 3,\n",
      "              amount = BigDecimal(123),\n",
      "              costPrice = BigDecimal(555555),\n",
      "              taxId = 1232L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]])),\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 889L,\n",
      "              itemId = 8193L,\n",
      "              itemPriceHistoryId = 445L,\n",
      "              quantity = 5,\n",
      "              amount = BigDecimal(124),\n",
      "              costPrice = BigDecimal(555556),\n",
      "              taxId = 1234L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]]))\n",
      "          ),\n",
      "          666L -> Seq(\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 890L,\n",
      "              itemId = 8194L,\n",
      "              itemPriceHistoryId = 446L,\n",
      "              quantity = 1,\n",
      "              amount = BigDecimal(125),\n",
      "              costPrice = BigDecimal(555557),\n",
      "              taxId = 1235L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]]))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      val addr = mock(classOf[Address])\n",
      "      val resp: JsResult[OnSalesJsonResponse] = RecommendEngine.sendOnSales(login, tran, Some(addr), api)\n",
      "      doWith(resp.get.header) { header =>\n",
      "        header.sequenceNumber === \"1234\"\n",
      "        header.statusCode === \"OK\"\n",
      "        header.message === \"msg\"\n",
      "      }\n",
      "    }\n",
      "\n",
      "    \"Can get recommendByItem\" in {\n",
      "      val api: RecoEngApi = new RecoEngApi {\n",
      "        def onSales(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          transactionMode: TransactionMode,\n",
      "          transactionTime: Long,\n",
      "          userCode: String,\n",
      "          itemTable: Seq[SalesItem]\n",
      "        ): JsResult[OnSalesJsonResponse] = null\n",
      "\n",
      "        def recommendByItem(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          salesItems: Seq[SalesItem],\n",
      "          sort: SortOrder,\n",
      "          paging: JsonRequestPaging\n",
      "        ): JsResult[RecommendByItemJsonResponse] = {\n",
      "          salesItems.size === 1\n",
      "          doWith(salesItems(0)) { item =>\n",
      "            item.storeCode === \"11111\"\n",
      "            item.itemCode === \"22222\"\n",
      "          }\n",
      "          sort === Desc(\"score\")\n",
      "          paging.offset === 0\n",
      "          paging.limit === 5\n",
      "\n",
      "          JsSuccess(\n",
      "            RecommendByItemJsonResponse(\n",
      "              JsonResponseHeader(sequenceNumber = \"1234\", statusCode = \"OK\", message = \"msg\"),\n",
      "              salesItems = Seq(\n",
      "                ScoredItem(\n",
      "                  storeCode = \"1212\",\n",
      "                  itemCode = \"2323\",\n",
      "                  score = 12\n",
      "                ),\n",
      "                ScoredItem(\n",
      "                  storeCode = \"3434\",\n",
      "                  itemCode = \"4545\",\n",
      "                  score = 11\n",
      "                )\n",
      "              ),\n",
      "              \"desc(\\\"col\\\")\",\n",
      "              JsonRequestPaging(\n",
      "                offset = 2,\n",
      "                limit = 20\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        }\n",
      "      }\n",
      "\n",
      "      val result: JsResult[RecommendByItemJsonResponse] =\n",
      "        RecommendEngine.sendRecommendByItem(\n",
      "          Seq(SalesItem(storeCode = \"11111\", itemCode = \"22222\", quantity = 1)),\n",
      "          api\n",
      "        )\n",
      "      doWith(result.get) { resp =>\n",
      "        doWith(resp.header) { header =>\n",
      "          header.sequenceNumber === \"1234\"\n",
      "          header.statusCode === \"OK\"\n",
      "          header.message === \"msg\"\n",
      "        }\n",
      "        doWith(resp.salesItems) { salesItems =>\n",
      "          salesItems.size === 2\n",
      "          doWith(salesItems(0)) { item =>\n",
      "            item.storeCode === \"1212\"\n",
      "            item.itemCode === \"2323\"\n",
      "            item.score === 12f\n",
      "          }\n",
      "          doWith(salesItems(1)) { item =>\n",
      "            item.storeCode === \"3434\"\n",
      "            item.itemCode === \"4545\"\n",
      "            item.score === 11f\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 2:\n",
      "/***********************************************************************\n",
      " * Copyright (c) 2013-2017 Commonwealth Computer Research, Inc.\n",
      " * All rights reserved. This program and the accompanying materials\n",
      " * are made available under the terms of the Apache License, Version 2.0\n",
      " * which accompanies this distribution and is available at\n",
      " * http://www.opensource.org/licenses/apache2.0.php.\n",
      " ***********************************************************************/\n",
      "\n",
      "package org.locationtech.geomesa.utils.stats\n",
      "\n",
      "import java.util.Date\n",
      "\n",
      "import com.typesafe.scalalogging.LazyLogging\n",
      "import com.vividsolutions.jts.geom.{Coordinate, Geometry}\n",
      "import org.geotools.geometry.jts.JTSFactoryFinder\n",
      "import org.locationtech.geomesa.utils.clearspring.HyperLogLog\n",
      "import org.locationtech.geomesa.utils.stats.MinMax.CardinalityBits\n",
      "import org.opengis.feature.simple.SimpleFeature\n",
      "\n",
      "import scala.collection.immutable.ListMap\n",
      "\n",
      "/**\n",
      " * The MinMax stat merely returns the min/max of an attribute's values.\n",
      " * Works with dates, integers, longs, doubles, and floats.\n",
      " *\n",
      " * @param attribute attribute index for the attribute the histogram is being made for\n",
      " * @tparam T the type of the attribute the stat is targeting (needs to be comparable)\n",
      " */\n",
      "class MinMax[T] private [stats] (val attribute: Int,\n",
      "                                 private [stats] var minValue: T,\n",
      "                                 private [stats] var maxValue: T,\n",
      "                                 private [stats] val hpp: HyperLogLog)\n",
      "                                (implicit val defaults: MinMax.MinMaxDefaults[T])\n",
      "    extends Stat with LazyLogging with Serializable {\n",
      "\n",
      "  // use a secondary constructor instead of companion apply to allow mixin types (i.e. ImmutableStat)\n",
      "  def this(attribute: Int)(implicit defaults: MinMax.MinMaxDefaults[T]) =\n",
      "    this(attribute, defaults.max, defaults.min, HyperLogLog(CardinalityBits))(defaults)\n",
      "\n",
      "  override type S = MinMax[T]\n",
      "\n",
      "  def min: T = if (isEmpty) { maxValue } else { minValue }\n",
      "  def max: T = if (isEmpty) { minValue } else { maxValue }\n",
      "  def bounds: (T, T) = (min, max)\n",
      "  def cardinality: Long = hpp.cardinality()\n",
      "  def tuple: (T, T, Long) = (min, max, cardinality)\n",
      "\n",
      "  override def observe(sf: SimpleFeature): Unit = {\n",
      "    val value = sf.getAttribute(attribute).asInstanceOf[T]\n",
      "    if (value != null) {\n",
      "      try {\n",
      "        minValue = defaults.min(value, minValue)\n",
      "        maxValue = defaults.max(value, maxValue)\n",
      "        hpp.offer(value)\n",
      "      } catch {\n",
      "        case e: Exception => logger.warn(s\"Error observing value '$value': ${e.toString}\")\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  // note: can't unobserve min/max without storing a lot more data\n",
      "  override def unobserve(sf: SimpleFeature): Unit = {}\n",
      "\n",
      "  override def +(other: MinMax[T]): MinMax[T] = {\n",
      "    if (other.isEmpty) {\n",
      "      new MinMax[T](attribute, minValue, maxValue, hpp.merge())\n",
      "    } else if (this.isEmpty) {\n",
      "      new MinMax[T](attribute, other.minValue, other.maxValue, other.hpp.merge())\n",
      "    } else {\n",
      "      val plus = new MinMax[T](attribute, minValue, maxValue, hpp.merge())\n",
      "      plus += other\n",
      "      plus\n",
      "    }\n",
      "  }\n",
      "\n",
      "  override def +=(other: MinMax[T]): Unit = {\n",
      "    if (other.isEmpty) {\n",
      "      // no-op\n",
      "    } else if (isEmpty) {\n",
      "      minValue = other.minValue\n",
      "      maxValue = other.maxValue\n",
      "      hpp += other.hpp\n",
      "    } else {\n",
      "      minValue = defaults.min(minValue, other.minValue)\n",
      "      maxValue = defaults.max(maxValue, other.maxValue)\n",
      "      hpp += other.hpp\n",
      "    }\n",
      "  }\n",
      "\n",
      "  override def toJsonObject: Any =\n",
      "    if (isEmpty) {\n",
      "      ListMap(\"min\" -> null, \"max\" -> null, \"cardinality\" -> 0)\n",
      "    } else {\n",
      "      ListMap(\"min\" -> minValue, \"max\" -> maxValue, \"cardinality\" -> cardinality)\n",
      "    }\n",
      "\n",
      "  override def isEmpty: Boolean = minValue == defaults.max\n",
      "\n",
      "  override def clear(): Unit = {\n",
      "    minValue = defaults.max\n",
      "    maxValue = defaults.min\n",
      "    java.util.Arrays.fill(hpp.registerSet.rawBits, 0)\n",
      "  }\n",
      "\n",
      "  override def isEquivalent(other: Stat): Boolean = other match {\n",
      "    case that: MinMax[T] =>\n",
      "      attribute == that.attribute && minValue == that.minValue &&\n",
      "          maxValue == that.maxValue && cardinality == that.cardinality\n",
      "    case _ => false\n",
      "  }\n",
      "}\n",
      "\n",
      "object MinMax {\n",
      "\n",
      "  val CardinalityBits: Int = 10\n",
      "\n",
      "  trait MinMaxDefaults[T] {\n",
      "    def min: T\n",
      "    def max: T\n",
      "    def min(left: T, right: T): T\n",
      "    def max(left: T, right: T): T\n",
      "  }\n",
      "\n",
      "  object MinMaxDefaults {\n",
      "    def apply[T](binding: Class[_]): MinMaxDefaults[T] = {\n",
      "      if (binding == classOf[String]) {\n",
      "        MinMaxString.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (binding == classOf[Integer]) {\n",
      "        MinMaxInt.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (binding == classOf[java.lang.Long]) {\n",
      "        MinMaxLong.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (binding == classOf[java.lang.Float]) {\n",
      "        MinMaxFloat.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (binding == classOf[java.lang.Double]) {\n",
      "        MinMaxDouble.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (classOf[Date].isAssignableFrom(binding)) {\n",
      "        MinMaxDate.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (classOf[Geometry].isAssignableFrom(binding)) {\n",
      "        MinMaxGeometry.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else {\n",
      "        throw new IllegalArgumentException(s\"No implicit default available for type: $binding\")\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  abstract class ComparableMinMax[T <: Comparable[T]] extends MinMaxDefaults[T] with Serializable {\n",
      "    override def min(left: T, right: T): T = if (left.compareTo(right) > 0) right else left\n",
      "    override def max(left: T, right: T): T = if (left.compareTo(right) < 0) right else left\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxString extends ComparableMinMax[String] with Serializable {\n",
      "    override val min: String = \"\"\n",
      "    override val max: String = \"\\uFFFF\\uFFFF\\uFFFF\"\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxInt extends ComparableMinMax[Integer] with Serializable {\n",
      "    override val min: Integer = Integer.MIN_VALUE\n",
      "    override val max: Integer = Integer.MAX_VALUE\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxLong extends ComparableMinMax[java.lang.Long] with Serializable {\n",
      "    override val min: java.lang.Long = java.lang.Long.MIN_VALUE\n",
      "    override val max: java.lang.Long = java.lang.Long.MAX_VALUE\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxFloat extends ComparableMinMax[java.lang.Float] with Serializable {\n",
      "    override val min: java.lang.Float = 0f - java.lang.Float.MAX_VALUE\n",
      "    override val max: java.lang.Float = java.lang.Float.MAX_VALUE\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxDouble extends ComparableMinMax[java.lang.Double] with Serializable  {\n",
      "    override val min: java.lang.Double = 0d - java.lang.Double.MAX_VALUE\n",
      "    override val max: java.lang.Double = java.lang.Double.MAX_VALUE\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxDate extends ComparableMinMax[Date] with Serializable {\n",
      "    override val min: Date = new Date(java.lang.Long.MIN_VALUE)\n",
      "    override val max: Date = new Date(java.lang.Long.MAX_VALUE)\n",
      "  }\n",
      "\n",
      "  /**\n",
      "    * Geometry min/max tracks the bounding box of each geometry, not the geometries themselves.\n",
      "    */\n",
      "  implicit object MinMaxGeometry extends MinMaxDefaults[Geometry] with Serializable {\n",
      "\n",
      "    private val gf = JTSFactoryFinder.getGeometryFactory\n",
      "\n",
      "    override val min: Geometry = gf.createPoint(new Coordinate(-180.0, -90.0))\n",
      "    override val max: Geometry = gf.createPoint(new Coordinate(180.0, 90.0))\n",
      "\n",
      "    override def min(left: Geometry, right: Geometry): Geometry = {\n",
      "      val (lx, ly) = { val e = left.getEnvelopeInternal; (e.getMinX, e.getMinY) }\n",
      "      val (rx, ry) = { val e = right.getEnvelopeInternal; (e.getMinX, e.getMinY) }\n",
      "\n",
      "      val x = math.min(lx, rx)\n",
      "      val y = math.min(ly, ry)\n",
      "\n",
      "      if (x == lx && y == ly) {\n",
      "        left\n",
      "      } else if (x == rx && y == ry) {\n",
      "        right\n",
      "      } else {\n",
      "        gf.createPoint(new Coordinate(x, y))\n",
      "      }\n",
      "    }\n",
      "\n",
      "    override def max(left: Geometry, right: Geometry): Geometry = {\n",
      "      val (lx, ly) = { val e = left.getEnvelopeInternal; (e.getMaxX, e.getMaxY) }\n",
      "      val (rx, ry) = { val e = right.getEnvelopeInternal; (e.getMaxX, e.getMaxY) }\n",
      "\n",
      "      val x = math.max(lx, rx)\n",
      "      val y = math.max(ly, ry)\n",
      "\n",
      "      if (x == lx && y == ly) {\n",
      "        left\n",
      "      } else if (x == rx && y == ry) {\n",
      "        right\n",
      "      } else {\n",
      "        gf.createPoint(new Coordinate(x, y))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 3:\n",
      "package cqrs.queries\n",
      "\n",
      "import java.time.Instant\n",
      "import java.util.UUID\n",
      "\n",
      "import io.circe.{Decoder, Encoder}\n",
      "import io.circe.generic.semiauto.{deriveDecoder, deriveEncoder}\n",
      "import io.circe.java8.time._\n",
      "\n",
      "import scala.collection.immutable.SortedMap\n",
      "\n",
      "/**\n",
      "  * This is the model used for querying.\n",
      "  */\n",
      "// TODO Add useful stats\n",
      "case class Meter(id: UUID, label: String, timeSeries: SortedMap[Instant, BigDecimal])\n",
      "\n",
      "object Meter {\n",
      "\n",
      "  implicit def decodeSortedMap[A : Decoder : Ordering, B : Decoder]: Decoder[SortedMap[A, B]] =\n",
      "    Decoder[Seq[(A, B)]].map(entries => (SortedMap.newBuilder[A, B] ++= entries).result())\n",
      "\n",
      "  implicit def encodeSortedMap[A : Encoder, B : Encoder]: Encoder[SortedMap[A, B]] =\n",
      "    Encoder.encodeList[(A, B)].contramap[SortedMap[A, B]](_.to[List])\n",
      "\n",
      "  implicit val decoder: Decoder[Meter] = deriveDecoder\n",
      "  implicit val encoder: Encoder[Meter] = deriveEncoder\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 4:\n",
      "/*\n",
      "Copyright 2012 Twitter, Inc.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "*/\n",
      "\n",
      "package com.twitter.algebird\n",
      "\n",
      "class VectorSpaceProperties extends CheckProperties {\n",
      "  import com.twitter.algebird.BaseVectorSpaceProperties._\n",
      "\n",
      "  // TODO: we won't need this when we have an Equatable trait\n",
      "  def mapEqFn(a: Map[Int, Double], b: Map[Int, Double]) = {\n",
      "    (a.keySet ++ b.keySet).forall { key =>\n",
      "      (a.get(key), b.get(key)) match {\n",
      "        case (Some(aVal), Some(bVal)) => beCloseTo(aVal, bVal)\n",
      "        case (Some(aVal), None) => beCloseTo(aVal, 0.0)\n",
      "        case (None, Some(bVal)) => beCloseTo(bVal, 0.0)\n",
      "        case _ => true\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  property(\"map int double scaling\") {\n",
      "    vectorSpaceLaws[Double, ({ type x[a] = Map[Int, a] })#x](mapEqFn(_, _))\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 5:\n",
      "/*\n",
      " * Tencent is pleased to support the open source community by making Angel available.\n",
      " *\n",
      " * Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.\n",
      " *\n",
      " * Licensed under the BSD 3-Clause License (the \"License\"); you may not use this file except in\n",
      " * compliance with the License. You may obtain a copy of the License at\n",
      " *\n",
      " * https://opensource.org/licenses/BSD-3-Clause\n",
      " *\n",
      " * Unless required by applicable law or agreed to in writing, software distributed under the License\n",
      " * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n",
      " * or implied. See the License for the specific language governing permissions and limitations under\n",
      " * the License.\n",
      " */\n",
      "\n",
      "package org.apache.spark.ml.classification.ps\n",
      "\n",
      "import scala.collection.JavaConverters._\n",
      "import scala.language.existentials\n",
      "import scala.util.Random\n",
      "import scala.util.control.Breaks._\n",
      "\n",
      "import org.apache.spark.SparkException\n",
      "import org.apache.spark.ml.attribute.NominalAttribute\n",
      "import org.apache.spark.ml.classification.LogisticRegressionSuite.{generateLogisticInput, generateMultinomialLogisticInput}\n",
      "import org.apache.spark.ml.classification.ProbabilisticClassifierSuite\n",
      "import org.apache.spark.ml.feature.{Instance, LabeledPoint}\n",
      "import org.apache.spark.ml.linalg.{DenseMatrix, Matrices, SparseMatrix, SparseVector, Vector, Vectors}\n",
      "import org.apache.spark.ml.param.ParamsSuite\n",
      "import org.apache.spark.ml.util.TestingUtils._\n",
      "import org.apache.spark.ml.util.{DefaultReadWriteTest, MLTestingUtils}\n",
      "import org.apache.spark.mllib.util.MLlibTestSparkContext\n",
      "import org.apache.spark.sql.functions.{col, lit, rand}\n",
      "import org.apache.spark.sql.types.LongType\n",
      "import org.apache.spark.sql.{Dataset, Row, SparkSession}\n",
      "import org.scalatest.{FunSuite, _}\n",
      "\n",
      "@Ignore\n",
      "class LogisticRegressionSuite extends FunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n",
      "\n",
      "  import testImplicits._\n",
      "  private val seed = 42\n",
      "  @transient var smallBinaryDataset: Dataset[_] = _\n",
      "  @transient var smallMultinomialDataset: Dataset[_] = _\n",
      "  @transient var binaryDataset: Dataset[_] = _\n",
      "  @transient var multinomialDataset: Dataset[_] = _\n",
      "  private val eps: Double = 1e-5\n",
      "\n",
      "  override def beforeAll(): Unit = {\n",
      "    super.beforeAll()\n",
      "    val spark = SparkSession.builder().getOrCreate()\n",
      "    import spark.implicits._\n",
      "\n",
      "    smallBinaryDataset = generateLogisticInput(1.0, 1.0, nPoints = 100, seed = seed).toDF()\n",
      "\n",
      "    smallMultinomialDataset = {\n",
      "      val nPoints = 100\n",
      "      val coefficients = Array(\n",
      "        -0.57997, 0.912083, -0.371077,\n",
      "        -0.16624, -0.84355, -0.048509)\n",
      "\n",
      "      val xMean = Array(5.843, 3.057)\n",
      "      val xVariance = Array(0.6856, 0.1899)\n",
      "\n",
      "      val testData = generateMultinomialLogisticInput(\n",
      "        coefficients, xMean, xVariance, addIntercept = true, nPoints, seed)\n",
      "\n",
      "      val df = sc.parallelize(testData, 4).toDF()\n",
      "      df.cache()\n",
      "      df\n",
      "    }\n",
      "\n",
      "    binaryDataset = {\n",
      "      val nPoints = 500\n",
      "      val coefficients = Array(-0.57997, 0.912083, -0.371077, -0.819866, 2.688191)\n",
      "      val xMean = Array(5.843, 3.057, 3.758, 1.199)\n",
      "      val xVariance = Array(0.6856, 0.1899, 3.116, 0.581)\n",
      "\n",
      "      val testData =\n",
      "        generateMultinomialLogisticInput(coefficients, xMean, xVariance,\n",
      "          addIntercept = true, nPoints, seed)\n",
      "\n",
      "      sc.parallelize(testData, 4).toDF().withColumn(\"weight\", rand(seed))\n",
      "    }\n",
      "\n",
      "    multinomialDataset = {\n",
      "      val nPoints = 10000\n",
      "      val coefficients = Array(\n",
      "        -0.57997, 0.912083, -0.371077, -0.819866, 2.688191,\n",
      "        -0.16624, -0.84355, -0.048509, -0.301789, 4.170682)\n",
      "\n",
      "      val xMean = Array(5.843, 3.057, 3.758, 1.199)\n",
      "      val xVariance = Array(0.6856, 0.1899, 3.116, 0.581)\n",
      "\n",
      "      val testData = generateMultinomialLogisticInput(\n",
      "        coefficients, xMean, xVariance, addIntercept = true, nPoints, seed)\n",
      "\n",
      "      val df = sc.parallelize(testData, 4).toDF().withColumn(\"weight\", rand(seed))\n",
      "      df.cache()\n",
      "      df\n",
      "    }\n",
      "  }\n",
      "\n",
      "  /**\n",
      "   * Enable the ignored test to export the dataset into CSV format,\n",
      "   * so we can validate the training accuracy compared with R's glmnet package.\n",
      "   */\n",
      "  ignore(\"export test data into CSV format\") {\n",
      "    binaryDataset.rdd.map { case Row(label: Double, features: Vector, weight: Double) =>\n",
      "      label + \",\" + weight + \",\" + features.toArray.mkString(\",\")\n",
      "    }.repartition(1).saveAsTextFile(\"target/tmp/LogisticRegressionSuite/binaryDataset\")\n",
      "    multinomialDataset.rdd.map { case Row(label: Double, features: Vector, weight: Double) =>\n",
      "      label + \",\" + weight + \",\" + features.toArray.mkString(\",\")\n",
      "    }.repartition(1).saveAsTextFile(\"target/tmp/LogisticRegressionSuite/multinomialDataset\")\n",
      "  }\n",
      "\n",
      "  test(\"params\") {\n",
      "    ParamsSuite.checkParams(new LogisticRegression)\n",
      "    val model = new LogisticRegressionModel(\"logReg\", Vectors.dense(0.0), 0.0)\n",
      "    ParamsSuite.checkParams(model)\n",
      "  }\n",
      "\n",
      "  test(\"empty probabilityCol\") {\n",
      "    val lr = new LogisticRegression().setProbabilityCol(\"\")\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    assert(model.hasSummary)\n",
      "    // Validate that we re-insert a probability column for evaluation\n",
      "    val fieldNames = model.summary.predictions.schema.fieldNames\n",
      "    assert(smallBinaryDataset.schema.fieldNames.toSet.subsetOf(\n",
      "      fieldNames.toSet))\n",
      "    assert(fieldNames.exists(s => s.startsWith(\"probability_\")))\n",
      "  }\n",
      "\n",
      "  test(\"setThreshold, getThreshold\") {\n",
      "    val lr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    // default\n",
      "    assert(lr.getThreshold === 0.5, \"LogisticRegression.threshold should default to 0.5\")\n",
      "    withClue(\"LogisticRegression should not have thresholds set by default.\") {\n",
      "      intercept[java.util.NoSuchElementException] { // Note: The exception type may change in future\n",
      "        lr.getThresholds\n",
      "      }\n",
      "    }\n",
      "    // Set via threshold.\n",
      "    // Intuition: Large threshold or large thresholds(1) makes class 0 more likely.\n",
      "    lr.setThreshold(1.0)\n",
      "    assert(lr.getThresholds === Array(0.0, 1.0))\n",
      "    lr.setThreshold(0.0)\n",
      "    assert(lr.getThresholds === Array(1.0, 0.0))\n",
      "    lr.setThreshold(0.5)\n",
      "    assert(lr.getThresholds === Array(0.5, 0.5))\n",
      "    // Set via thresholds\n",
      "    val lr2 = new LogisticRegression().setFamily(\"binomial\")\n",
      "    lr2.setThresholds(Array(0.3, 0.7))\n",
      "    val expectedThreshold = 1.0 / (1.0 + 0.3 / 0.7)\n",
      "    assert(lr2.getThreshold ~== expectedThreshold relTol 1E-7)\n",
      "    // thresholds and threshold must be consistent\n",
      "    lr2.setThresholds(Array(0.1, 0.2, 0.3))\n",
      "    withClue(\"getThreshold should throw error if thresholds has length != 2.\") {\n",
      "      intercept[IllegalArgumentException] {\n",
      "        lr2.getThreshold\n",
      "      }\n",
      "    }\n",
      "    // thresholds and threshold must be consistent: values\n",
      "    withClue(\"fit with ParamMap should throw error if threshold, thresholds do not match.\") {\n",
      "      intercept[IllegalArgumentException] {\n",
      "        lr2.fit(smallBinaryDataset,\n",
      "          lr2.thresholds -> Array(0.3, 0.7), lr2.threshold -> (expectedThreshold / 2.0))\n",
      "      }\n",
      "    }\n",
      "    withClue(\"fit with ParamMap should throw error if threshold, thresholds do not match.\") {\n",
      "      intercept[IllegalArgumentException] {\n",
      "        val lr2model = lr2.fit(smallBinaryDataset,\n",
      "          lr2.thresholds -> Array(0.3, 0.7), lr2.threshold -> (expectedThreshold / 2.0))\n",
      "        lr2model.getThreshold\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  test(\"thresholds prediction\") {\n",
      "    val blr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    val binaryModel = blr.fit(smallBinaryDataset)\n",
      "\n",
      "    binaryModel.setThreshold(1.0)\n",
      "    val binaryZeroPredictions =\n",
      "      binaryModel.transform(smallBinaryDataset).select(\"prediction\").collect()\n",
      "    assert(binaryZeroPredictions.forall(_.getDouble(0) === 0.0))\n",
      "\n",
      "    binaryModel.setThreshold(0.0)\n",
      "    val binaryOnePredictions =\n",
      "      binaryModel.transform(smallBinaryDataset).select(\"prediction\").collect()\n",
      "    assert(binaryOnePredictions.forall(_.getDouble(0) === 1.0))\n",
      "\n",
      "\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    val model = mlr.fit(smallMultinomialDataset)\n",
      "    val basePredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "\n",
      "    // should predict all zeros\n",
      "    model.setThresholds(Array(1, 1000, 1000))\n",
      "    val zeroPredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(zeroPredictions.forall(_.getDouble(0) === 0.0))\n",
      "\n",
      "    // should predict all ones\n",
      "    model.setThresholds(Array(1000, 1, 1000))\n",
      "    val onePredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(onePredictions.forall(_.getDouble(0) === 1.0))\n",
      "\n",
      "    // should predict all twos\n",
      "    model.setThresholds(Array(1000, 1000, 1))\n",
      "    val twoPredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(twoPredictions.forall(_.getDouble(0) === 2.0))\n",
      "\n",
      "    // constant threshold scaling is the same as no thresholds\n",
      "    model.setThresholds(Array(1000, 1000, 1000))\n",
      "    val scaledPredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(scaledPredictions.zip(basePredictions).forall { case (scaled, base) =>\n",
      "      scaled.getDouble(0) === base.getDouble(0)\n",
      "    })\n",
      "\n",
      "    // force it to use the predict method\n",
      "    model.setRawPredictionCol(\"\").setProbabilityCol(\"\").setThresholds(Array(0, 1, 1))\n",
      "    val predictionsWithPredict =\n",
      "      model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(predictionsWithPredict.forall(_.getDouble(0) === 0.0))\n",
      "  }\n",
      "\n",
      "  test(\"logistic regression doesn't fit intercept when fitIntercept is off\") {\n",
      "    val lr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    lr.setFitIntercept(false)\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    assert(model.intercept === 0.0)\n",
      "\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    mlr.setFitIntercept(false)\n",
      "    val mlrModel = mlr.fit(smallMultinomialDataset)\n",
      "    assert(mlrModel.interceptVector === Vectors.sparse(3, Seq()))\n",
      "  }\n",
      "\n",
      "  test(\"logistic regression with setters\") {\n",
      "    // Set params, train, and check as many params as we can.\n",
      "    val lr = new LogisticRegression()\n",
      "      .setMaxIter(10)\n",
      "      .setRegParam(1.0)\n",
      "      .setThreshold(0.6)\n",
      "      .setProbabilityCol(\"myProbability\")\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    val parent = model.parent.asInstanceOf[LogisticRegression]\n",
      "    assert(parent.getMaxIter === 10)\n",
      "    assert(parent.getRegParam === 1.0)\n",
      "    assert(parent.getThreshold === 0.6)\n",
      "    assert(model.getThreshold === 0.6)\n",
      "\n",
      "    // Modify model params, and check that the params worked.\n",
      "    model.setThreshold(1.0)\n",
      "    val predAllZero = model.transform(smallBinaryDataset)\n",
      "      .select(\"prediction\", \"myProbability\")\n",
      "      .collect()\n",
      "      .map { case Row(pred: Double, prob: Vector) => pred }\n",
      "    assert(predAllZero.forall(_ === 0),\n",
      "      s\"With threshold=1.0, expected predictions to be all 0, but only\" +\n",
      "      s\" ${predAllZero.count(_ === 0)} of ${smallBinaryDataset.count()} were 0.\")\n",
      "    // Call transform with params, and check that the params worked.\n",
      "    val predNotAllZero =\n",
      "      model.transform(smallBinaryDataset, model.threshold -> 0.0,\n",
      "        model.probabilityCol -> \"myProb\")\n",
      "        .select(\"prediction\", \"myProb\")\n",
      "        .collect()\n",
      "        .map { case Row(pred: Double, prob: Vector) => pred }\n",
      "    assert(predNotAllZero.exists(_ !== 0.0))\n",
      "\n",
      "    // Call fit() with new params, and check as many params as we can.\n",
      "    lr.setThresholds(Array(0.6, 0.4))\n",
      "    val model2 = lr.fit(smallBinaryDataset, lr.maxIter -> 5, lr.regParam -> 0.1,\n",
      "      lr.probabilityCol -> \"theProb\")\n",
      "    val parent2 = model2.parent.asInstanceOf[LogisticRegression]\n",
      "    assert(parent2.getMaxIter === 5)\n",
      "    assert(parent2.getRegParam === 0.1)\n",
      "    assert(parent2.getThreshold === 0.4)\n",
      "    assert(model2.getThreshold === 0.4)\n",
      "    assert(model2.getProbabilityCol === \"theProb\")\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression: Predictor, Classifier methods\") {\n",
      "    val sqlContext = smallMultinomialDataset.sqlContext\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "\n",
      "    val model = mlr.fit(smallMultinomialDataset)\n",
      "    assert(model.numClasses === 3)\n",
      "    val numFeatures = smallMultinomialDataset.select(\"features\").first().getAs[Vector](0).size\n",
      "    assert(model.numFeatures === numFeatures)\n",
      "\n",
      "    val results = model.transform(smallMultinomialDataset)\n",
      "    // check that raw prediction is coefficients dot features + intercept\n",
      "    results.select(\"rawPrediction\", \"features\").collect().foreach {\n",
      "      case Row(raw: Vector, features: Vector) =>\n",
      "        assert(raw.size === 3)\n",
      "        val margins = Array.tabulate(3) { k =>\n",
      "          var margin = 0.0\n",
      "          features.foreachActive { (index, value) =>\n",
      "            margin += value * model.coefficientMatrix(k, index)\n",
      "          }\n",
      "          margin += model.interceptVector(k)\n",
      "          margin\n",
      "        }\n",
      "        assert(raw ~== Vectors.dense(margins) relTol eps)\n",
      "    }\n",
      "\n",
      "    // Compare rawPrediction with probability\n",
      "    results.select(\"rawPrediction\", \"probability\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector) =>\n",
      "        assert(raw.size === 3)\n",
      "        assert(prob.size === 3)\n",
      "        val max = raw.toArray.max\n",
      "        val subtract = if (max > 0) max else 0.0\n",
      "        val sum = raw.toArray.map(x => math.exp(x - subtract)).sum\n",
      "        val probFromRaw0 = math.exp(raw(0) - subtract) / sum\n",
      "        val probFromRaw1 = math.exp(raw(1) - subtract) / sum\n",
      "        assert(prob(0) ~== probFromRaw0 relTol eps)\n",
      "        assert(prob(1) ~== probFromRaw1 relTol eps)\n",
      "        assert(prob(2) ~== 1.0 - probFromRaw1 - probFromRaw0 relTol eps)\n",
      "    }\n",
      "\n",
      "    // Compare prediction with probability\n",
      "    results.select(\"prediction\", \"probability\").collect().foreach {\n",
      "      case Row(pred: Double, prob: Vector) =>\n",
      "        val predFromProb = prob.toArray.zipWithIndex.maxBy(_._1)._2\n",
      "        assert(pred == predFromProb)\n",
      "    }\n",
      "\n",
      "    // force it to use probability2prediction\n",
      "    model.setProbabilityCol(\"\")\n",
      "    val resultsUsingProb2Predict =\n",
      "      model.transform(smallMultinomialDataset).select(\"prediction\").as[Double].collect()\n",
      "    resultsUsingProb2Predict.zip(results.select(\"prediction\").as[Double].collect()).foreach {\n",
      "      case (pred1, pred2) => assert(pred1 === pred2)\n",
      "    }\n",
      "\n",
      "    // force it to use predict\n",
      "    model.setRawPredictionCol(\"\").setProbabilityCol(\"\")\n",
      "    val resultsUsingPredict =\n",
      "      model.transform(smallMultinomialDataset).select(\"prediction\").as[Double].collect()\n",
      "    resultsUsingPredict.zip(results.select(\"prediction\").as[Double].collect()).foreach {\n",
      "      case (pred1, pred2) => assert(pred1 === pred2)\n",
      "    }\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression: Predictor, Classifier methods\") {\n",
      "    val sqlContext = smallBinaryDataset.sqlContext\n",
      "    val lr = new LogisticRegression().setFamily(\"binomial\")\n",
      "\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    assert(model.numClasses === 2)\n",
      "    val numFeatures = smallBinaryDataset.select(\"features\").first().getAs[Vector](0).size\n",
      "    assert(model.numFeatures === numFeatures)\n",
      "\n",
      "    val results = model.transform(smallBinaryDataset)\n",
      "\n",
      "    // Compare rawPrediction with probability\n",
      "    results.select(\"rawPrediction\", \"probability\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector) =>\n",
      "        assert(raw.size === 2)\n",
      "        assert(prob.size === 2)\n",
      "        val probFromRaw1 = 1.0 / (1.0 + math.exp(-raw(1)))\n",
      "        assert(prob(1) ~== probFromRaw1 relTol eps)\n",
      "        assert(prob(0) ~== 1.0 - probFromRaw1 relTol eps)\n",
      "    }\n",
      "\n",
      "    // Compare prediction with probability\n",
      "    results.select(\"prediction\", \"probability\").collect().foreach {\n",
      "      case Row(pred: Double, prob: Vector) =>\n",
      "        val predFromProb = prob.toArray.zipWithIndex.maxBy(_._1)._2\n",
      "        assert(pred == predFromProb)\n",
      "    }\n",
      "\n",
      "    // force it to use probability2prediction\n",
      "    model.setProbabilityCol(\"\")\n",
      "    val resultsUsingProb2Predict =\n",
      "      model.transform(smallBinaryDataset).select(\"prediction\").as[Double].collect()\n",
      "    resultsUsingProb2Predict.zip(results.select(\"prediction\").as[Double].collect()).foreach {\n",
      "      case (pred1, pred2) => assert(pred1 === pred2)\n",
      "    }\n",
      "\n",
      "    // force it to use predict\n",
      "    model.setRawPredictionCol(\"\").setProbabilityCol(\"\")\n",
      "    val resultsUsingPredict =\n",
      "      model.transform(smallBinaryDataset).select(\"prediction\").as[Double].collect()\n",
      "    resultsUsingPredict.zip(results.select(\"prediction\").as[Double].collect()).foreach {\n",
      "      case (pred1, pred2) => assert(pred1 === pred2)\n",
      "    }\n",
      "  }\n",
      "\n",
      "  test(\"coefficients and intercept methods\") {\n",
      "    val mlr = new LogisticRegression().setMaxIter(1).setFamily(\"multinomial\")\n",
      "    val mlrModel = mlr.fit(smallMultinomialDataset)\n",
      "    val thrownCoef = intercept[SparkException] {\n",
      "      mlrModel.coefficients\n",
      "    }\n",
      "    val thrownIntercept = intercept[SparkException] {\n",
      "      mlrModel.intercept\n",
      "    }\n",
      "    assert(thrownCoef.getMessage().contains(\"use coefficientMatrix instead\"))\n",
      "    assert(thrownIntercept.getMessage().contains(\"use interceptVector instead\"))\n",
      "\n",
      "    val blr = new LogisticRegression().setMaxIter(1).setFamily(\"binomial\")\n",
      "    val blrModel = blr.fit(smallBinaryDataset)\n",
      "    assert(blrModel.coefficients.size === 1)\n",
      "    assert(blrModel.intercept !== 0.0)\n",
      "  }\n",
      "\n",
      "  test(\"overflow prediction for multiclass\") {\n",
      "    val spark = SparkSession.builder().getOrCreate()\n",
      "    import spark.implicits._\n",
      "    val model = new LogisticRegressionModel(\"mLogReg\",\n",
      "      Matrices.dense(3, 2, Array(0.0, 0.0, 0.0, 1.0, 2.0, 3.0)),\n",
      "      Vectors.dense(0.0, 0.0, 0.0), 3, true)\n",
      "    val overFlowData = Seq(\n",
      "      LabeledPoint(1.0, Vectors.dense(0.0, 1000.0)),\n",
      "      LabeledPoint(1.0, Vectors.dense(0.0, -1.0))\n",
      "    ).toDF()\n",
      "    val results = model.transform(overFlowData).select(\"rawPrediction\", \"probability\").collect()\n",
      "\n",
      "    // probabilities are correct when margins have to be adjusted\n",
      "    val raw1 = results(0).getAs[Vector](0)\n",
      "    val prob1 = results(0).getAs[Vector](1)\n",
      "    assert(raw1 === Vectors.dense(1000.0, 2000.0, 3000.0))\n",
      "    assert(prob1 ~== Vectors.dense(0.0, 0.0, 1.0) absTol eps)\n",
      "\n",
      "    // probabilities are correct when margins don't have to be adjusted\n",
      "    val raw2 = results(1).getAs[Vector](0)\n",
      "    val prob2 = results(1).getAs[Vector](1)\n",
      "    assert(raw2 === Vectors.dense(-1.0, -2.0, -3.0))\n",
      "    assert(prob2 ~== Vectors.dense(0.66524096, 0.24472847, 0.09003057) relTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"MultiClassSummarizer\") {\n",
      "    val summarizer1 = (new MultiClassSummarizer)\n",
      "      .add(0.0).add(3.0).add(4.0).add(3.0).add(6.0)\n",
      "    assert(summarizer1.histogram === Array[Double](1, 0, 0, 2, 1, 0, 1))\n",
      "    assert(summarizer1.countInvalid === 0)\n",
      "    assert(summarizer1.numClasses === 7)\n",
      "\n",
      "    val summarizer2 = (new MultiClassSummarizer)\n",
      "      .add(1.0).add(5.0).add(3.0).add(0.0).add(4.0).add(1.0)\n",
      "    assert(summarizer2.histogram === Array[Double](1, 2, 0, 1, 1, 1))\n",
      "    assert(summarizer2.countInvalid === 0)\n",
      "    assert(summarizer2.numClasses === 6)\n",
      "\n",
      "    val summarizer3 = (new MultiClassSummarizer)\n",
      "      .add(0.0).add(1.3).add(5.2).add(2.5).add(2.0).add(4.0).add(4.0).add(4.0).add(1.0)\n",
      "    assert(summarizer3.histogram === Array[Double](1, 1, 1, 0, 3))\n",
      "    assert(summarizer3.countInvalid === 3)\n",
      "    assert(summarizer3.numClasses === 5)\n",
      "\n",
      "    val summarizer4 = (new MultiClassSummarizer)\n",
      "      .add(3.1).add(4.3).add(2.0).add(1.0).add(3.0)\n",
      "    assert(summarizer4.histogram === Array[Double](0, 1, 1, 1))\n",
      "    assert(summarizer4.countInvalid === 2)\n",
      "    assert(summarizer4.numClasses === 4)\n",
      "\n",
      "    val summarizer5 = new MultiClassSummarizer\n",
      "    assert(summarizer5.histogram.isEmpty)\n",
      "    assert(summarizer5.numClasses === 0)\n",
      "\n",
      "    // small map merges large one\n",
      "    val summarizerA = summarizer1.merge(summarizer2)\n",
      "    assert(summarizerA.hashCode() === summarizer2.hashCode())\n",
      "    assert(summarizerA.histogram === Array[Double](2, 2, 0, 3, 2, 1, 1))\n",
      "    assert(summarizerA.countInvalid === 0)\n",
      "    assert(summarizerA.numClasses === 7)\n",
      "\n",
      "    // large map merges small one\n",
      "    val summarizerB = summarizer3.merge(summarizer4)\n",
      "    assert(summarizerB.hashCode() === summarizer3.hashCode())\n",
      "    assert(summarizerB.histogram === Array[Double](1, 2, 2, 1, 3))\n",
      "    assert(summarizerB.countInvalid === 5)\n",
      "    assert(summarizerB.numClasses === 5)\n",
      "  }\n",
      "\n",
      "  test(\"MultiClassSummarizer with weighted samples\") {\n",
      "    val summarizer1 = (new MultiClassSummarizer)\n",
      "      .add(label = 0.0, weight = 0.2).add(3.0, 0.8).add(4.0, 3.2).add(3.0, 1.3).add(6.0, 3.1)\n",
      "    assert(Vectors.dense(summarizer1.histogram) ~==\n",
      "      Vectors.dense(Array(0.2, 0, 0, 2.1, 3.2, 0, 3.1)) absTol 1E-10)\n",
      "    assert(summarizer1.countInvalid === 0)\n",
      "    assert(summarizer1.numClasses === 7)\n",
      "\n",
      "    val summarizer2 = (new MultiClassSummarizer)\n",
      "      .add(1.0, 1.1).add(5.0, 2.3).add(3.0).add(0.0).add(4.0).add(1.0).add(2, 0.0)\n",
      "    assert(Vectors.dense(summarizer2.histogram) ~==\n",
      "      Vectors.dense(Array[Double](1.0, 2.1, 0.0, 1, 1, 2.3)) absTol 1E-10)\n",
      "    assert(summarizer2.countInvalid === 0)\n",
      "    assert(summarizer2.numClasses === 6)\n",
      "\n",
      "    val summarizer = summarizer1.merge(summarizer2)\n",
      "    assert(Vectors.dense(summarizer.histogram) ~==\n",
      "      Vectors.dense(Array(1.2, 2.1, 0.0, 3.1, 4.2, 2.3, 3.1)) absTol 1E-10)\n",
      "    assert(summarizer.countInvalid === 0)\n",
      "    assert(summarizer.numClasses === 7)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept without regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setStandardization(true)\n",
      "      .setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true).setStandardization(false)\n",
      "      .setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 0))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                          s0\n",
      "      (Intercept)  2.7355261\n",
      "      data.V3     -0.5734389\n",
      "      data.V4      0.8911736\n",
      "      data.V5     -0.3878645\n",
      "      data.V6     -0.8060570\n",
      "\n",
      "     */\n",
      "    val coefficientsR = Vectors.dense(-0.5734389, 0.8911736, -0.3878645, -0.8060570)\n",
      "    val interceptR = 2.7355261\n",
      "\n",
      "    assert(model1.intercept ~== interceptR relTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsR relTol 1E-3)\n",
      "\n",
      "    // Without regularization, with or without standardization will converge to the same solution.\n",
      "    assert(model2.intercept ~== interceptR relTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR relTol 1E-3)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression without intercept without regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false).setStandardization(true)\n",
      "      .setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false).setStandardization(false)\n",
      "      .setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 0, intercept=FALSE))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                          s0\n",
      "      (Intercept)  .\n",
      "      data.V3     -0.3448461\n",
      "      data.V4      1.2776453\n",
      "      data.V5     -0.3539178\n",
      "      data.V6     -0.7469384\n",
      "\n",
      "     */\n",
      "    val coefficientsR = Vectors.dense(-0.3448461, 1.2776453, -0.3539178, -0.7469384)\n",
      "\n",
      "    assert(model1.intercept ~== 0.0 relTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsR relTol 1E-2)\n",
      "\n",
      "    // Without regularization, with or without standardization should converge to the same solution.\n",
      "    assert(model2.intercept ~== 0.0 relTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR relTol 1E-2)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept with L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.12).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.12).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1,\n",
      "      lambda = 0.12, standardize=T))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept) -0.06775980\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.03933146\n",
      "      data.V6     -0.03047580\n",
      "\n",
      "     */\n",
      "    val coefficients = Array(-0.57997, 0.912083, -0.371077, -0.819866, 2.688191)\n",
      "    val coefficientsRStd = Vectors.dense(0.0, 0.0, -0.03933146, -0.03047580)\n",
      "    val interceptRStd = -0.06775980\n",
      "\n",
      "    assert(model1.intercept ~== interceptRStd relTol 1E-2)\n",
      "    assert(model1.coefficients ~= coefficientsRStd absTol 2E-2)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1,\n",
      "      lambda = 0.12, standardize=F))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                          s0\n",
      "      (Intercept)  0.3544768\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.1626191\n",
      "      data.V6      .\n",
      "\n",
      "     */\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.0, -0.1626191, 0.0)\n",
      "    val interceptR = 0.3544768\n",
      "\n",
      "    println(s\"coff ${model2.coefficientMatrix.toArray.mkString(\" \")}\")\n",
      "    println(s\"intercept ${model2.interceptVector.toArray.mkString(\" \")}\")\n",
      "\n",
      "    assert(model2.intercept ~== interceptR relTol 1E-2)\n",
      "    assert(model2.coefficients ~== coefficientsR absTol 1E-3)\n",
      "    // TODO: move this to a standalone test of compression after SPARK-17471\n",
      "    assert(model2.coefficients.isInstanceOf[SparseVector])\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression without intercept with L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.12).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.12).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1,\n",
      "      lambda = 0.12, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1,\n",
      "      lambda = 0.12, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.04967635\n",
      "      data.V6     -0.04757757\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.08433195\n",
      "      data.V6      .\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(0.0, 0.0, -0.04967635, -0.04757757)\n",
      "\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.0, -0.08433195, 0.0)\n",
      "\n",
      "    assert(model1.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsRStd absTol 1E-3)\n",
      "    assert(model2.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR absTol 1E-3)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept with L2 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(1.37).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(1.37).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 1.37, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 1.37, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  0.12707703\n",
      "      data.V3     -0.06980967\n",
      "      data.V4      0.10803933\n",
      "      data.V5     -0.04800404\n",
      "      data.V6     -0.10165096\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  0.46613016\n",
      "      data.V3     -0.04944529\n",
      "      data.V4      0.02326772\n",
      "      data.V5     -0.11362772\n",
      "      data.V6     -0.06312848\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(-0.06980967, 0.10803933, -0.04800404, -0.10165096)\n",
      "    val interceptRStd = 0.12707703\n",
      "    val coefficientsR = Vectors.dense(-0.04944529, 0.02326772, -0.11362772, -0.06312848)\n",
      "    val interceptR = 0.46613016\n",
      "\n",
      "    assert(model1.intercept ~== interceptRStd relTol 1E-2)\n",
      "    assert(model1.coefficients ~= coefficientsRStd relTol 1E-2)\n",
      "    assert(model2.intercept ~== interceptR relTol 3E-2)\n",
      "    assert(model2.coefficients ~= coefficientsR relTol 1E-2)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression without intercept with L2 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(1.37).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(1.37).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 1.37, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 1.37, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3     -0.06000152\n",
      "      data.V4      0.12598737\n",
      "      data.V5     -0.04669009\n",
      "      data.V6     -0.09941025\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                            s0\n",
      "      (Intercept)  .\n",
      "      data.V3     -0.005482255\n",
      "      data.V4      0.048106338\n",
      "      data.V5     -0.093411640\n",
      "      data.V6     -0.054149798\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(-0.06000152, 0.12598737, -0.04669009, -0.09941025)\n",
      "    val coefficientsR = Vectors.dense(-0.005482255, 0.048106338, -0.093411640, -0.054149798)\n",
      "\n",
      "    assert(model1.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsRStd relTol 1E-2)\n",
      "    assert(model2.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR relTol 1E-2)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept with ElasticNet regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setMaxIter(200)\n",
      "      .setElasticNetParam(0.38).setRegParam(0.21).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.38).setRegParam(0.21).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0.38,\n",
      "      lambda = 0.21, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0.38,\n",
      "      lambda = 0.21, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  0.49991996\n",
      "      data.V3     -0.04131110\n",
      "      data.V4      .\n",
      "      data.V5     -0.08585233\n",
      "      data.V6     -0.15875400\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                          s0\n",
      "      (Intercept)  0.5024256\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.1846038\n",
      "      data.V6     -0.0559614\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(-0.04131110, 0.0, -0.08585233, -0.15875400)\n",
      "    val interceptRStd = 0.49991996\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.0, -0.1846038, -0.0559614)\n",
      "    val interceptR = 0.5024256\n",
      "\n",
      "    assert(model1.intercept ~== interceptRStd relTol 6E-3)\n",
      "    assert(model1.coefficients ~== coefficientsRStd absTol 5E-3)\n",
      "    assert(model2.intercept ~== interceptR relTol 6E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR absTol 1E-3)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression without intercept with ElasticNet regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.38).setRegParam(0.21).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.38).setRegParam(0.21).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0.38,\n",
      "      lambda = 0.21, intercept=FALSE, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0.38,\n",
      "      lambda = 0.21, intercept=FALSE, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3      .\n",
      "      data.V4      0.06859390\n",
      "      data.V5     -0.07900058\n",
      "      data.V6     -0.14684320\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3      .\n",
      "      data.V4      0.03060637\n",
      "      data.V5     -0.11126742\n",
      "      data.V6      .\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(0.0, 0.06859390, -0.07900058, -0.14684320)\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.03060637, -0.11126742, 0.0)\n",
      "\n",
      "    assert(model1.intercept ~== 0.0 relTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsRStd absTol 1E-2)\n",
      "    assert(model2.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR absTol 1E-2)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept with strong L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(1.0).setRegParam(6.0).setStandardization(true)\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(1.0).setRegParam(6.0).setStandardization(false)\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    val histogram = binaryDataset.as[Instance].rdd.map { i => (i.label, i.weight)}\n",
      "      .treeAggregate(new MultiClassSummarizer)(\n",
      "        seqOp = (c, v) => (c, v) match {\n",
      "          case (classSummarizer: MultiClassSummarizer, (label: Double, weight: Double)) =>\n",
      "            classSummarizer.add(label, weight)\n",
      "        },\n",
      "        combOp = (c1, c2) => (c1, c2) match {\n",
      "          case (classSummarizer1: MultiClassSummarizer, classSummarizer2: MultiClassSummarizer) =>\n",
      "            classSummarizer1.merge(classSummarizer2)\n",
      "        }).histogram\n",
      "\n",
      "    /*\n",
      "       For binary logistic regression with strong L1 regularization, all the coefficients\n",
      "       will be zeros. As a result,\n",
      "       {{{\n",
      "       P(0) = 1 / (1 + \\exp(b)), and\n",
      "       P(1) = \\exp(b) / (1 + \\exp(b))\n",
      "       }}}, hence\n",
      "       {{{\n",
      "       b = \\log{P(1) / P(0)} = \\log{count_1 / count_0}\n",
      "       }}}\n",
      "     */\n",
      "    val interceptTheory = math.log(histogram(1) / histogram(0))\n",
      "    val coefficientsTheory = Vectors.dense(0.0, 0.0, 0.0, 0.0)\n",
      "\n",
      "    assert(model1.intercept ~== interceptTheory relTol 1E-5)\n",
      "    assert(model1.coefficients ~= coefficientsTheory absTol 1E-6)\n",
      "\n",
      "    assert(model2.intercept ~== interceptTheory relTol 1E-5)\n",
      "    assert(model2.coefficients ~= coefficientsTheory absTol 1E-6)\n",
      "\n",
      "    /*\n",
      "       Using the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "       library(\"glmnet\")\n",
      "       data <- read.csv(\"path\", header=FALSE)\n",
      "       label = factor(data$V1)\n",
      "       w = data$V2\n",
      "       features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "       coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1.0,\n",
      "       lambda = 6.0))\n",
      "       coefficients\n",
      "\n",
      "       5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "       (Intercept) -0.2516986\n",
      "       data.V3      0.0000000\n",
      "       data.V4      .\n",
      "       data.V5      .\n",
      "       data.V6      .\n",
      "     */\n",
      "    val interceptR = -0.2516986\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.0, 0.0, 0.0)\n",
      "\n",
      "    assert(model1.intercept ~== interceptR relTol 1E-5)\n",
      "    assert(model1.coefficients ~== coefficientsR absTol 1E-6)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept with strong L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(1.0).setRegParam(6.0).setStandardization(true)\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(1.0).setRegParam(6.0).setStandardization(false)\n",
      "\n",
      "    val sqlContext = multinomialDataset.sqlContext\n",
      "    import sqlContext.implicits._\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "\n",
      "    val histogram = multinomialDataset.as[Instance].rdd.map(i => (i.label, i.weight))\n",
      "      .treeAggregate(new MultiClassSummarizer)(\n",
      "        seqOp = (c, v) => (c, v) match {\n",
      "          case (classSummarizer: MultiClassSummarizer, (label: Double, weight: Double)) =>\n",
      "            classSummarizer.add(label, weight)\n",
      "        },\n",
      "        combOp = (c1, c2) => (c1, c2) match {\n",
      "          case (classSummarizer1: MultiClassSummarizer, classSummarizer2: MultiClassSummarizer) =>\n",
      "            classSummarizer1.merge(classSummarizer2)\n",
      "        }).histogram\n",
      "    val numFeatures = multinomialDataset.as[Instance].first().features.size\n",
      "    val numClasses = histogram.length\n",
      "\n",
      "    /*\n",
      "       For multinomial logistic regression with strong L1 regularization, all the coefficients\n",
      "       will be zeros. As a result, the intercepts will be proportional to the log counts in the\n",
      "       histogram.\n",
      "       {{{\n",
      "         \\exp(b_k) = count_k * \\exp(\\lambda)\n",
      "         b_k = \\log(count_k) * \\lambda\n",
      "       }}}\n",
      "       \\lambda is a free parameter, so choose the phase \\lambda such that the\n",
      "       mean is centered. This yields\n",
      "       {{{\n",
      "         b_k = \\log(count_k)\n",
      "         b_k' = b_k - \\mean(b_k)\n",
      "       }}}\n",
      "     */\n",
      "    val rawInterceptsTheory = histogram.map(c => math.log(c + 1)) // add 1 for smoothing\n",
      "    val rawMean = rawInterceptsTheory.sum / rawInterceptsTheory.length\n",
      "    val interceptsTheory = Vectors.dense(rawInterceptsTheory.map(_ - rawMean))\n",
      "    val coefficientsTheory = new DenseMatrix(numClasses, numFeatures,\n",
      "      Array.fill[Double](numClasses * numFeatures)(0.0), isTransposed = true)\n",
      "\n",
      "    assert(model1.interceptVector ~== interceptsTheory relTol 1E-3)\n",
      "    assert(model1.coefficientMatrix ~= coefficientsTheory absTol 1E-6)\n",
      "\n",
      "    assert(model2.interceptVector ~== interceptsTheory relTol 1E-3)\n",
      "    assert(model2.coefficientMatrix ~= coefficientsTheory absTol 1E-6)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept without regularization\") {\n",
      "\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.0).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.0).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\",\n",
      "      alpha = 0, lambda = 0))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -2.10320093\n",
      "      data.V3  0.24337896\n",
      "      data.V4 -0.05916156\n",
      "      data.V5  0.14446790\n",
      "      data.V6  0.35976165\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               0.3394473\n",
      "      data.V3 -0.3443375\n",
      "      data.V4  0.9181331\n",
      "      data.V5 -0.2283959\n",
      "      data.V6 -0.4388066\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               1.76375361\n",
      "      data.V3  0.10095851\n",
      "      data.V4 -0.85897154\n",
      "      data.V5  0.08392798\n",
      "      data.V6  0.07904499\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.24337896, -0.05916156, 0.14446790, 0.35976165,\n",
      "      -0.3443375, 0.9181331, -0.2283959, -0.4388066,\n",
      "      0.10095851, -0.85897154, 0.08392798, 0.07904499), isTransposed = true)\n",
      "    val interceptsR = Vectors.dense(-2.10320093, 0.3394473, 1.76375361)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model1.coefficientMatrix.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model1.interceptVector ~== interceptsR relTol 0.05)\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model2.coefficientMatrix.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.interceptVector ~== interceptsR relTol 0.05)\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression without intercept without regularization\") {\n",
      "\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.0).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.0).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0,\n",
      "      lambda = 0, intercept=F))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  0.07276291\n",
      "      data.V4 -0.36325496\n",
      "      data.V5  0.12015088\n",
      "      data.V6  0.31397340\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3 -0.3180040\n",
      "      data.V4  0.9679074\n",
      "      data.V5 -0.2252219\n",
      "      data.V6 -0.4319914\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3  0.2452411\n",
      "      data.V4 -0.6046524\n",
      "      data.V5  0.1050710\n",
      "      data.V6  0.1180180\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.07276291, -0.36325496, 0.12015088, 0.31397340,\n",
      "      -0.3180040, 0.9679074, -0.2252219, -0.4319914,\n",
      "      0.2452411, -0.6046524, 0.1050710, 0.1180180), isTransposed = true)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model1.coefficientMatrix.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model1.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model2.coefficientMatrix.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept with L1 regularization\") {\n",
      "\n",
      "    // use tighter constraints because OWL-QN solver takes longer to converge\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.05).setStandardization(true)\n",
      "      .setMaxIter(300).setTol(1e-10).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.05).setStandardization(false)\n",
      "      .setMaxIter(300).setTol(1e-10).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\",\n",
      "      alpha = 1, lambda = 0.05, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 1,\n",
      "      lambda = 0.05, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -0.62244703\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  0.08419825\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -0.2804845\n",
      "      data.V3 -0.1336960\n",
      "      data.V4  0.3717091\n",
      "      data.V5 -0.1530363\n",
      "      data.V6 -0.2035286\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               0.9029315\n",
      "      data.V3  .\n",
      "      data.V4 -0.4629737\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -0.44215290\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  0.01767089\n",
      "      data.V6  0.02542866\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               0.76308326\n",
      "      data.V3 -0.06818576\n",
      "      data.V4  .\n",
      "      data.V5 -0.20446351\n",
      "      data.V6 -0.13017924\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -0.3209304\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.08419825,\n",
      "      -0.1336960, 0.3717091, -0.1530363, -0.2035286,\n",
      "      0.0, -0.4629737, 0.0, 0.0), isTransposed = true)\n",
      "    val interceptsRStd = Vectors.dense(-0.62244703, -0.2804845, 0.9029315)\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.01767089, 0.02542866,\n",
      "      -0.06818576, 0.0, -0.20446351, -0.13017924,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "    val interceptsR = Vectors.dense(-0.44215290, 0.76308326, -0.3209304)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.2)\n",
      "    assert(model1.interceptVector ~== interceptsRStd relTol 0.1)\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.02)\n",
      "    assert(model2.interceptVector ~== interceptsR relTol 0.1)\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression without intercept with L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.05).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.05).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 1,\n",
      "      lambda = 0.05, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 1,\n",
      "      lambda = 0.05, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              .\n",
      "      data.V3 .\n",
      "      data.V4 .\n",
      "      data.V5 .\n",
      "      data.V6 0.01144225\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3 -0.1678787\n",
      "      data.V4  0.5385351\n",
      "      data.V5 -0.1573039\n",
      "      data.V6 -0.2471624\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  0.1929409\n",
      "      data.V5 -0.1889121\n",
      "      data.V6 -0.1010413\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.01144225,\n",
      "      -0.1678787, 0.5385351, -0.1573039, -0.2471624,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.0,\n",
      "      0.0, 0.1929409, -0.1889121, -0.1010413,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.01)\n",
      "    assert(model2.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept with L2 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.1).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.1).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame( data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\",\n",
      "      alpha = 0, lambda = 0.1, intercept=T, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0,\n",
      "      lambda = 0.1, intercept=T, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                         s0\n",
      "              -1.5898288335\n",
      "      data.V3  0.1691226336\n",
      "      data.V4  0.0002983651\n",
      "      data.V5  0.1001732896\n",
      "      data.V6  0.2554575585\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               0.2125746\n",
      "      data.V3 -0.2304586\n",
      "      data.V4  0.6153492\n",
      "      data.V5 -0.1537017\n",
      "      data.V6 -0.2975443\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               1.37725427\n",
      "      data.V3  0.06133600\n",
      "      data.V4 -0.61564761\n",
      "      data.V5  0.05352840\n",
      "      data.V6  0.04208671\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -1.5681088\n",
      "      data.V3  0.1508182\n",
      "      data.V4  0.0121955\n",
      "      data.V5  0.1217930\n",
      "      data.V6  0.2162850\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               1.1217130\n",
      "      data.V3 -0.2028984\n",
      "      data.V4  0.2862431\n",
      "      data.V5 -0.1843559\n",
      "      data.V6 -0.2481218\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               0.44639579\n",
      "      data.V3  0.05208012\n",
      "      data.V4 -0.29843864\n",
      "      data.V5  0.06256289\n",
      "      data.V6  0.03183676\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.1691226336, 0.0002983651, 0.1001732896, 0.2554575585,\n",
      "      -0.2304586, 0.6153492, -0.1537017, -0.2975443,\n",
      "      0.06133600, -0.61564761, 0.05352840, 0.04208671), isTransposed = true)\n",
      "    val interceptsRStd = Vectors.dense(-1.5898288335, 0.2125746, 1.37725427)\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.1508182, 0.0121955, 0.1217930, 0.2162850,\n",
      "      -0.2028984, 0.2862431, -0.1843559, -0.2481218,\n",
      "      0.05208012, -0.29843864, 0.06256289, 0.03183676), isTransposed = true)\n",
      "    val interceptsR = Vectors.dense(-1.5681088, 1.1217130, 0.44639579)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.001)\n",
      "    assert(model1.interceptVector ~== interceptsRStd relTol 0.05)\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model2.interceptVector ~== interceptsR relTol 0.05)\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression without intercept with L2 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.1).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.1).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0,\n",
      "      lambda = 0.1, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0,\n",
      "      lambda = 0.1, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  0.04048126\n",
      "      data.V4 -0.23075758\n",
      "      data.V5  0.08228864\n",
      "      data.V6  0.22277648\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3 -0.2149745\n",
      "      data.V4  0.6478666\n",
      "      data.V5 -0.1515158\n",
      "      data.V6 -0.2930498\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  0.17449321\n",
      "      data.V4 -0.41710901\n",
      "      data.V5  0.06922716\n",
      "      data.V6  0.07027332\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                        s0\n",
      "               .\n",
      "      data.V3 -0.003949652\n",
      "      data.V4 -0.142982415\n",
      "      data.V5  0.091439598\n",
      "      data.V6  0.179286241\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3 -0.09071124\n",
      "      data.V4  0.39752531\n",
      "      data.V5 -0.16233832\n",
      "      data.V6 -0.22206059\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  0.09466090\n",
      "      data.V4 -0.25454290\n",
      "      data.V5  0.07089872\n",
      "      data.V6  0.04277435\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.04048126, -0.23075758, 0.08228864, 0.22277648,\n",
      "      -0.2149745, 0.6478666, -0.1515158, -0.2930498,\n",
      "      0.17449321, -0.41710901, 0.06922716, 0.07027332), isTransposed = true)\n",
      "\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      -0.003949652, -0.142982415, 0.091439598, 0.179286241,\n",
      "      -0.09071124, 0.39752531, -0.16233832, -0.22206059,\n",
      "      0.09466090, -0.25454290, 0.07089872, 0.04277435), isTransposed = true)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.01)\n",
      "    assert(model2.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept with elasticnet regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(0.5).setRegParam(0.1).setStandardization(true)\n",
      "      .setMaxIter(300).setTol(1e-10)\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(0.5).setRegParam(0.1).setStandardization(false)\n",
      "      .setMaxIter(300).setTol(1e-10)\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0.5,\n",
      "      lambda = 0.1, intercept=T, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0.5,\n",
      "      lambda = 0.1, intercept=T, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -0.50133383\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  0.08351653\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -0.3151913\n",
      "      data.V3 -0.1058702\n",
      "      data.V4  0.3183251\n",
      "      data.V5 -0.1212969\n",
      "      data.V6 -0.1629778\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               0.8165252\n",
      "      data.V3  .\n",
      "      data.V4 -0.3943069\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -0.38857157\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  0.02384198\n",
      "      data.V6  0.03127749\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               0.62492165\n",
      "      data.V3 -0.04949061\n",
      "      data.V4  .\n",
      "      data.V5 -0.18584462\n",
      "      data.V6 -0.08952455\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -0.2363501\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.08351653,\n",
      "      -0.1058702, 0.3183251, -0.1212969, -0.1629778,\n",
      "      0.0, -0.3943069, 0.0, 0.0), isTransposed = true)\n",
      "    val interceptsRStd = Vectors.dense(-0.50133383, -0.3151913, 0.8165252)\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.02384198, 0.03127749,\n",
      "      -0.04949061, 0.0, -0.18584462, -0.08952455,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "    val interceptsR = Vectors.dense(-0.38857157, 0.62492165, -0.2363501)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector ~== interceptsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.01)\n",
      "    assert(model2.interceptVector ~== interceptsR absTol 0.01)\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression without intercept with elasticnet regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(0.5).setRegParam(0.1).setStandardization(true)\n",
      "      .setMaxIter(300).setTol(1e-10)\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(0.5).setRegParam(0.1).setStandardization(false)\n",
      "      .setMaxIter(300).setTol(1e-10)\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0.5,\n",
      "      lambda = 0.1, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0.5,\n",
      "      lambda = 0.1, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              .\n",
      "      data.V3 .\n",
      "      data.V4 .\n",
      "      data.V5 .\n",
      "      data.V6 0.03238285\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3 -0.1328284\n",
      "      data.V4  0.4219321\n",
      "      data.V5 -0.1247544\n",
      "      data.V6 -0.1893318\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              .\n",
      "      data.V3 0.004572312\n",
      "      data.V4 .\n",
      "      data.V5 .\n",
      "      data.V6 .\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  0.14571623\n",
      "      data.V5 -0.16456351\n",
      "      data.V6 -0.05866264\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.03238285,\n",
      "      -0.1328284, 0.4219321, -0.1247544, -0.1893318,\n",
      "      0.004572312, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.0,\n",
      "      0.0, 0.14571623, -0.16456351, -0.05866264,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.01)\n",
      "    assert(model2.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"evaluate on test set\") {\n",
      "    // TODO: add for multiclass when model summary becomes available\n",
      "    // Evaluate on test set should be same as that of the transformed training data.\n",
      "    val lr = new LogisticRegression()\n",
      "      .setMaxIter(10)\n",
      "      .setRegParam(1.0)\n",
      "      .setThreshold(0.6)\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    val summary = model.summary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
      "\n",
      "    val sameSummary =\n",
      "      model.evaluate(smallBinaryDataset).asInstanceOf[BinaryLogisticRegressionSummary]\n",
      "    assert(summary.areaUnderROC === sameSummary.areaUnderROC)\n",
      "    assert(summary.roc.collect() === sameSummary.roc.collect())\n",
      "    assert(summary.pr.collect === sameSummary.pr.collect())\n",
      "    assert(\n",
      "      summary.fMeasureByThreshold.collect() === sameSummary.fMeasureByThreshold.collect())\n",
      "    assert(summary.recallByThreshold.collect() === sameSummary.recallByThreshold.collect())\n",
      "    assert(\n",
      "      summary.precisionByThreshold.collect() === sameSummary.precisionByThreshold.collect())\n",
      "  }\n",
      "\n",
      "  test(\"evaluate with labels that are not doubles\") {\n",
      "    // Evaluate a test set with Label that is a numeric type other than Double\n",
      "    val lr = new LogisticRegression()\n",
      "      .setMaxIter(1)\n",
      "      .setRegParam(1.0)\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    val summary = model.evaluate(smallBinaryDataset).asInstanceOf[BinaryLogisticRegressionSummary]\n",
      "\n",
      "    val longLabelData = smallBinaryDataset.select(col(model.getLabelCol).cast(LongType),\n",
      "      col(model.getFeaturesCol))\n",
      "    val longSummary = model.evaluate(longLabelData).asInstanceOf[BinaryLogisticRegressionSummary]\n",
      "\n",
      "    assert(summary.areaUnderROC === longSummary.areaUnderROC)\n",
      "  }\n",
      "\n",
      "  test(\"statistics on training data\") {\n",
      "    // Test that loss is monotonically decreasing.\n",
      "    val lr = new LogisticRegression()\n",
      "      .setMaxIter(10)\n",
      "      .setRegParam(1.0)\n",
      "      .setThreshold(0.6)\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    assert(\n",
      "      model.summary\n",
      "        .objectiveHistory\n",
      "        .sliding(2)\n",
      "        .forall(x => x(0) >= x(1)))\n",
      "  }\n",
      "\n",
      "  test(\"set family\") {\n",
      "    val lr = new LogisticRegression().setMaxIter(1)\n",
      "    // don't set anything for binary classification\n",
      "    val model1 = lr.fit(binaryDataset)\n",
      "    assert(model1.coefficientMatrix.numRows === 1 && model1.coefficientMatrix.numCols === 4)\n",
      "    assert(model1.interceptVector.size === 1)\n",
      "\n",
      "    // set to multinomial for binary classification\n",
      "    val model2 = lr.setFamily(\"multinomial\").fit(binaryDataset)\n",
      "    assert(model2.coefficientMatrix.numRows === 2 && model2.coefficientMatrix.numCols === 4)\n",
      "    assert(model2.interceptVector.size === 2)\n",
      "\n",
      "    // set to binary for binary classification\n",
      "    val model3 = lr.setFamily(\"binomial\").fit(binaryDataset)\n",
      "    assert(model3.coefficientMatrix.numRows === 1 && model3.coefficientMatrix.numCols === 4)\n",
      "    assert(model3.interceptVector.size === 1)\n",
      "\n",
      "    // don't set anything for multiclass classification\n",
      "    val mlr = new LogisticRegression().setMaxIter(1)\n",
      "    val model4 = mlr.fit(multinomialDataset)\n",
      "    assert(model4.coefficientMatrix.numRows === 3 && model4.coefficientMatrix.numCols === 4)\n",
      "    assert(model4.interceptVector.size === 3)\n",
      "\n",
      "    // set to binary for multiclass classification\n",
      "    mlr.setFamily(\"binomial\")\n",
      "    val thrown = intercept[IllegalArgumentException] {\n",
      "      mlr.fit(multinomialDataset)\n",
      "    }\n",
      "    assert(thrown.getMessage.contains(\"Binomial family only supports 1 or 2 outcome classes\"))\n",
      "\n",
      "    // set to multinomial for multiclass\n",
      "    mlr.setFamily(\"multinomial\")\n",
      "    val model5 = mlr.fit(multinomialDataset)\n",
      "    assert(model5.coefficientMatrix.numRows === 3 && model5.coefficientMatrix.numCols === 4)\n",
      "    assert(model5.interceptVector.size === 3)\n",
      "  }\n",
      "\n",
      "  test(\"set initial model\") {\n",
      "    val lr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    val model1 = lr.fit(smallBinaryDataset)\n",
      "    val lr2 = new LogisticRegression().setInitialModel(model1).setMaxIter(5).setFamily(\"binomial\")\n",
      "    val model2 = lr2.fit(smallBinaryDataset)\n",
      "    val predictions1 = model1.transform(smallBinaryDataset).select(\"prediction\").collect()\n",
      "    val predictions2 = model2.transform(smallBinaryDataset).select(\"prediction\").collect()\n",
      "    predictions1.zip(predictions2).foreach { case (Row(p1: Double), Row(p2: Double)) =>\n",
      "      assert(p1 === p2)\n",
      "    }\n",
      "    assert(model2.summary.totalIterations === 1)\n",
      "\n",
      "    val lr3 = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    val model3 = lr3.fit(smallMultinomialDataset)\n",
      "    val lr4 = new LogisticRegression()\n",
      "      .setInitialModel(model3).setMaxIter(5).setFamily(\"multinomial\")\n",
      "    val model4 = lr4.fit(smallMultinomialDataset)\n",
      "    val predictions3 = model3.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    val predictions4 = model4.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    predictions3.zip(predictions4).foreach { case (Row(p1: Double), Row(p2: Double)) =>\n",
      "      assert(p1 === p2)\n",
      "    }\n",
      "    // TODO: check that it converges in a single iteration when model summary is available\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with all labels the same\") {\n",
      "    val sameLabels = smallBinaryDataset\n",
      "      .withColumn(\"zeroLabel\", lit(0.0))\n",
      "      .withColumn(\"oneLabel\", lit(1.0))\n",
      "\n",
      "    // fitIntercept=true\n",
      "    val lrIntercept = new LogisticRegression()\n",
      "      .setFitIntercept(true)\n",
      "      .setMaxIter(3)\n",
      "      .setFamily(\"binomial\")\n",
      "\n",
      "    val allZeroInterceptModel = lrIntercept\n",
      "      .setLabelCol(\"zeroLabel\")\n",
      "      .fit(sameLabels)\n",
      "    assert(allZeroInterceptModel.coefficients ~== Vectors.dense(0.0) absTol 1E-3)\n",
      "    assert(allZeroInterceptModel.intercept === Double.NegativeInfinity)\n",
      "    assert(allZeroInterceptModel.summary.totalIterations === 0)\n",
      "\n",
      "    val allOneInterceptModel = lrIntercept\n",
      "      .setLabelCol(\"oneLabel\")\n",
      "      .fit(sameLabels)\n",
      "    assert(allOneInterceptModel.coefficients ~== Vectors.dense(0.0) absTol 1E-3)\n",
      "    assert(allOneInterceptModel.intercept === Double.PositiveInfinity)\n",
      "    assert(allOneInterceptModel.summary.totalIterations === 0)\n",
      "\n",
      "    // fitIntercept=false\n",
      "    val lrNoIntercept = new LogisticRegression()\n",
      "      .setFitIntercept(false)\n",
      "      .setMaxIter(3)\n",
      "      .setFamily(\"binomial\")\n",
      "\n",
      "    val allZeroNoInterceptModel = lrNoIntercept\n",
      "      .setLabelCol(\"zeroLabel\")\n",
      "      .fit(sameLabels)\n",
      "    assert(allZeroNoInterceptModel.intercept === 0.0)\n",
      "    assert(allZeroNoInterceptModel.summary.totalIterations > 0)\n",
      "\n",
      "    val allOneNoInterceptModel = lrNoIntercept\n",
      "      .setLabelCol(\"oneLabel\")\n",
      "      .fit(sameLabels)\n",
      "    assert(allOneNoInterceptModel.intercept === 0.0)\n",
      "    assert(allOneNoInterceptModel.summary.totalIterations > 0)\n",
      "  }\n",
      "\n",
      "  test(\"multiclass logistic regression with all labels the same\") {\n",
      "    val spark = SparkSession.builder().getOrCreate()\n",
      "    import spark.implicits._\n",
      "\n",
      "    val constantData = Seq(\n",
      "      LabeledPoint(4.0, Vectors.dense(0.0)),\n",
      "      LabeledPoint(4.0, Vectors.dense(1.0)),\n",
      "      LabeledPoint(4.0, Vectors.dense(2.0))).toDF()\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    val model = mlr.fit(constantData)\n",
      "    val results = model.transform(constantData)\n",
      "    results.select(\"rawPrediction\", \"probability\", \"prediction\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector, pred: Double) =>\n",
      "        assert(raw === Vectors.dense(Array(0.0, 0.0, 0.0, 0.0, Double.PositiveInfinity)))\n",
      "        assert(prob === Vectors.dense(Array(0.0, 0.0, 0.0, 0.0, 1.0)))\n",
      "        assert(pred === 4.0)\n",
      "    }\n",
      "\n",
      "    // force the model to be trained with only one class\n",
      "    val constantZeroData = Seq(\n",
      "      LabeledPoint(0.0, Vectors.dense(0.0)),\n",
      "      LabeledPoint(0.0, Vectors.dense(1.0)),\n",
      "      LabeledPoint(0.0, Vectors.dense(2.0))).toDF()\n",
      "    val modelZeroLabel = mlr.setFitIntercept(false).fit(constantZeroData)\n",
      "    val resultsZero = modelZeroLabel.transform(constantZeroData)\n",
      "    resultsZero.select(\"rawPrediction\", \"probability\", \"prediction\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector, pred: Double) =>\n",
      "        assert(prob === Vectors.dense(Array(1.0)))\n",
      "        assert(pred === 0.0)\n",
      "    }\n",
      "\n",
      "    // ensure that the correct value is predicted when numClasses passed through metadata\n",
      "    val labelMeta = NominalAttribute.defaultAttr.withName(\"label\").withNumValues(6).toMetadata()\n",
      "    val constantDataWithMetadata = constantData\n",
      "      .select(constantData(\"label\").as(\"label\", labelMeta), constantData(\"features\"))\n",
      "    val modelWithMetadata = mlr.setFitIntercept(true).fit(constantDataWithMetadata)\n",
      "    val resultsWithMetadata = modelWithMetadata.transform(constantDataWithMetadata)\n",
      "    resultsWithMetadata.select(\"rawPrediction\", \"probability\", \"prediction\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector, pred: Double) =>\n",
      "        assert(raw === Vectors.dense(Array(0.0, 0.0, 0.0, 0.0, Double.PositiveInfinity, 0.0)))\n",
      "        assert(prob === Vectors.dense(Array(0.0, 0.0, 0.0, 0.0, 1.0, 0.0)))\n",
      "        assert(pred === 4.0)\n",
      "    }\n",
      "    // TODO: check num iters is zero when it become available in the model\n",
      "  }\n",
      "\n",
      "  test(\"compressed storage\") {\n",
      "    val spark = SparkSession.builder().getOrCreate()\n",
      "    import spark.implicits._\n",
      "\n",
      "    val moreClassesThanFeatures = Seq(\n",
      "      LabeledPoint(4.0, Vectors.dense(0.0, 0.0, 0.0)),\n",
      "      LabeledPoint(4.0, Vectors.dense(1.0, 1.0, 1.0)),\n",
      "      LabeledPoint(4.0, Vectors.dense(2.0, 2.0, 2.0))).toDF()\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    val model = mlr.fit(moreClassesThanFeatures)\n",
      "    assert(model.coefficientMatrix.isInstanceOf[SparseMatrix])\n",
      "    assert(model.coefficientMatrix.asInstanceOf[SparseMatrix].colPtrs.length === 4)\n",
      "    val moreFeaturesThanClasses = Seq(\n",
      "      LabeledPoint(1.0, Vectors.dense(0.0, 0.0, 0.0)),\n",
      "      LabeledPoint(1.0, Vectors.dense(1.0, 1.0, 1.0)),\n",
      "      LabeledPoint(1.0, Vectors.dense(2.0, 2.0, 2.0))).toDF()\n",
      "    val model2 = mlr.fit(moreFeaturesThanClasses)\n",
      "    assert(model2.coefficientMatrix.isInstanceOf[SparseMatrix])\n",
      "    assert(model2.coefficientMatrix.asInstanceOf[SparseMatrix].colPtrs.length === 3)\n",
      "\n",
      "    val blr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    val blrModel = blr.fit(moreFeaturesThanClasses)\n",
      "    assert(blrModel.coefficientMatrix.isInstanceOf[SparseMatrix])\n",
      "    assert(blrModel.coefficientMatrix.asInstanceOf[SparseMatrix].colPtrs.length === 2)\n",
      "  }\n",
      "\n",
      "  test(\"numClasses specified in metadata/inferred\") {\n",
      "    val lr = new LogisticRegression().setMaxIter(1).setFamily(\"multinomial\")\n",
      "\n",
      "    // specify more classes than unique label values\n",
      "    val labelMeta = NominalAttribute.defaultAttr.withName(\"label\").withNumValues(4).toMetadata()\n",
      "    val df = smallMultinomialDataset.select(smallMultinomialDataset(\"label\").as(\"label\", labelMeta),\n",
      "      smallMultinomialDataset(\"features\"))\n",
      "    val model1 = lr.fit(df)\n",
      "    assert(model1.numClasses === 4)\n",
      "    assert(model1.interceptVector.size === 4)\n",
      "\n",
      "    // specify two classes when there are really three\n",
      "    val labelMeta1 = NominalAttribute.defaultAttr.withName(\"label\").withNumValues(2).toMetadata()\n",
      "    val df1 = smallMultinomialDataset\n",
      "      .select(smallMultinomialDataset(\"label\").as(\"label\", labelMeta1),\n",
      "        smallMultinomialDataset(\"features\"))\n",
      "    val thrown = intercept[IllegalArgumentException] {\n",
      "      lr.fit(df1)\n",
      "    }\n",
      "    assert(thrown.getMessage.contains(\"less than the number of unique labels\"))\n",
      "\n",
      "    // lr should infer the number of classes if not specified\n",
      "    val model3 = lr.fit(smallMultinomialDataset)\n",
      "    assert(model3.numClasses === 3)\n",
      "  }\n",
      "\n",
      "  test(\"should support all NumericType labels and not support other types\") {\n",
      "    val lr = new LogisticRegression().setMaxIter(1)\n",
      "    MLTestingUtils.checkNumericTypes[LogisticRegressionModel, LogisticRegression](\n",
      "      lr, spark) { (expected, actual) =>\n",
      "        assert(expected.intercept === actual.intercept)\n",
      "        assert(expected.coefficients.toArray === actual.coefficients.toArray)\n",
      "      }\n",
      "  }\n",
      "}\n",
      "\n",
      "object LogisticRegressionSuite {\n",
      "\n",
      "  /**\n",
      "   * Mapping from all Params to valid settings which differ from the defaults.\n",
      "   * This is useful for tests which need to exercise all Params, such as save/load.\n",
      "   * This excludes input columns to simplify some tests.\n",
      "   */\n",
      "  val allParamSettings: Map[String, Any] = ProbabilisticClassifierSuite.allParamSettings ++ Map(\n",
      "    \"probabilityCol\" -> \"myProbability\",\n",
      "    \"thresholds\" -> Array(0.4, 0.6),\n",
      "    \"regParam\" -> 0.01,\n",
      "    \"elasticNetParam\" -> 0.1,\n",
      "    \"maxIter\" -> 2,  // intentionally small\n",
      "    \"fitIntercept\" -> true,\n",
      "    \"tol\" -> 0.8,\n",
      "    \"standardization\" -> false,\n",
      "    \"threshold\" -> 0.6\n",
      "  )\n",
      "\n",
      "  def generateLogisticInputAsList(\n",
      "    offset: Double,\n",
      "    scale: Double,\n",
      "    nPoints: Int,\n",
      "    seed: Int): java.util.List[LabeledPoint] = {\n",
      "    generateLogisticInput(offset, scale, nPoints, seed).asJava\n",
      "  }\n",
      "\n",
      "  // Generate input of the form Y = logistic(offset + scale*X)\n",
      "  def generateLogisticInput(\n",
      "      offset: Double,\n",
      "      scale: Double,\n",
      "      nPoints: Int,\n",
      "      seed: Int): Seq[LabeledPoint] = {\n",
      "    val rnd = new Random(seed)\n",
      "    val x1 = Array.fill[Double](nPoints)(rnd.nextGaussian())\n",
      "\n",
      "    val y = (0 until nPoints).map { i =>\n",
      "      val p = 1.0 / (1.0 + math.exp(-(offset + scale * x1(i))))\n",
      "      if (rnd.nextDouble() < p) 1.0 else 0.0\n",
      "    }\n",
      "\n",
      "    val testData = (0 until nPoints).map(i => LabeledPoint(y(i), Vectors.dense(Array(x1(i)))))\n",
      "    testData\n",
      "  }\n",
      "\n",
      "  /**\n",
      "   * Generates `k` classes multinomial synthetic logistic input in `n` dimensional space given the\n",
      "   * model weights and mean/variance of the features. The synthetic data will be drawn from\n",
      "   * the probability distribution constructed by weights using the following formula.\n",
      "   *\n",
      "   * P(y = 0 | x) = 1 / norm\n",
      "   * P(y = 1 | x) = exp(x * w_1) / norm\n",
      "   * P(y = 2 | x) = exp(x * w_2) / norm\n",
      "   * ...\n",
      "   * P(y = k-1 | x) = exp(x * w_{k-1}) / norm\n",
      "   * where norm = 1 + exp(x * w_1) + exp(x * w_2) + ... + exp(x * w_{k-1})\n",
      "   *\n",
      "   * @param weights matrix is flatten into a vector; as a result, the dimension of weights vector\n",
      "   *                will be (k - 1) * (n + 1) if `addIntercept == true`, and\n",
      "   *                if `addIntercept != true`, the dimension will be (k - 1) * n.\n",
      "   * @param xMean the mean of the generated features. Lots of time, if the features are not properly\n",
      "   *              standardized, the algorithm with poor implementation will have difficulty\n",
      "   *              to converge.\n",
      "   * @param xVariance the variance of the generated features.\n",
      "   * @param addIntercept whether to add intercept.\n",
      "   * @param nPoints the number of instance of generated data.\n",
      "   * @param seed the seed for random generator. For consistent testing result, it will be fixed.\n",
      "   */\n",
      "  def generateMultinomialLogisticInput(\n",
      "      weights: Array[Double],\n",
      "      xMean: Array[Double],\n",
      "      xVariance: Array[Double],\n",
      "      addIntercept: Boolean,\n",
      "      nPoints: Int,\n",
      "      seed: Int): Seq[LabeledPoint] = {\n",
      "    val rnd = new Random(seed)\n",
      "\n",
      "    val xDim = xMean.length\n",
      "    val xWithInterceptsDim = if (addIntercept) xDim + 1 else xDim\n",
      "    val nClasses = weights.length / xWithInterceptsDim + 1\n",
      "\n",
      "    val x = Array.fill[Vector](nPoints)(Vectors.dense(Array.fill[Double](xDim)(rnd.nextGaussian())))\n",
      "\n",
      "    x.foreach { vector =>\n",
      "      // This doesn't work if `vector` is a sparse vector.\n",
      "      val vectorArray = vector.toArray\n",
      "      var i = 0\n",
      "      val len = vectorArray.length\n",
      "      while (i < len) {\n",
      "        vectorArray(i) = vectorArray(i) * math.sqrt(xVariance(i)) + xMean(i)\n",
      "        i += 1\n",
      "      }\n",
      "    }\n",
      "\n",
      "    val y = (0 until nPoints).map { idx =>\n",
      "      val xArray = x(idx).toArray\n",
      "      val margins = Array.ofDim[Double](nClasses)\n",
      "      val probs = Array.ofDim[Double](nClasses)\n",
      "\n",
      "      for (i <- 0 until nClasses - 1) {\n",
      "        for (j <- 0 until xDim) margins(i + 1) += weights(i * xWithInterceptsDim + j) * xArray(j)\n",
      "        if (addIntercept) margins(i + 1) += weights((i + 1) * xWithInterceptsDim - 1)\n",
      "      }\n",
      "      // Preventing the overflow when we compute the probability\n",
      "      val maxMargin = margins.max\n",
      "      if (maxMargin > 0) for (i <- 0 until nClasses) margins(i) -= maxMargin\n",
      "\n",
      "      // Computing the probabilities for each class from the margins.\n",
      "      val norm = {\n",
      "        var temp = 0.0\n",
      "        for (i <- 0 until nClasses) {\n",
      "          probs(i) = math.exp(margins(i))\n",
      "          temp += probs(i)\n",
      "        }\n",
      "        temp\n",
      "      }\n",
      "      for (i <- 0 until nClasses) probs(i) /= norm\n",
      "\n",
      "      // Compute the cumulative probability so we can generate a random number and assign a label.\n",
      "      for (i <- 1 until nClasses) probs(i) += probs(i - 1)\n",
      "      val p = rnd.nextDouble()\n",
      "      var y = 0\n",
      "      breakable {\n",
      "        for (i <- 0 until nClasses) {\n",
      "          if (p < probs(i)) {\n",
      "            y = i\n",
      "            break\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      y\n",
      "    }\n",
      "\n",
      "    val testData = (0 until nPoints).map(i => LabeledPoint(y(i), x(i)))\n",
      "    testData\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 6:\n",
      "<gh_stars>0\n",
      "/*\n",
      " * Copyright 2016-2021 47 Degrees Open Source <https://www.47deg.com>\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not use this file except in compliance with the License.\n",
      " * You may obtain a copy of the License at\n",
      " *\n",
      " *     http://www.apache.org/licenses/LICENSE-2.0\n",
      " *\n",
      " * Unless required by applicable law or agreed to in writing, software\n",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      " * See the License for the specific language governing permissions and\n",
      " * limitations under the License.\n",
      " */\n",
      "\n",
      "package github4s\n",
      "\n",
      "import cats.data.NonEmptyList\n",
      "import cats.syntax.all._\n",
      "import github4s.domain._\n",
      "import io.circe.Decoder.Result\n",
      "import io.circe._\n",
      "import io.circe.generic.semiauto.deriveDecoder\n",
      "\n",
      "/**\n",
      " * Implicit circe decoders of domains objects\n",
      " */\n",
      "object Decoders {\n",
      "  final case class Author(\n",
      "      login: Option[String],\n",
      "      avatar_url: Option[String],\n",
      "      html_url: Option[String]\n",
      "  )\n",
      "\n",
      "  implicit val decodeAuthor: Decoder[Author] = deriveDecoder[Author]\n",
      "\n",
      "  implicit val decodeCommit: Decoder[Commit] = Decoder.instance { c =>\n",
      "    for {\n",
      "      sha     <- c.downField(\"sha\").as[String]\n",
      "      message <- c.downField(\"commit\").downField(\"message\").as[String]\n",
      "      date    <- c.downField(\"commit\").downField(\"author\").downField(\"date\").as[String]\n",
      "      url     <- c.downField(\"html_url\").as[String]\n",
      "      author  <- c.downField(\"author\").as[Option[Author]]\n",
      "    } yield Commit(\n",
      "      sha = sha,\n",
      "      message = message,\n",
      "      date = date,\n",
      "      url = url,\n",
      "      login = author.flatMap(_.login),\n",
      "      avatar_url = author.flatMap(_.avatar_url),\n",
      "      author_url = author.flatMap(_.html_url)\n",
      "    )\n",
      "  }\n",
      "\n",
      "  implicit val decodeBranch: Decoder[Branch] = Decoder.instance { c =>\n",
      "    for {\n",
      "      name            <- c.downField(\"name\").as[String]\n",
      "      commit          <- c.downField(\"commit\").as[BranchCommit]\n",
      "      branchProtected <- c.downField(\"protected\").as[Option[Boolean]]\n",
      "      protection_url  <- c.downField(\"protection_url\").as[Option[String]]\n",
      "    } yield Branch(\n",
      "      name = name,\n",
      "      commit = commit,\n",
      "      `protected` = branchProtected,\n",
      "      protection_url = protection_url\n",
      "    )\n",
      "  }\n",
      "\n",
      "  implicit val decodeBranchCommit: Decoder[BranchCommit] = Decoder.instance { c =>\n",
      "    for {\n",
      "      url <- c.downField(\"url\").as[String]\n",
      "      sha <- c.downField(\"sha\").as[String]\n",
      "    } yield BranchCommit(\n",
      "      url = url,\n",
      "      sha = sha\n",
      "    )\n",
      "  }\n",
      "\n",
      "  def readRepoUrls(c: HCursor): Either[DecodingFailure, Map[String, String]] =\n",
      "    RepoUrlKeys.allFields\n",
      "      .traverse(name => c.downField(name).as[Option[String]].map(_.map(value => name -> value)))\n",
      "      .map(_.flatten.toMap)\n",
      "\n",
      "  implicit val decodeStatusRepository: Decoder[StatusRepository] = {\n",
      "    Decoder.instance { c =>\n",
      "      for {\n",
      "        id          <- c.downField(\"id\").as[Long]\n",
      "        name        <- c.downField(\"name\").as[String]\n",
      "        full_name   <- c.downField(\"full_name\").as[String]\n",
      "        owner       <- c.downField(\"owner\").as[Option[User]]\n",
      "        priv        <- c.downField(\"private\").as[Boolean]\n",
      "        description <- c.downField(\"description\").as[Option[String]]\n",
      "        fork        <- c.downField(\"fork\").as[Boolean]\n",
      "        repoUrls    <- readRepoUrls(c)\n",
      "      } yield StatusRepository(\n",
      "        id = id,\n",
      "        name = name,\n",
      "        full_name = full_name,\n",
      "        owner = owner,\n",
      "        `private` = priv,\n",
      "        description = description,\n",
      "        fork = fork,\n",
      "        urls = repoUrls\n",
      "      )\n",
      "    }\n",
      "  }\n",
      "\n",
      "  implicit val decodeRepositoryBase: Decoder[RepositoryBase] = {\n",
      "\n",
      "    Decoder.instance { c =>\n",
      "      for {\n",
      "        id                <- c.downField(\"id\").as[Long]\n",
      "        name              <- c.downField(\"name\").as[String]\n",
      "        full_name         <- c.downField(\"full_name\").as[String]\n",
      "        owner             <- c.downField(\"owner\").as[User]\n",
      "        priv              <- c.downField(\"private\").as[Boolean]\n",
      "        description       <- c.downField(\"description\").as[Option[String]]\n",
      "        fork              <- c.downField(\"fork\").as[Boolean]\n",
      "        archived          <- c.downField(\"archived\").as[Boolean]\n",
      "        created_at        <- c.downField(\"created_at\").as[String]\n",
      "        updated_at        <- c.downField(\"updated_at\").as[String]\n",
      "        pushed_at         <- c.downField(\"pushed_at\").as[String]\n",
      "        homepage          <- c.downField(\"homepage\").as[Option[String]]\n",
      "        language          <- c.downField(\"language\").as[Option[String]]\n",
      "        organization      <- c.downField(\"organization\").as[Option[User]]\n",
      "        size              <- c.downField(\"size\").as[Int]\n",
      "        stargazers_count  <- c.downField(\"stargazers_count\").as[Int]\n",
      "        watchers_count    <- c.downField(\"watchers_count\").as[Int]\n",
      "        forks_count       <- c.downField(\"forks_count\").as[Int]\n",
      "        open_issues_count <- c.downField(\"open_issues_count\").as[Int]\n",
      "        open_issues       <- c.downField(\"open_issues\").as[Option[Int]]\n",
      "        watchers          <- c.downField(\"watchers\").as[Option[Int]]\n",
      "        network_count     <- c.downField(\"network_count\").as[Option[Int]]\n",
      "        subscribers_count <- c.downField(\"subscribers_count\").as[Option[Int]]\n",
      "        has_issues        <- c.downField(\"has_issues\").as[Boolean]\n",
      "        has_downloads     <- c.downField(\"has_downloads\").as[Boolean]\n",
      "        has_wiki          <- c.downField(\"has_wiki\").as[Boolean]\n",
      "        has_pages         <- c.downField(\"has_pages\").as[Boolean]\n",
      "        url               <- c.downField(\"url\").as[String]\n",
      "        html_url          <- c.downField(\"html_url\").as[String]\n",
      "        git_url           <- c.downField(\"git_url\").as[String]\n",
      "        ssh_url           <- c.downField(\"ssh_url\").as[String]\n",
      "        clone_url         <- c.downField(\"clone_url\").as[String]\n",
      "        svn_url           <- c.downField(\"svn_url\").as[String]\n",
      "        permissions       <- c.downField(\"permissions\").as[Option[RepoPermissions]]\n",
      "        repoUrls          <- readRepoUrls(c)\n",
      "      } yield RepositoryBase(\n",
      "        id = id,\n",
      "        name = name,\n",
      "        full_name = full_name,\n",
      "        owner = owner,\n",
      "        `private` = priv,\n",
      "        description = description,\n",
      "        fork = fork,\n",
      "        archived = archived,\n",
      "        created_at = created_at,\n",
      "        updated_at = updated_at,\n",
      "        pushed_at = pushed_at,\n",
      "        homepage = homepage,\n",
      "        language = language,\n",
      "        organization = organization,\n",
      "        permissions = permissions,\n",
      "        status = RepoStatus(\n",
      "          size = size,\n",
      "          stargazers_count = stargazers_count,\n",
      "          watchers_count = watchers_count,\n",
      "          forks_count = forks_count,\n",
      "          open_issues_count = open_issues_count,\n",
      "          open_issues = open_issues,\n",
      "          watchers = watchers,\n",
      "          network_count = network_count,\n",
      "          subscribers_count = subscribers_count,\n",
      "          has_issues = has_issues,\n",
      "          has_downloads = has_downloads,\n",
      "          has_wiki = has_wiki,\n",
      "          has_pages = has_pages\n",
      "        ),\n",
      "        urls = RepoUrls(\n",
      "          url = url,\n",
      "          html_url = html_url,\n",
      "          git_url = git_url,\n",
      "          ssh_url = ssh_url,\n",
      "          clone_url = clone_url,\n",
      "          svn_url = svn_url,\n",
      "          otherUrls = repoUrls\n",
      "        )\n",
      "      )\n",
      "    }\n",
      "  }\n",
      "\n",
      "  implicit val decodeRepository: Decoder[Repository] = for {\n",
      "    base   <- decodeRepositoryBase\n",
      "    parent <- Decoder[Option[RepositoryBase]].at(\"parent\")\n",
      "    source <- Decoder[Option[RepositoryBase]].at(\"source\")\n",
      "  } yield Repository.fromBaseRepos(base, parent, source)\n",
      "\n",
      "  implicit val decodePRStatus: Decoder[PullRequestReviewState] =\n",
      "    Decoder.decodeString.emap {\n",
      "      case PRRStateApproved.value         => PRRStateApproved.asRight\n",
      "      case PRRStateChangesRequested.value => PRRStateChangesRequested.asRight\n",
      "      case PRRStateCommented.value        => PRRStateCommented.asRight\n",
      "      case PRRStatePending.value          => PRRStatePending.asRight\n",
      "      case PRRStateDismissed.value        => PRRStateDismissed.asRight\n",
      "      case other                          => s\"Unknown pull request review state: $other\".asLeft\n",
      "    }\n",
      "\n",
      "  implicit val decodeGistFile: Decoder[GistFile] = Decoder.instance { c =>\n",
      "    for {\n",
      "      content <- c.downField(\"content\").as[String]\n",
      "    } yield GistFile(\n",
      "      content = content\n",
      "    )\n",
      "  }\n",
      "\n",
      "  implicit val decodeGist: Decoder[Gist] = Decoder.instance { c =>\n",
      "    for {\n",
      "      url         <- c.downField(\"url\").as[String]\n",
      "      id          <- c.downField(\"id\").as[String]\n",
      "      description <- c.downField(\"description\").as[String]\n",
      "      public      <- c.downField(\"public\").as[Boolean]\n",
      "      files       <- c.downField(\"files\").as[Map[String, GistFile]]\n",
      "    } yield Gist(\n",
      "      url = url,\n",
      "      id = id,\n",
      "      description = description,\n",
      "      public = public,\n",
      "      files = files\n",
      "    )\n",
      "  }\n",
      "\n",
      "  implicit val decodeStarredRepository: Decoder[StarredRepository] =\n",
      "    Decoder[Repository]\n",
      "      .map(StarredRepository(_))\n",
      "      .or(\n",
      "        Decoder.instance(c =>\n",
      "          for {\n",
      "            starred_at <- c.downField(\"starred_at\").as[String]\n",
      "            repo       <- c.downField(\"repo\").as[Repository]\n",
      "          } yield StarredRepository(repo, Some(starred_at))\n",
      "        )\n",
      "      )\n",
      "\n",
      "  implicit val decoderCreatePullRequestData: Decoder[CreatePullRequestData] =\n",
      "    deriveDecoder[CreatePullRequestData]\n",
      "  implicit val decoderCreatePullRequestIssue: Decoder[CreatePullRequestIssue] =\n",
      "    deriveDecoder[CreatePullRequestIssue]\n",
      "  implicit val decoderNewBlobRequest: Decoder[NewBlobRequest]   = deriveDecoder[NewBlobRequest]\n",
      "  implicit val decoderNewGistRequest: Decoder[NewGistRequest]   = deriveDecoder[NewGistRequest]\n",
      "  implicit val decoderNewIssueRequest: Decoder[NewIssueRequest] = deriveDecoder[NewIssueRequest]\n",
      "  implicit val decoderNewReleaseRequest: Decoder[NewReleaseRequest] =\n",
      "    deriveDecoder[NewReleaseRequest]\n",
      "  implicit val decoderSubscriptionRequest: Decoder[SubscriptionRequest] =\n",
      "    deriveDecoder[SubscriptionRequest]\n",
      "  implicit val decoderTreeData: Decoder[TreeData] = {\n",
      "    val sha  = deriveDecoder[TreeDataSha]\n",
      "    val blob = deriveDecoder[TreeDataBlob]\n",
      "    sha.widen[TreeData] or blob.widen[TreeData]\n",
      "  }\n",
      "  implicit val decoderUpdateReferenceRequest: Decoder[UpdateReferenceRequest] =\n",
      "    deriveDecoder[UpdateReferenceRequest]\n",
      "  implicit val decoderWriteFileRequest: Decoder[WriteFileRequest] = deriveDecoder[WriteFileRequest]\n",
      "  implicit val decoderReviewersRequest: Decoder[ReviewersRequest] = deriveDecoder[ReviewersRequest]\n",
      "  implicit val decoderNewStatusRequest: Decoder[NewStatusRequest] = deriveDecoder[NewStatusRequest]\n",
      "  implicit val decoderNewTagRequest: Decoder[NewTagRequest]       = deriveDecoder[NewTagRequest]\n",
      "  implicit val decoderNewTreeRequest: Decoder[NewTreeRequest]     = deriveDecoder[NewTreeRequest]\n",
      "  implicit val decoderNewCommitRequest: Decoder[NewCommitRequest] = deriveDecoder[NewCommitRequest]\n",
      "\n",
      "  implicit def decodeNonEmptyList[T](implicit D: Decoder[T]): Decoder[NonEmptyList[T]] = {\n",
      "\n",
      "    def decodeCursors(cursors: List[HCursor]): Result[NonEmptyList[T]] =\n",
      "      cursors.toNel\n",
      "        .toRight(DecodingFailure(\"Empty Response\", Nil))\n",
      "        .flatMap(nelCursors => nelCursors.traverse(_.as[T]))\n",
      "\n",
      "    Decoder.instance { c =>\n",
      "      c.as[T] match {\n",
      "        case Right(r) => Right(NonEmptyList(r, Nil))\n",
      "        case Left(_)  => c.as[List[HCursor]] flatMap decodeCursors\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  implicit val decoderCommitter: Decoder[Committer] = deriveDecoder[Committer]\n",
      "  implicit val decoderWriteResponseCommit: Decoder[WriteResponseCommit] =\n",
      "    deriveDecoder[WriteResponseCommit]\n",
      "  implicit val decoderWriteFileResponse: Decoder[WriteFileResponse] =\n",
      "    deriveDecoder[WriteFileResponse]\n",
      "  implicit val decoderPullRequestFile: Decoder[PullRequestFile] = deriveDecoder[PullRequestFile]\n",
      "  implicit val decoderPullRequestReview: Decoder[PullRequestReview] =\n",
      "    deriveDecoder[PullRequestReview]\n",
      "  implicit val decoderUser: Decoder[User] = deriveDecoder[User]\n",
      "\n",
      "  implicit val decoderRepoPermissions: Decoder[RepoPermissions] = deriveDecoder[RepoPermissions]\n",
      "  implicit val decoderPullRequestBase: Decoder[PullRequestBase] = deriveDecoder[PullRequestBase]\n",
      "\n",
      "  implicit val decoderPullRequest: Decoder[PullRequest]           = deriveDecoder[PullRequest]\n",
      "  implicit val decoderRefObject: Decoder[RefObject]               = deriveDecoder[RefObject]\n",
      "  implicit val decoderRef: Decoder[Ref]                           = deriveDecoder[Ref]\n",
      "  implicit val decoderRefAuthor: Decoder[RefAuthor]               = deriveDecoder[RefAuthor]\n",
      "  implicit val decoderRefCommit: Decoder[RefCommit]               = deriveDecoder[RefCommit]\n",
      "  implicit val decoderRefInfo: Decoder[RefInfo]                   = deriveDecoder[RefInfo]\n",
      "  implicit val decoderTreeDataResult: Decoder[TreeDataResult]     = deriveDecoder[TreeDataResult]\n",
      "  implicit val decoderTreeResult: Decoder[TreeResult]             = deriveDecoder[TreeResult]\n",
      "  implicit val decoderTag: Decoder[Tag]                           = deriveDecoder[Tag]\n",
      "  implicit val decoderLabel: Decoder[Label]                       = deriveDecoder[Label]\n",
      "  implicit val decoderIssuePullRequest: Decoder[IssuePullRequest] = deriveDecoder[IssuePullRequest]\n",
      "  implicit val decoderIssue: Decoder[Issue]                       = deriveDecoder[Issue]\n",
      "  implicit val decoderSearchIssuesResult: Decoder[SearchIssuesResult] =\n",
      "    deriveDecoder[SearchIssuesResult]\n",
      "  implicit val decoderSearchReposResult: Decoder[SearchReposResult] = deriveDecoder\n",
      "  implicit val decoderComment: Decoder[Comment]                     = deriveDecoder[Comment]\n",
      "  implicit val decoderStatus: Decoder[Status]                       = deriveDecoder[Status]\n",
      "  implicit val decoderCombinedStatus: Decoder[CombinedStatus]       = deriveDecoder[CombinedStatus]\n",
      "  implicit val decoderContent: Decoder[Content]                     = deriveDecoder[Content]\n",
      "  implicit val decoderBlobContent: Decoder[BlobContent]             = deriveDecoder[BlobContent]\n",
      "  implicit val decoderSubscription: Decoder[Subscription]           = deriveDecoder[Subscription]\n",
      "  implicit val decoderOAuthToken: Decoder[OAuthToken]               = deriveDecoder[OAuthToken]\n",
      "  implicit val decoderRelease: Decoder[Release]                     = deriveDecoder[Release]\n",
      "  implicit val decoderUserRepoPermission: Decoder[UserRepoPermission] =\n",
      "    deriveDecoder[UserRepoPermission]\n",
      "\n",
      "  implicit val decodeStargazer: Decoder[Stargazer] =\n",
      "    decoderUser\n",
      "      .map(Stargazer(_))\n",
      "      .or(\n",
      "        Decoder.instance(c =>\n",
      "          for {\n",
      "            starred_at <- c.downField(\"starred_at\").as[String]\n",
      "            user       <- c.downField(\"user\").as[User]\n",
      "          } yield Stargazer(user, Some(starred_at))\n",
      "        )\n",
      "      )\n",
      "\n",
      "  implicit val decodeTeam: Decoder[Team]           = deriveDecoder[Team]\n",
      "  implicit val decodeCreator: Decoder[Creator]     = deriveDecoder[Creator]\n",
      "  implicit val decodeMilestone: Decoder[Milestone] = deriveDecoder[Milestone]\n",
      "  implicit val decodeProject: Decoder[Project]     = deriveDecoder[Project]\n",
      "  implicit val decodeColumn: Decoder[Column]       = deriveDecoder[Column]\n",
      "  implicit val decodeCard: Decoder[Card]           = deriveDecoder[Card]\n",
      "\n",
      "  implicit val decodeReviewers: Decoder[ReviewersResponse] =\n",
      "    deriveDecoder[ReviewersResponse]\n",
      "  implicit val decoderCommentData: Decoder[CommentData] = deriveDecoder[CommentData]\n",
      "  implicit val decoderPullRequestReviewEvent: Decoder[PullRequestReviewEvent] =\n",
      "    Decoder[String].emap {\n",
      "      case s if s == PRREventApprove.value        => Right(PRREventApprove)\n",
      "      case s if s == PRREventRequestChanges.value => Right(PRREventRequestChanges)\n",
      "      case s if s == PRREventComment.value        => Right(PRREventComment)\n",
      "      case s if s == PRREventPending.value        => Right(PRREventPending)\n",
      "      case other                                  => Left(s\"Bad event: $other\")\n",
      "    }\n",
      "  implicit val decoderCreateReviewComment: Decoder[CreateReviewComment] =\n",
      "    deriveDecoder[CreateReviewComment]\n",
      "  implicit val decoderCreatePRReviewRequest: Decoder[CreatePRReviewRequest] =\n",
      "    deriveDecoder[CreatePRReviewRequest]\n",
      "  implicit val decoderCreatePullRequest: Decoder[CreatePullRequest] = {\n",
      "    val data  = deriveDecoder[CreatePullRequestData]\n",
      "    val issue = deriveDecoder[CreatePullRequestIssue]\n",
      "    data.widen[CreatePullRequest] or issue.widen[CreatePullRequest]\n",
      "  }\n",
      "  implicit val decoderCreateReferenceRequest: Decoder[CreateReferenceRequest] =\n",
      "    deriveDecoder[CreateReferenceRequest]\n",
      "  implicit val decoderDeleteFileRequest: Decoder[DeleteFileRequest] =\n",
      "    deriveDecoder[DeleteFileRequest]\n",
      "  implicit val decoderEditGistFile: Decoder[EditGistFile]         = deriveDecoder[EditGistFile]\n",
      "  implicit val decoderEditGistRequest: Decoder[EditGistRequest]   = deriveDecoder[EditGistRequest]\n",
      "  implicit val decoderEditIssueRequest: Decoder[EditIssueRequest] = deriveDecoder[EditIssueRequest]\n",
      "  implicit val decoderMilestoneData: Decoder[MilestoneData]       = deriveDecoder[MilestoneData]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 7:\n",
      "import dotty.tools.sbtplugin.DottyPlugin.autoImport._\n",
      "import sbt._\n",
      "import Keys._\n",
      "import com.typesafe.tools.mima.plugin.MimaKeys.{mimaPreviousArtifacts, mimaCurrentClassfiles, mimaBinaryIssueFilters}\n",
      "import com.typesafe.tools.mima.core._\n",
      "import com.typesafe.tools.mima.core.ProblemFilters._\n",
      "import com.typesafe.sbt.osgi.OsgiKeys\n",
      "import com.typesafe.sbt.osgi.SbtOsgi\n",
      "import com.typesafe.sbt.osgi.SbtOsgi.autoImport._\n",
      "import org.scalajs.sbtplugin.ScalaJSPlugin\n",
      "import org.scalajs.sbtplugin.ScalaJSPlugin.autoImport.{scalaJSLinkerConfig, jsEnv}\n",
      "\n",
      "trait DottyBuild { this: BuildCommons =>\n",
      "\n",
      "  // List of available night build at https://repo1.maven.org/maven2/ch/epfl/lamp/dotty-compiler_0.27/\n",
      "  // lazy val dottyVersion = dottyLatestNightlyBuild.get\n",
      "  lazy val dottyVersion = System.getProperty(\"scalatest.dottyVersion\", \"3.0.0-RC2\")\n",
      "  lazy val dottySettings = List(\n",
      "    scalaVersion := dottyVersion,\n",
      "    scalacOptions ++= List(\"-language:implicitConversions\", \"-noindent\", \"-Xprint-suspension\")\n",
      "  )\n",
      "\n",
      "  // https://github.com/sbt/sbt/issues/2205#issuecomment-144375501\n",
      "  private lazy val packageManagedSources =\n",
      "    mappings in (Compile, packageSrc) ++= { // publish generated sources\n",
      "      val srcs = (managedSources in Compile).value\n",
      "      val sdirs = (managedSourceDirectories in Compile).value\n",
      "      val base = baseDirectory.value\n",
      "      import Path._\n",
      "      (srcs --- sdirs --- base) pair (relativeTo(sdirs) | relativeTo(base) | flat)\n",
      "    }\n",
      "\n",
      "  lazy val scalacticDotty = project.in(file(\"dotty/scalactic\"))\n",
      "    .enablePlugins(SbtOsgi)\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(scalacticDocSettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Scalactic\",\n",
      "      organization := \"org.scalactic\",\n",
      "      moduleName := \"scalactic\",\n",
      "      initialCommands in console := \"import org.scalactic._\",\n",
      "      packageManagedSources,\n",
      "      sourceGenerators in Compile += {\n",
      "        Def.task {\n",
      "          // From scalactic-macro\n",
      "          GenScalacticDotty.genMacroScala((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "          ScalacticGenResourcesJVM.genResources((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenAnyVals.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\" / \"anyvals\", version.value, scalaVersion.value, true) ++\n",
      "          GenEvery.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenColCompatHelper.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          // end from scalactic-macro\n",
      "          GenScalacticDotty.genScala((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "          GenVersions.genScalacticVersions((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          ScalacticGenResourcesJVM.genFailureMessages((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenArrayHelper.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value)\n",
      "        }.taskValue\n",
      "      },\n",
      "      resourceGenerators in Compile += Def.task {\n",
      "        GenScalacticDotty.genResource((resourceManaged in Compile).value)\n",
      "      }.taskValue,\n",
      "      //scalacticDocSourcesSetting,\n",
      "      //docTaskSetting,\n",
      "      publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "      mimaPreviousArtifacts := Set(organization.value %% name.value % previousReleaseVersion),\n",
      "      mimaCurrentClassfiles := (classDirectory in Compile).value.getParentFile / (name.value + \"_\" + scalaBinaryVersion.value + \"-\" + releaseVersion + \".jar\")\n",
      "    ).settings(osgiSettings: _*).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\n",
      "      \"org.scalactic\",\n",
      "      \"org.scalactic.anyvals\",\n",
      "      \"org.scalactic.exceptions\",\n",
      "      \"org.scalactic.source\"\n",
      "    ),\n",
      "    OsgiKeys.importPackage := Seq(\n",
      "      \"org.scalatest.*\",\n",
      "      \"org.scalactic.*\",\n",
      "      \"scala.util.parsing.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.xml.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.*;version=\\\"$<range;[==,=+);$<replace;\"+scalaBinaryVersion.value+\";-;.>>\\\"\",\n",
      "      \"*;resolution:=optional\"\n",
      "    ),\n",
      "    OsgiKeys.additionalHeaders:= Map(\n",
      "      \"Bundle-Name\" -> \"Scalactic\",\n",
      "      \"Bundle-Description\" -> \"Scalactic is an open-source library for Scala projects.\",\n",
      "      \"Bundle-DocURL\" -> \"http://www.scalactic.org/\",\n",
      "      \"Bundle-Vendor\" -> \"Artima, Inc.\"\n",
      "    )\n",
      "  )\n",
      "\n",
      "  lazy val scalacticDottyJS = project.in(file(\"dotty/scalactic.js\"))\n",
      "    .enablePlugins(SbtOsgi)\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(scalacticDocSettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Scalactic\",\n",
      "      organization := \"org.scalactic\",\n",
      "      moduleName := \"scalactic\",\n",
      "      initialCommands in console := \"import org.scalactic._\",\n",
      "      packageManagedSources,\n",
      "      sourceGenerators in Compile += {\n",
      "        Def.task {\n",
      "          // From scalactic-macro\n",
      "          GenScalacticDotty.genMacroScala((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "          ScalacticGenResourcesJSVM.genResources((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenAnyVals.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\" / \"anyvals\", version.value, scalaVersion.value, true) ++\n",
      "          GenEvery.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenColCompatHelper.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          // end from scalactic-macro\n",
      "          GenScalacticDotty.genScalaJS((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "          GenVersions.genScalacticVersions((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          ScalacticGenResourcesJSVM.genFailureMessages((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenArrayHelper.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value)\n",
      "        }.taskValue\n",
      "      },\n",
      "      resourceGenerators in Compile += Def.task {\n",
      "        GenScalacticDotty.genResource((resourceManaged in Compile).value)\n",
      "      }.taskValue,\n",
      "      //scalacticDocSourcesSetting,\n",
      "      //docTaskSetting,\n",
      "      publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "      mimaPreviousArtifacts := Set(organization.value %% name.value % previousReleaseVersion),\n",
      "      mimaCurrentClassfiles := (classDirectory in Compile).value.getParentFile / (name.value + \"_\" + scalaBinaryVersion.value + \"-\" + releaseVersion + \".jar\")\n",
      "    ).settings(osgiSettings: _*).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\n",
      "      \"org.scalactic\",\n",
      "      \"org.scalactic.anyvals\",\n",
      "      \"org.scalactic.exceptions\",\n",
      "      \"org.scalactic.source\"\n",
      "    ),\n",
      "    OsgiKeys.importPackage := Seq(\n",
      "      \"org.scalatest.*\",\n",
      "      \"org.scalactic.*\",\n",
      "      \"scala.util.parsing.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.xml.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.*;version=\\\"$<range;[==,=+);$<replace;\"+scalaBinaryVersion.value+\";-;.>>\\\"\",\n",
      "      \"*;resolution:=optional\"\n",
      "    ),\n",
      "    OsgiKeys.additionalHeaders:= Map(\n",
      "      \"Bundle-Name\" -> \"Scalactic\",\n",
      "      \"Bundle-Description\" -> \"Scalactic is an open-source library for Scala projects.\",\n",
      "      \"Bundle-DocURL\" -> \"http://www.scalactic.org/\",\n",
      "      \"Bundle-Vendor\" -> \"Artima, Inc.\"\n",
      "    )\n",
      "  ).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestCoreDotty = project.in(file(\"dotty/core\"))\n",
      "    .enablePlugins(SbtOsgi)\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Core Dotty\",\n",
      "      organization := \"org.scalatest\",\n",
      "      moduleName := \"scalatest-core\",\n",
      "      initialCommands in console := \"\"\"|import org.scalatest._\n",
      "                                       |import org.scalactic._\n",
      "                                       |import Matchers._\"\"\".stripMargin,\n",
      "      libraryDependencies ++= scalaXmlDependency(scalaVersion.value),\n",
      "      libraryDependencies ++= scalatestLibraryDependencies,\n",
      "      packageManagedSources,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenModulesDotty.genScalaTestCore((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenVersions.genScalaTestVersions((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        ScalaTestGenResourcesJVM.genResources((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        ScalaTestGenResourcesJVM.genFailureMessages((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)  ++\n",
      "        GenConfigMap.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      javaSourceManaged := target.value / \"java\",\n",
      "      managedSourceDirectories in Compile += javaSourceManaged.value,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenScalaTestDotty.genJava((javaSourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      resourceGenerators in Compile += Def.task {\n",
      "          GenScalaTestDotty.genHtml((resourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenTable.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        GenCompatibleClasses.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\" / \"tools\", version.value, scalaVersion.value)\n",
      "        //GenSafeStyles.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      //scalatestJSDocTaskSetting,\n",
      "      publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "      mimaPreviousArtifacts := Set(organization.value %% name.value % previousReleaseVersion),\n",
      "      mimaCurrentClassfiles := (classDirectory in Compile).value.getParentFile / (name.value + \"_\" + scalaBinaryVersion.value + \"-\" + releaseVersion + \".jar\"),\n",
      "      mimaBinaryIssueFilters ++= {\n",
      "        Seq(\n",
      "          exclude[MissingClassProblem](\"org.scalatest.tools.SbtCommandParser$\"),\n",
      "          exclude[MissingClassProblem](\"org.scalatest.tools.SbtCommandParser\")\n",
      "        )\n",
      "      }\n",
      "    ).settings(osgiSettings: _*).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\n",
      "      \"org.scalatest\", \n",
      "        \"org.scalatest.concurrent\",  \n",
      "        \"org.scalatest.enablers\",  \n",
      "        \"org.scalatest.exceptions\",  \n",
      "        \"org.scalatest.events\", \n",
      "        \"org.scalatest.fixture\",  \n",
      "        \"org.scalatest.prop\", \n",
      "        \"org.scalatest.tags\", \n",
      "        \"org.scalatest.tagobjects\", \n",
      "        \"org.scalatest.time\", \n",
      "        \"org.scalatest.tools\",  \n",
      "        \"org.scalatest.verbs\"\n",
      "    ),\n",
      "    OsgiKeys.importPackage := Seq(\n",
      "      \"org.scalatest.*\",\n",
      "      \"org.scalactic.*\",\n",
      "      \"scala.util.parsing.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.xml.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.*;version=\\\"$<range;[==,=+);$<replace;\"+scalaBinaryVersion.value+\";-;.>>\\\"\",\n",
      "      \"*;resolution:=optional\"\n",
      "    ),\n",
      "    OsgiKeys.additionalHeaders:= Map(\n",
      "      \"Bundle-Name\" -> \"ScalaTest Core Dotty\",\n",
      "      \"Bundle-Description\" -> \"ScalaTest is an open-source test framework for the Javascript Platform designed to increase your productivity by letting you write fewer lines of test code that more clearly reveal your intent.\",\n",
      "      \"Bundle-DocURL\" -> \"http://www.scalatest.org/\",\n",
      "      \"Bundle-Vendor\" -> \"Artima, Inc.\",\n",
      "      \"Main-Class\" -> \"org.scalatest.tools.Runner\"\n",
      "    )\n",
      "  ).dependsOn(scalacticDotty, scalatestCompatible)\n",
      "\n",
      "  lazy val scalatestCoreDottyJS = project.in(file(\"dotty/core.js\"))\n",
      "    .enablePlugins(SbtOsgi)\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Core Dotty\",\n",
      "      organization := \"org.scalatest\",\n",
      "      moduleName := \"scalatest-core\",\n",
      "      initialCommands in console := \"\"\"|import org.scalatest._\n",
      "                                       |import org.scalactic._\n",
      "                                       |import Matchers._\"\"\".stripMargin,\n",
      "      libraryDependencies ++= scalaXmlDependency(scalaVersion.value),\n",
      "      libraryDependencies += (\"org.scala-js\" %% \"scalajs-test-interface\" % scalaJSVersion).withDottyCompat(dottyVersion), \n",
      "      packageManagedSources,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenModulesDotty.genScalaTestCoreJS((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenScalaTestDotty.genScalaJS((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenVersions.genScalaTestVersions((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        ScalaTestGenResourcesJSVM.genResources((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        ScalaTestGenResourcesJSVM.genFailureMessages((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)  ++\n",
      "        GenConfigMap.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      javaSourceManaged := target.value / \"java\",\n",
      "      managedSourceDirectories in Compile += javaSourceManaged.value,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenScalaTestDotty.genJava((javaSourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      resourceGenerators in Compile += Def.task {\n",
      "          GenScalaTestDotty.genHtml((resourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenTable.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "        //GenSafeStyles.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      //scalatestJSDocTaskSetting,\n",
      "      publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "      mimaPreviousArtifacts := Set(organization.value %% name.value % previousReleaseVersion),\n",
      "      mimaCurrentClassfiles := (classDirectory in Compile).value.getParentFile / (name.value + \"_\" + scalaBinaryVersion.value + \"-\" + releaseVersion + \".jar\"),\n",
      "      mimaBinaryIssueFilters ++= {\n",
      "        Seq(\n",
      "          exclude[MissingClassProblem](\"org.scalatest.tools.SbtCommandParser$\"),\n",
      "          exclude[MissingClassProblem](\"org.scalatest.tools.SbtCommandParser\")\n",
      "        )\n",
      "      }\n",
      "    ).settings(osgiSettings: _*).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\n",
      "      \"org.scalatest\", \n",
      "      \"org.scalatest.compatible\", \n",
      "      \"org.scalatest.concurrent\",  \n",
      "      \"org.scalatest.enablers\",  \n",
      "      \"org.scalatest.exceptions\",  \n",
      "      \"org.scalatest.events\", \n",
      "      \"org.scalatest.fixture\",  \n",
      "      \"org.scalatest.prop\", \n",
      "      \"org.scalatest.tags\", \n",
      "      \"org.scalatest.tagobjects\", \n",
      "      \"org.scalatest.time\", \n",
      "      \"org.scalatest.tools\",  \n",
      "      \"org.scalatest.verbs\"\n",
      "    ),\n",
      "    OsgiKeys.importPackage := Seq(\n",
      "      \"org.scalatest.*\",\n",
      "      \"org.scalactic.*\",\n",
      "      \"scala.util.parsing.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.xml.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.*;version=\\\"$<range;[==,=+);$<replace;\"+scalaBinaryVersion.value+\";-;.>>\\\"\",\n",
      "      \"*;resolution:=optional\"\n",
      "    ),\n",
      "    OsgiKeys.additionalHeaders:= Map(\n",
      "      \"Bundle-Name\" -> \"ScalaTest Core Dotty\",\n",
      "      \"Bundle-Description\" -> \"ScalaTest is an open-source test framework for the Javascript Platform designed to increase your productivity by letting you write fewer lines of test code that more clearly reveal your intent.\",\n",
      "      \"Bundle-DocURL\" -> \"http://www.scalatest.org/\",\n",
      "      \"Bundle-Vendor\" -> \"Artima, Inc.\",\n",
      "      \"Main-Class\" -> \"org.scalatest.tools.Runner\"\n",
      "    )\n",
      "  ).dependsOn(scalacticDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  private implicit class DottyProjectEx(private val p: Project) {\n",
      "    /** common settings for all scalatest modules */\n",
      "    def scalatestModule(name: String, title: String): Project = p\n",
      "      .enablePlugins(SbtOsgi)\n",
      "      .settings(sharedSettings: _*)\n",
      "      .settings(dottySettings: _*)\n",
      "      .settings(\n",
      "        projectTitle := title,\n",
      "        organization := \"org.scalatest\",\n",
      "        moduleName := name,\n",
      "        packageManagedSources,\n",
      "        publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "        osgiSettings,\n",
      "        OsgiKeys.additionalHeaders := Map(\n",
      "          \"Bundle-Name\" -> title,\n",
      "          \"Bundle-Description\" -> \"ScalaTest is an open-source test framework for the Javascript Platform designed to increase your productivity by letting you write fewer lines of test code that more clearly reveal your intent.\",\n",
      "          \"Bundle-DocURL\" -> \"http://www.scalatest.org/\",\n",
      "          \"Bundle-Vendor\" -> \"Artima, Inc.\"\n",
      "        ),\n",
      "      )\n",
      "\n",
      "    /** common settings for all scalatest sub modules (all modules, except the `scalatest` module) */\n",
      "    def scalatestSubModule(name: String, title: String, gen: GenModulesDotty.GenFn): Project =\n",
      "      scalatestModule(name, title).settings(\n",
      "        sourceGenerators in Compile += Def.task {\n",
      "          gen((sourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "        }.taskValue,\n",
      "        OsgiKeys.importPackage := Seq(\n",
      "          \"org.scalatest.*\",\n",
      "          \"*;resolution:=optional\"\n",
      "        ),\n",
      "      )\n",
      "\n",
      "    /** common settings for all scalatest `style` modules such as `featurespec`, `funsuite`,.. */\n",
      "    def scalatestStyleModule(style: String, title: String): Project =\n",
      "      scalatestSubModule(s\"scalatest-$style\", title, GenModulesDotty(style))\n",
      "        .settings(\n",
      "          OsgiKeys.exportPackage := Seq(s\"org.scalatest.$style\"),\n",
      "        ).dependsOn(scalatestCoreDotty)\n",
      "\n",
      "    /** common settings for all scalatest js `style` modules such as `featurespec`, `funsuite`,.. */\n",
      "    def scalatestStyleModuleJS(style: String, title: String): Project =\n",
      "      scalatestSubModule(s\"scalatest-$style\", title, GenModulesDotty.applyJS(style))\n",
      "        .settings(\n",
      "          OsgiKeys.exportPackage := Seq(s\"org.scalatest.$style\"),\n",
      "        ).dependsOn(scalatestCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "        \n",
      "  }\n",
      "  \n",
      "  lazy val scalatestFeatureSpecDotty = project.in(file(\"dotty/featurespec\"))\n",
      "    .scalatestStyleModule(\"featurespec\", \"ScalaTest FeatureSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestFeatureSpecDottyJS = project.in(file(\"dotty/featurespec.js\"))\n",
      "    .scalatestStyleModuleJS(\"featurespec\", \"ScalaTest FeatureSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestFlatSpecDotty = project.in(file(\"dotty/flatspec\"))\n",
      "    .scalatestStyleModule(\"flatspec\", \"ScalaTest FlatSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestFlatSpecDottyJS = project.in(file(\"dotty/flatspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"flatspec\", \"ScalaTest FlatSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestFreeSpecDotty = project.in(file(\"dotty/freespec\"))\n",
      "    .scalatestStyleModule(\"freespec\", \"ScalaTest FreeSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestFreeSpecDottyJS = project.in(file(\"dotty/freespec.js\"))\n",
      "    .scalatestStyleModuleJS(\"freespec\", \"ScalaTest FreeSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestFunSuiteDotty = project.in(file(\"dotty/funsuite\"))\n",
      "    .scalatestStyleModule(\"funsuite\", \"ScalaTest FunSuite Dotty\")\n",
      "\n",
      "  lazy val scalatestFunSuiteDottyJS = project.in(file(\"dotty/funsuite.js\"))\n",
      "    .scalatestStyleModuleJS(\"funsuite\", \"ScalaTest FunSuite Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestFunSpecDotty = project.in(file(\"dotty/funspec\"))\n",
      "    .scalatestStyleModule(\"funspec\", \"ScalaTest FunSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestFunSpecDottyJS = project.in(file(\"dotty/funspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"funspec\", \"ScalaTest FunSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestPropSpecDotty = project.in(file(\"dotty/propspec\"))\n",
      "    .scalatestStyleModule(\"propspec\", \"ScalaTest PropSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestPropSpecDottyJS = project.in(file(\"dotty/propspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"propspec\", \"ScalaTest PropSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestRefSpecDotty = project.in(file(\"dotty/refspec\"))\n",
      "    .scalatestStyleModule(\"refspec\", \"ScalaTest RefSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestRefSpecDottyJS = project.in(file(\"dotty/refspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"refspec\", \"ScalaTest RefSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestWordSpecDotty = project.in(file(\"dotty/wordspec\"))\n",
      "    .scalatestStyleModule(\"wordspec\", \"ScalaTest WordSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestWordSpecDottyJS = project.in(file(\"dotty/wordspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"wordspec\", \"ScalaTest WordSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestDiagramsDotty = project.in(file(\"dotty/diagrams\"))\n",
      "    .scalatestStyleModule(\"diagrams\", \"ScalaTest Diagrams Dotty\")\n",
      "\n",
      "  lazy val scalatestDiagramsDottyJS = project.in(file(\"dotty/diagrams.js\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-diagrams\", \n",
      "      \"ScalaTest Diagrams Dotty JS\", \n",
      "      (targetDir, version, scalaVersion) =>\n",
      "        GenScalaTestDotty.genDiagramsScalaJS(targetDir / \"org\" / \"scalatest\", version, scalaVersion)\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\n",
      "        \"org.scalatest\", \n",
      "        \"org.scalatest.diagrams\"\n",
      "      ),\n",
      "    ).dependsOn(scalatestCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestMatchersCoreDotty = project.in(file(\"dotty/matchers-core\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-matchers-core\",\n",
      "      \"ScalaTest Matchers Core Dotty\",\n",
      "      (targetDir, version, scalaVersion) => {\n",
      "        GenModulesDotty.genScalaTestMatchersCore(targetDir, version, scalaVersion) ++\n",
      "          GenFactoriesDotty.genMain(targetDir / \"org\" / \"scalatest\" / \"matchers\" / \"dsl\", version, scalaVersion)\n",
      "      }\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\n",
      "        \"org.scalatest.matchers\",\n",
      "        \"org.scalatest.matchers.dsl\"\n",
      "      ),\n",
      "    ).dependsOn(scalatestCoreDotty)\n",
      "\n",
      "  lazy val scalatestMatchersCoreDottyJS = project.in(file(\"dotty/matchers-core.js\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-matchers-core\",\n",
      "      \"ScalaTest Matchers Core Dotty JS\",\n",
      "      (targetDir, version, scalaVersion) => {\n",
      "        GenModulesDotty.genScalaTestMatchersCoreJS(targetDir, version, scalaVersion) ++\n",
      "        GenScalaTestDotty.genMatchersCoreScalaJS(targetDir, version, scalaVersion) ++\n",
      "        GenFactoriesDotty.genMain(targetDir / \"org\" / \"scalatest\" / \"matchers\" / \"dsl\", version, scalaVersion)\n",
      "      }\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\n",
      "        \"org.scalatest.matchers\",\n",
      "        \"org.scalatest.matchers.dsl\"\n",
      "      ),\n",
      "    ).dependsOn(scalatestCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestShouldMatchersDotty = project.in(file(\"dotty/shouldmatchers\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-shouldmatchers\",\n",
      "      \"ScalaTest Should Matchers Dotty\",\n",
      "      GenModulesDotty.genScalaTestShouldMatchers\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\"org.scalatest.matchers.should\"),\n",
      "    ).dependsOn(scalatestMatchersCoreDotty)\n",
      "\n",
      "  lazy val scalatestShouldMatchersDottyJS = project.in(file(\"dotty/shouldmatchers.js\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-shouldmatchers\",\n",
      "      \"ScalaTest Should Matchers Dotty JS\",\n",
      "      (targetDir, version, scalaVersion) => {\n",
      "        GenModulesDotty.genScalaTestShouldMatchersJS(targetDir, version, scalaVersion) ++ \n",
      "        GenScalaTestDotty.genShouldMatchersScalaJS(targetDir, version, scalaVersion)\n",
      "      }\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\"org.scalatest.matchers.should\"),\n",
      "    ).dependsOn(scalatestMatchersCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestMustMatchersDotty = project.in(file(\"dotty/mustmatchers\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-mustmatchers\",\n",
      "      \"ScalaTest Must Matchers Dotty\",\n",
      "      (targetDir, version, scalaVersion) =>\n",
      "        GenMatchers.genMainForDotty(targetDir / \"org\" / \"scalatest\", version, scalaVersion)\n",
      "    ).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\"org.scalatest.matchers.must\"),\n",
      "  ).dependsOn(scalatestMatchersCoreDotty)\n",
      "\n",
      "  lazy val scalatestMustMatchersDottyJS = project.in(file(\"dotty/mustmatchers.js\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-mustmatchers\",\n",
      "      \"ScalaTest Must Matchers DottyJS \",\n",
      "      (targetDir, version, scalaVersion) =>\n",
      "        GenMatchers.genMainForDottyJS(targetDir / \"org\" / \"scalatest\", version, scalaVersion) ++ \n",
      "        GenScalaTestDotty.genMustMatchersScalaJS(targetDir, version, scalaVersion)\n",
      "    ).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\"org.scalatest.matchers.must\"),\n",
      "  ).dependsOn(scalatestMatchersCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestModulesDotty = project.in(file(\"modules/dotty/modules-aggregation\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(\n",
      "      noPublishSettings,\n",
      "      scalacOptions in (Compile, doc) := List.empty\n",
      "    ).aggregate(\n",
      "      scalatestCoreDotty, \n",
      "      scalatestFeatureSpecDotty, \n",
      "      scalatestFlatSpecDotty, \n",
      "      scalatestFreeSpecDotty, \n",
      "      scalatestFunSuiteDotty, \n",
      "      scalatestFunSpecDotty, \n",
      "      scalatestPropSpecDotty, \n",
      "      scalatestRefSpecDotty, \n",
      "      scalatestWordSpecDotty, \n",
      "      scalatestDiagramsDotty, \n",
      "      scalatestMatchersCoreDotty, \n",
      "      scalatestShouldMatchersDotty, \n",
      "      scalatestMustMatchersDotty\n",
      "    )\n",
      "\n",
      "  lazy val scalatestDotty = project.in(file(\"dotty/scalatest\"))\n",
      "    .scalatestModule(\"scalatest\", \"ScalaTest Dotty\")\n",
      "    .settings(\n",
      "      // Little trick to get rid of bnd error when publish.\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        (crossTarget.value / \"classes\").mkdirs()\n",
      "        Seq.empty[File]\n",
      "      }.taskValue,\n",
      "      OsgiKeys.privatePackage := Seq.empty, \n",
      "    ).dependsOn(\n",
      "      scalatestCoreDotty, \n",
      "      scalatestFeatureSpecDotty, \n",
      "      scalatestFlatSpecDotty, \n",
      "      scalatestFreeSpecDotty, \n",
      "      scalatestFunSuiteDotty, \n",
      "      scalatestFunSpecDotty, \n",
      "      scalatestPropSpecDotty, \n",
      "      scalatestRefSpecDotty, \n",
      "      scalatestWordSpecDotty, \n",
      "      scalatestDiagramsDotty, \n",
      "      scalatestMatchersCoreDotty, \n",
      "      scalatestShouldMatchersDotty, \n",
      "      scalatestMustMatchersDotty\n",
      "    ).aggregate(\n",
      "      scalatestCoreDotty, \n",
      "      scalatestFeatureSpecDotty, \n",
      "      scalatestFlatSpecDotty, \n",
      "      scalatestFreeSpecDotty, \n",
      "      scalatestFunSuiteDotty, \n",
      "      scalatestFunSpecDotty, \n",
      "      scalatestPropSpecDotty, \n",
      "      scalatestRefSpecDotty, \n",
      "      scalatestWordSpecDotty, \n",
      "      scalatestDiagramsDotty, \n",
      "      scalatestMatchersCoreDotty, \n",
      "      scalatestShouldMatchersDotty, \n",
      "      scalatestMustMatchersDotty\n",
      "    )\n",
      "\n",
      "  lazy val scalatestDottyJS = project.in(file(\"dotty/scalatest.js\"))\n",
      "    .scalatestModule(\"scalatest\", \"ScalaTest Dotty JS\")\n",
      "    .settings(\n",
      "      // Little trick to get rid of bnd error when publish.\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        (crossTarget.value / \"classes\").mkdirs()\n",
      "        Seq.empty[File]\n",
      "      }.taskValue,\n",
      "      OsgiKeys.privatePackage := Seq.empty, \n",
      "    ).dependsOn(\n",
      "      scalatestCoreDottyJS, \n",
      "      scalatestFeatureSpecDottyJS, \n",
      "      scalatestFlatSpecDottyJS, \n",
      "      scalatestFreeSpecDottyJS, \n",
      "      scalatestFunSuiteDottyJS, \n",
      "      scalatestFunSpecDottyJS, \n",
      "      scalatestPropSpecDottyJS, \n",
      "      scalatestRefSpecDottyJS, \n",
      "      scalatestWordSpecDottyJS, \n",
      "      scalatestDiagramsDottyJS, \n",
      "      scalatestMatchersCoreDottyJS, \n",
      "      scalatestShouldMatchersDottyJS, \n",
      "      scalatestMustMatchersDottyJS\n",
      "    ).aggregate(\n",
      "      scalatestCoreDottyJS, \n",
      "      scalatestFeatureSpecDottyJS, \n",
      "      scalatestFlatSpecDottyJS, \n",
      "      scalatestFreeSpecDottyJS, \n",
      "      scalatestFunSuiteDottyJS, \n",
      "      scalatestFunSpecDottyJS, \n",
      "      scalatestPropSpecDottyJS, \n",
      "      scalatestRefSpecDottyJS, \n",
      "      scalatestWordSpecDottyJS, \n",
      "      scalatestDiagramsDottyJS, \n",
      "      scalatestMatchersCoreDottyJS, \n",
      "      scalatestShouldMatchersDottyJS, \n",
      "      scalatestMustMatchersDottyJS\n",
      "    ).enablePlugins(ScalaJSPlugin) \n",
      "\n",
      "  private lazy val noPublishSettings = Seq(\n",
      "    publishArtifact := false,\n",
      "    publish := {},\n",
      "    publishLocal := {},\n",
      "  )\n",
      "\n",
      "  lazy val commonTestDotty = project.in(file(\"dotty/common-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Common test classes used by scalactic and scalatest\",\n",
      "      libraryDependencies ++= crossBuildTestLibraryDependencies.value,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenCommonTestDotty.genMain((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenGen.genMain((sourceManaged in Compile).value / \"scala\" / \"org\" / \"scalatest\" / \"prop\", version.value, scalaVersion.value) ++\n",
      "        GenCompatibleClasses.genTest((sourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      noPublishSettings,\n",
      "    ).dependsOn(scalacticDotty, LocalProject(\"scalatestDotty\"))\n",
      "\n",
      "  lazy val commonTestDottyJS = project.in(file(\"dotty/common-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Common test classes used by scalactic and scalatest\",\n",
      "      libraryDependencies ++= crossBuildTestLibraryDependencies.value,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenCommonTestDotty.genMainJS((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenGen.genMain((sourceManaged in Compile).value / \"scala\" / \"org\" / \"scalatest\" / \"prop\", version.value, scalaVersion.value) ++\n",
      "        GenCompatibleClasses.genTest((sourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      noPublishSettings,\n",
      "    ).dependsOn(scalacticDottyJS, LocalProject(\"scalatestDottyJS\")).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalacticTestDotty = project.in(file(\"dotty/scalactic-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Scalactic Test\",\n",
      "      organization := \"org.scalactic\",\n",
      "      testOptions in Test ++=\n",
      "        Seq(Tests.Argument(TestFrameworks.ScalaTest,\n",
      "          \"-oDIF\",\n",
      "          \"-W\", \"120\", \"60\")),    \n",
      "      logBuffered in Test := false,\n",
      "      noPublishSettings,\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalacticDotty.genTest((sourceManaged in Test).value, version.value, scalaVersion.value) /*++\n",
      "        GenAnyVals.genTest((sourceManaged in Test).value / \"scala\" / \"org\" / \"scalactic\" / \"anyvals\", version.value, scalaVersion.value)*/\n",
      "      }.taskValue\n",
      "    ).dependsOn(scalacticDotty, scalatestDotty % \"test\", commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalacticTestDottyJS = project.in(file(\"dotty/scalactic-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Scalactic Test JS\",\n",
      "      organization := \"org.scalactic\",\n",
      "      scalaJSLinkerConfig ~= { _.withOptimizer(false) },\n",
      "      testOptions in Test ++=\n",
      "        Seq(Tests.Argument(TestFrameworks.ScalaTest, \"-oDIF\")),\n",
      "      jsEnv := {\n",
      "        import org.scalajs.jsenv.nodejs.NodeJSEnv\n",
      "        new NodeJSEnv(\n",
      "          NodeJSEnv.Config()\n",
      "            .withArgs(List(\"--max_old_space_size=3000\")))\n",
      "      }, \n",
      "      parallelExecution in Test := false,\n",
      "      fork in Test := false,\n",
      "      logBuffered in Test := false,\n",
      "      noPublishSettings,\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalacticDotty.genTestJS((sourceManaged in Test).value, version.value, scalaVersion.value) /*++\n",
      "        GenAnyVals.genTest((sourceManaged in Test).value / \"scala\" / \"org\" / \"scalactic\" / \"anyvals\", version.value, scalaVersion.value)*/\n",
      "      }.taskValue\n",
      "    ).dependsOn(scalacticDottyJS, scalatestDottyJS % \"test\", commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  def sharedTestSettingsDotty: Seq[Setting[_]] = \n",
      "    Seq(\n",
      "      organization := \"org.scalatest\",\n",
      "      libraryDependencies ++= scalatestLibraryDependencies,\n",
      "      libraryDependencies ++= \n",
      "        Seq(\n",
      "          \"org.scalatestplus\" %% \"testng-6-7\" % plusTestNGVersion % \"test\",\n",
      "          \"org.scalatestplus\" %% \"junit-4-13\" % plusJUnitVersion % \"test\"\n",
      "        ),\n",
      "      testOptions in Test := scalatestTestOptions,\n",
      "      logBuffered in Test := false,\n",
      "      //fork in Test := true,\n",
      "      //parallelExecution in Test := true,\n",
      "      //testForkedParallel in Test := true,\n",
      "      baseDirectory in Test := file(\"./\"),\n",
      "    ) ++ noPublishSettings\n",
      "\n",
      "  lazy val scalatestTestDotty = project.in(file(\"dotty/scalatest-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Test\",\n",
      "      javaSourceManaged := target.value / \"java\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenRegularTests4.genJava((javaSourceManaged in Compile).value) ++\n",
      "        GenScalaTestDotty.genTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\").aggregate(\n",
      "      scalatestDiagramsTestDotty, \n",
      "      scalatestFeatureSpecTestDotty, \n",
      "      scalatestFlatSpecTestDotty, \n",
      "      scalatestFreeSpecTestDotty, \n",
      "      scalatestFunSpecTestDotty, \n",
      "      scalatestFunSuiteTestDotty, \n",
      "      scalatestPropSpecTestDotty, \n",
      "      scalatestWordSpecTestDotty\n",
      "    )\n",
      "\n",
      "  def scalatestTestDottyJSOptions =\n",
      "    Seq(Tests.Argument(TestFrameworks.ScalaTest,\n",
      "      \"-l\", \"org.scalatest.tags.Slow\",\n",
      "      \"-m\", \"org.scalatest\",\n",
      "      \"-m\", \"org.scalactic\",\n",
      "      \"-m\", \"org.scalactic.anyvals\",\n",
      "      \"-m\", \"org.scalactic.algebra\",\n",
      "      \"-m\", \"org.scalactic.enablers\",\n",
      "      \"-m\", \"org.scalatest.fixture\",\n",
      "      \"-m\", \"org.scalatest.concurrent\",\n",
      "      \"-m\", \"org.scalatest.events\",\n",
      "      \"-m\", \"org.scalatest.prop\",\n",
      "      \"-m\", \"org.scalatest.tools\",\n",
      "      \"-m\", \"org.scalatest.matchers\",\n",
      "      \"-m\", \"org.scalatest.matchers\",\n",
      "      \"-m\", \"org.scalatest.matchers.should\",\n",
      "      \"-m\", \"org.scalatest.matchers.must\",\n",
      "      \"-m\", \"org.scalatest.matchers.dsl\",\n",
      "      \"-m\", \"org.scalatest.verbs\",\n",
      "      \"-m\", \"org.scalatest.suiteprop\",\n",
      "      \"-m\", \"org.scalatest.path\",\n",
      "      \"-m\", \"org.scalatest.exceptions\",\n",
      "      \"-m\", \"org.scalatest.time\",\n",
      "      \"-m\", \"org.scalatest.words\",\n",
      "      \"-m\", \"org.scalatest.enablers\",\n",
      "      \"-m\", \"org.scalatest.expectations\",\n",
      "      \"-m\", \"org.scalatest.diagrams\",\n",
      "      \"-m\", \"org.scalatest.featurespec\",\n",
      "      \"-m\", \"org.scalatest.flatspec\",\n",
      "      \"-m\", \"org.scalatest.freespec\",\n",
      "      \"-m\", \"org.scalatest.funspec\",\n",
      "      \"-m\", \"org.scalatest.funsuite\",\n",
      "      \"-m\", \"org.scalatest.propspec\",\n",
      "      \"-m\", \"org.scalatest.wordspec\",\n",
      "      \"-oDIF\"))    \n",
      "\n",
      "  def sharedTestSettingsDottyJS: Seq[Setting[_]] = \n",
      "    Seq(\n",
      "      organization := \"org.scalatest\",\n",
      "      //jsDependencies += RuntimeDOM % \"test\",\n",
      "      scalaJSLinkerConfig ~= { _.withOptimizer(false) },\n",
      "      //jsEnv := NodeJSEnv(executable = \"node\").value,\n",
      "      //jsEnv := PhantomJSEnv().value,\n",
      "      jsEnv := {\n",
      "        import org.scalajs.jsenv.nodejs.NodeJSEnv\n",
      "        new NodeJSEnv(\n",
      "          NodeJSEnv.Config()\n",
      "            .withArgs(List(/*\"--max_new_space_size=3000\", */\"--max_old_space_size=3000\")))\n",
      "      },\n",
      "      //Seq(Compile, Test).flatMap(c => inConfig(c)(jsEnv := RhinoJSEnv().value)), // to use rhino\n",
      "      testOptions in Test := scalatestTestDottyJSOptions,\n",
      "      parallelExecution in Test := false,\n",
      "      fork in Test := false,\n",
      "      publishArtifact := false,\n",
      "      publish := {},\n",
      "      publishLocal := {},\n",
      "      scalacOptions ++= (if (scalaBinaryVersion.value == \"2.10\" || scalaVersion.value.startsWith(\"2.13\")) Seq.empty[String] else Seq(\"-Ypartial-unification\"))\n",
      "    )  \n",
      "\n",
      "  lazy val scalatestTestDottyJS = project.in(file(\"dotty/scalatest-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Test\",\n",
      "      scalaJSLinkerConfig ~= { _.withOptimizer(false).withSemantics(_.withStrictFloats(true)) },\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        //GenRegularTests4.genJava((javaSourceManaged in Compile).value) ++\n",
      "        GenScalaTestDotty.genTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(scalacticDottyJS, scalatestDottyJS % \"test\", commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "     .aggregate(\n",
      "       scalatestDiagramsTestDottyJS, \n",
      "       scalatestFeatureSpecTestDottyJS, \n",
      "       scalatestFlatSpecTestDottyJS, \n",
      "       scalatestFreeSpecTestDottyJS, \n",
      "       scalatestFunSpecTestDottyJS, \n",
      "       scalatestFunSuiteTestDottyJS, \n",
      "       scalatestPropSpecTestDottyJS, \n",
      "       scalatestWordSpecTestDottyJS\n",
      "     ).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "\n",
      "  lazy val scalatestDiagramsTestDotty = project.in(file(\"dotty/diagrams-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Diagrams Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genDiagramsTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestDiagramsTestDottyJS = project.in(file(\"dotty/diagrams-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Diagrams Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genDiagramsTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFeatureSpecTestDotty = project.in(file(\"dotty/featurespec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FeatureSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFeatureSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFeatureSpecTestDottyJS = project.in(file(\"dotty/featurespec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FeatureSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFeatureSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFlatSpecTestDotty = project.in(file(\"dotty/flatspec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FlatSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFlatSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFlatSpecTestDottyJS = project.in(file(\"dotty/flatspec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FlatSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFlatSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFreeSpecTestDotty = project.in(file(\"dotty/freespec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FreeSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFreeSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFreeSpecTestDottyJS = project.in(file(\"dotty/freespec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FreeSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFreeSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFunSpecTestDotty = project.in(file(\"dotty/funspec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FunSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFunSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFunSpecTestDottyJS = project.in(file(\"dotty/funspec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FunSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFunSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFunSuiteTestDotty = project.in(file(\"dotty/funsuite-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FunSuite Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFunSuiteTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFunSuiteTestDottyJS = project.in(file(\"dotty/funsuite-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FunSuite Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFunSuiteTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestPropSpecTestDotty = project.in(file(\"dotty/propspec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest PropSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genPropSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestPropSpecTestDottyJS = project.in(file(\"dotty/propspec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest PropSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genPropSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)             \n",
      "\n",
      "  lazy val scalatestWordSpecTestDotty = project.in(file(\"dotty/wordspec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest WordSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genWordSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestWordSpecTestDottyJS = project.in(file(\"dotty/wordspec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest WordSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genWordSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin) \n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 8:\n",
      "package is.hail.utils.richUtils\n",
      "\n",
      "import is.hail.expr._\n",
      "import is.hail.annotations.Region\n",
      "import is.hail.asm4s.Code\n",
      "import is.hail.expr.types._\n",
      "\n",
      "class RichCodeRegion(val region: Code[Region]) extends AnyVal {\n",
      "  def size: Code[Long] = region.invoke[Long](\"size\")\n",
      "\n",
      "  def copyFrom(other: Code[Region], readStart: Code[Long], writeStart: Code[Long], n: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Region, Long, Long, Long, Unit](\"copyFrom\", other, readStart, writeStart, n)\n",
      "  }\n",
      "\n",
      "  def storeInt(off: Code[Long], v: Code[Int]): Code[Unit] = {\n",
      "    region.invoke[Long,Int,Unit](\"storeInt\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeLong(off: Code[Long], v: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long,Long,Unit](\"storeLong\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeFloat(off: Code[Long], v: Code[Float]): Code[Unit] = {\n",
      "    region.invoke[Long,Float,Unit](\"storeFloat\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeDouble(off: Code[Long], v: Code[Double]): Code[Unit] = {\n",
      "    region.invoke[Long,Double,Unit](\"storeDouble\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeAddress(off: Code[Long], a: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long,Long,Unit](\"storeAddress\", off, a)\n",
      "  }\n",
      "\n",
      "  def storeByte(off: Code[Long], b: Code[Byte]): Code[Unit] = {\n",
      "    region.invoke[Long, Byte, Unit](\"storeByte\", off, b)\n",
      "  }\n",
      "\n",
      "  def storeBytes(off: Code[Long], bytes: Code[Array[Byte]]): Code[Unit] = {\n",
      "    region.invoke[Long, Array[Byte], Unit](\"storeBytes\", off, bytes)\n",
      "  }\n",
      "\n",
      "  def allocate(alignment: Code[Long], n: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long, Long](\"allocate\", alignment, n)\n",
      "  }\n",
      "\n",
      "  def loadBoolean(off: Code[Long]): Code[Boolean] = {\n",
      "    region.invoke[Long, Boolean](\"loadBoolean\", off)\n",
      "  }\n",
      "\n",
      "  def loadByte(off: Code[Long]): Code[Byte] = {\n",
      "    region.invoke[Long, Byte](\"loadByte\", off)\n",
      "  }\n",
      "\n",
      "  def loadInt(off: Code[Long]): Code[Int] = {\n",
      "    region.invoke[Long, Int](\"loadInt\", off)\n",
      "  }\n",
      "\n",
      "  def loadLong(off: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"loadLong\", off)\n",
      "  }\n",
      "\n",
      "  def loadFloat(off: Code[Long]): Code[Float] = {\n",
      "    region.invoke[Long, Float](\"loadFloat\", off)\n",
      "  }\n",
      "\n",
      "  def loadDouble(off: Code[Long]): Code[Double] = {\n",
      "    region.invoke[Long, Double](\"loadDouble\", off)\n",
      "  }\n",
      "\n",
      "  def loadAddress(off: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"loadAddress\", off)\n",
      "  }\n",
      "\n",
      "  def loadBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Boolean] = {\n",
      "    region.invoke[Long, Long, Boolean](\"loadBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def loadIRIntermediate(typ: Type): Code[Long] => Code[_] = typ.fundamentalType match {\n",
      "    case _: TBoolean => loadBoolean\n",
      "    case _: TInt32 => loadInt\n",
      "    case _: TInt64 => loadLong\n",
      "    case _: TFloat32 => loadFloat\n",
      "    case _: TFloat64 => loadDouble\n",
      "    case _: TArray => loadAddress\n",
      "    case _: TBinary => loadAddress\n",
      "    case _: TBaseStruct => off => off\n",
      "  }\n",
      "\n",
      "  def setBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"setBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def setBit(byteOff: Code[Long], bitOff: Long): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"setBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def clearBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"clearBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def storeBit(byteOff: Code[Long], bitOff: Code[Long], b: Code[Boolean]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Boolean, Unit](\"setBit\", byteOff, bitOff, b)\n",
      "  }\n",
      "\n",
      "  def appendInt(i: Code[Int]): Code[Long] = {\n",
      "    region.invoke[Int, Long](\"appendInt\", i)\n",
      "  }\n",
      "\n",
      "  def appendLong(l: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"appendLong\", l)\n",
      "  }\n",
      "\n",
      "  def appendFloat(f: Code[Float]): Code[Long] = {\n",
      "    region.invoke[Float, Long](\"appendFloat\", f)\n",
      "  }\n",
      "\n",
      "  def appendDouble(d: Code[Double]): Code[Long] = {\n",
      "    region.invoke[Double, Long](\"appendDouble\", d)\n",
      "  }\n",
      "\n",
      "  def appendByte(b: Code[Byte]): Code[Long] = {\n",
      "    region.invoke[Byte, Long](\"appendByte\", b)\n",
      "  }\n",
      "\n",
      "  def appendBytes(bytes: Code[Array[Byte]]): Code[Long] = {\n",
      "    region.invoke[Array[Byte], Long](\"appendBytes\", bytes)\n",
      "  }\n",
      "\n",
      "  def appendBytes(bytes: Code[Array[Byte]], bytesOff: Code[Long], n: Code[Int]): Code[Long] = {\n",
      "    region.invoke[Array[Byte],Long, Int, Long](\"appendBytes\", bytes, bytesOff, n)\n",
      "  }\n",
      "\n",
      "  def appendString(string: Code[String]): Code[Long] = {\n",
      "    region.invoke[String, Long](\"appendString\", string)\n",
      "  }\n",
      "\n",
      "  def clear(): Code[Unit] = {\n",
      "    region.invoke[Unit](\"clear\")\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 9:\n",
      "<reponame>JackBuggins/spark\n",
      "/*\n",
      " * Licensed to the Apache Software Foundation (ASF) under one or more\n",
      " * contributor license agreements.  See the NOTICE file distributed with\n",
      " * this work for additional information regarding copyright ownership.\n",
      " * The ASF licenses this file to You under the Apache License, Version 2.0\n",
      " * (the \"License\"); you may not use this file except in compliance with\n",
      " * the License.  You may obtain a copy of the License at\n",
      " *\n",
      " *    http://www.apache.org/licenses/LICENSE-2.0\n",
      " *\n",
      " * Unless required by applicable law or agreed to in writing, software\n",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      " * See the License for the specific language governing permissions and\n",
      " * limitations under the License.\n",
      " */\n",
      "\n",
      "package org.apache.spark.sql.catalyst.plans.logical\n",
      "\n",
      "import scala.collection.mutable\n",
      "import scala.reflect.runtime.universe.TypeTag\n",
      "\n",
      "import org.apache.spark.sql.catalyst.dsl.expressions._\n",
      "import org.apache.spark.sql.catalyst.dsl.plans._\n",
      "import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n",
      "import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, ExpressionSet, UnspecifiedFrame}\n",
      "import org.apache.spark.sql.catalyst.plans._\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "\n",
      "class DistinctKeyVisitorSuite extends PlanTest {\n",
      "\n",
      "  private val a = AttributeReference(\"a\", IntegerType)()\n",
      "  private val b = AttributeReference(\"b\", IntegerType)()\n",
      "  private val c = AttributeReference(\"c\", IntegerType)()\n",
      "  private val d = a.as(\"aliased_a\")\n",
      "  private val e = b.as(\"aliased_b\")\n",
      "  private val f = Alias(a + 1, (a + 1).toString)()\n",
      "  private val x = AttributeReference(\"x\", IntegerType)()\n",
      "  private val y = AttributeReference(\"y\", IntegerType)()\n",
      "  private val z = AttributeReference(\"z\", IntegerType)()\n",
      "\n",
      "\n",
      "  private val t1 = LocalRelation(a, b, c).as(\"t1\")\n",
      "  private val t2 = LocalRelation(x, y, z).as(\"t2\")\n",
      "\n",
      "  private def checkDistinctAttributes(plan: LogicalPlan, distinctKeys: Set[ExpressionSet]) = {\n",
      "    assert(plan.analyze.distinctKeys === distinctKeys)\n",
      "  }\n",
      "\n",
      "  implicit private def productEncoder[T <: Product : TypeTag] = ExpressionEncoder[T]()\n",
      "\n",
      "  test(\"Aggregate's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\", 1), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\")($\"a\"), Set(ExpressionSet(Seq(a))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\"), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\", 1)($\"a\", $\"b\"), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\", 1), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\", 1)($\"a\", $\"b\", 1), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"a\"), Set.empty)\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\"), Set.empty)\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\")($\"a\", max($\"b\")), Set(ExpressionSet(Seq(a))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\", d, e),\n",
      "      Set(ExpressionSet(Seq(a, b)), ExpressionSet(Seq(d.toAttribute, e.toAttribute))))\n",
      "    checkDistinctAttributes(t1.groupBy()(sum($\"c\")), Set.empty)\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\")($\"a\", $\"a\" % 10, d, sum($\"b\")),\n",
      "      Set(ExpressionSet(Seq(a)), ExpressionSet(Seq(d.toAttribute))))\n",
      "    checkDistinctAttributes(t1.groupBy(f.child, $\"b\")(f, $\"b\", sum($\"c\")),\n",
      "      Set(ExpressionSet(Seq(f.toAttribute, b))))\n",
      "  }\n",
      "\n",
      "  test(\"Distinct's distinct attributes\") {\n",
      "    checkDistinctAttributes(Distinct(t1), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(Distinct(t1.select($\"a\", $\"c\")), Set(ExpressionSet(Seq(a, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Except's distinct attributes\") {\n",
      "    checkDistinctAttributes(Except(t1, t2, false), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(Except(t1, t2, true), Set.empty)\n",
      "  }\n",
      "\n",
      "  test(\"Filter's distinct attributes\") {\n",
      "    checkDistinctAttributes(Filter($\"a\" > 1, t1), Set.empty)\n",
      "    checkDistinctAttributes(Filter($\"a\" > 1, Distinct(t1)), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Limit's distinct attributes\") {\n",
      "    checkDistinctAttributes(Distinct(t1).limit(10), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(LocalLimit(10, Distinct(t1)), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(t1.limit(1), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Intersect's distinct attributes\") {\n",
      "    checkDistinctAttributes(Intersect(t1, t2, false), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(Intersect(t1, t2, true), Set.empty)\n",
      "  }\n",
      "\n",
      "  test(\"Join's distinct attributes\") {\n",
      "    Seq(LeftSemi, LeftAnti).foreach { joinType =>\n",
      "      checkDistinctAttributes(\n",
      "        Distinct(t1).join(t2, joinType, Some($\"a\" === $\"x\")), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    }\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1).join(Distinct(t2), Inner, Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" === $\"z\")),\n",
      "      Set(ExpressionSet(Seq(a, b, c)), ExpressionSet(Seq(x, y, z))))\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1)\n",
      "        .join(Distinct(t2), LeftOuter, Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" === $\"z\")),\n",
      "      Set(ExpressionSet(Seq(a, b, c))))\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1)\n",
      "        .join(Distinct(t2), RightOuter, Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" === $\"z\")),\n",
      "      Set(ExpressionSet(Seq(x, y, z))))\n",
      "\n",
      "    Seq(Inner, Cross, LeftOuter, RightOuter).foreach { joinType =>\n",
      "      checkDistinctAttributes(t1.join(t2, joinType, Some($\"a\" === $\"x\")),\n",
      "        Set.empty)\n",
      "      checkDistinctAttributes(\n",
      "        Distinct(t1).join(Distinct(t2), joinType, Some($\"a\" === $\"x\" && $\"b\" === $\"y\")),\n",
      "        Set.empty)\n",
      "      checkDistinctAttributes(\n",
      "        Distinct(t1).join(Distinct(t2), joinType,\n",
      "          Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" % 5 === $\"z\" % 5)),\n",
      "        Set.empty)\n",
      "    }\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1).join(Distinct(t2), Cross, Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" === $\"z\")),\n",
      "      Set.empty)\n",
      "  }\n",
      "\n",
      "  test(\"Project's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.select($\"a\", $\"b\"), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).select($\"a\"), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).select($\"a\", $\"b\", d, e), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1)\n",
      "      .select($\"a\", $\"b\", $\"c\", 1), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(Distinct(t1).select($\"a\", $\"b\", c, d),\n",
      "      Set(ExpressionSet(Seq(a, b, c)), ExpressionSet(Seq(b, c, d.toAttribute))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\", d).select($\"a\", $\"b\", e),\n",
      "      Set(ExpressionSet(Seq(a, b)), ExpressionSet(Seq(a, e.toAttribute))))\n",
      "  }\n",
      "\n",
      "  test(\"Repartition's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.repartition(8), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).repartition(8), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(RepartitionByExpression(Seq(a), Distinct(t1), None),\n",
      "      Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Sample's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.sample(0, 0.2, false, 1), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).sample(0, 0.2, false, 1), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Window's distinct attributes\") {\n",
      "    val winExpr = windowExpr(count($\"b\"),\n",
      "      windowSpec($\"a\" :: Nil, $\"b\".asc :: Nil, UnspecifiedFrame))\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1)\n",
      "        .select($\"a\", $\"b\", $\"c\", winExpr.as(Symbol(\"window\"))), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1).select($\"a\", $\"b\", winExpr.as(Symbol(\"window\"))), Set())\n",
      "  }\n",
      "\n",
      "  test(\"Tail's distinct attributes\") {\n",
      "    checkDistinctAttributes(Tail(10, Distinct(t1)), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Sort's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.sortBy($\"a\".asc), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).sortBy($\"a\".asc), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"RebalancePartitions's distinct attributes\") {\n",
      "    checkDistinctAttributes(RebalancePartitions(Seq(a), Distinct(t1)),\n",
      "      Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"WithCTE's distinct attributes\") {\n",
      "    checkDistinctAttributes(WithCTE(Distinct(t1), mutable.ArrayBuffer.empty[CTERelationDef].toSeq),\n",
      "      Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 examples\n",
    "for i, example in enumerate(dataset):\n",
    "    if i < 10:\n",
    "        print(f\"Example {i}:\")\n",
    "        print(example['content'])\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f7161a48294e44a6d93e34bdeceae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "package com.wavesplatform\n",
      "\n",
      "import java.io.File\n",
      "import java.nio.ByteBuffer\n",
      "import java.util\n",
      "\n",
      "import com.typesafe.config.ConfigFactory\n",
      "import com.wavesplatform.database.{Keys, LevelDBWriter}\n",
      "import com.wavesplatform.db.openDB\n",
      "import com.wavesplatform.settings.{WavesSettings, loadConfig}\n",
      "import com.wavesplatform.state.{ByteStr, EitherExt2}\n",
      "import com.wavesplatform.utils.{Base58, Base64}\n",
      "import org.slf4j.bridge.SLF4JBridgeHandler\n",
      "import scorex.account.{Address, AddressScheme}\n",
      "import scorex.utils.ScorexLogging\n",
      "\n",
      "import scala.collection.JavaConverters._\n",
      "import scala.util.Try\n",
      "\n",
      "object Explorer extends ScorexLogging {\n",
      "  case class Stats(entryCount: Long, totalKeySize: Long, totalValueSize: Long)\n",
      "\n",
      "  private val keys = Array(\n",
      "    \"version\",\n",
      "    \"height\",\n",
      "    \"score\",\n",
      "    \"block-at-height\",\n",
      "    \"height-of\",\n",
      "    \"waves-balance-history\",\n",
      "    \"waves-balance\",\n",
      "    \"assets-for-address\",\n",
      "    \"asset-balance-history\",\n",
      "    \"asset-balance\",\n",
      "    \"asset-info-history\",\n",
      "    \"asset-info\",\n",
      "    \"lease-balance-history\",\n",
      "    \"lease-balance\",\n",
      "    \"lease-status-history\",\n",
      "    \"lease-status\",\n",
      "    \"filled-volume-and-fee-history\",\n",
      "    \"filled-volume-and-fee\",\n",
      "    \"transaction-info\",\n",
      "    \"address-transaction-history\",\n",
      "    \"address-transaction-ids-at-height\",\n",
      "    \"changed-addresses\",\n",
      "    \"transaction-ids-at-height\",\n",
      "    \"address-id-of-alias\",\n",
      "    \"last-address-id\",\n",
      "    \"address-to-id\",\n",
      "    \"id-of-address\",\n",
      "    \"address-script-history\",\n",
      "    \"address-script\",\n",
      "    \"approved-features\",\n",
      "    \"activated-features\",\n",
      "    \"data-key-chunk-count\",\n",
      "    \"data-key-chunk\",\n",
      "    \"data-history\",\n",
      "    \"data\",\n",
      "    \"sponsorship-history\",\n",
      "    \"sponsorship\",\n",
      "    \"addresses-for-waves-seq-nr\",\n",
      "    \"addresses-for-waves\",\n",
      "    \"addresses-for-asset-seq-nr\",\n",
      "    \"addresses-for-asset\",\n",
      "    \"address-transaction-ids-seq-nr\",\n",
      "    \"address-transaction-ids\"\n",
      "  )\n",
      "\n",
      "  def main(args: Array[String]): Unit = {\n",
      "    SLF4JBridgeHandler.removeHandlersForRootLogger()\n",
      "    SLF4JBridgeHandler.install()\n",
      "\n",
      "    val configFilename = Try(args(0)).toOption.getOrElse(\"TN-testnet.conf\")\n",
      "\n",
      "    val settings = WavesSettings.fromConfig(loadConfig(ConfigFactory.parseFile(new File(configFilename))))\n",
      "    AddressScheme.current = new AddressScheme {\n",
      "      override val chainId: Byte = settings.blockchainSettings.addressSchemeCharacter.toByte\n",
      "    }\n",
      "\n",
      "    log.info(s\"Data directory: ${settings.dataDirectory}\")\n",
      "\n",
      "    val db     = openDB(settings.dataDirectory)\n",
      "    val reader = new LevelDBWriter(db, settings.blockchainSettings.functionalitySettings)\n",
      "\n",
      "    val blockchainHeight = reader.height\n",
      "    log.info(s\"Blockchain height is $blockchainHeight\")\n",
      "    try {\n",
      "\n",
      "      val flag = args(1).toUpperCase\n",
      "\n",
      "      flag match {\n",
      "        case \"B\" =>\n",
      "          val maybeBlockId = Base58.decode(args(2)).toOption.map(ByteStr.apply)\n",
      "          if (maybeBlockId.isDefined) {\n",
      "            val kBlockHeight     = Keys.heightOf(maybeBlockId.get)\n",
      "            val blockHeightBytes = db.get(kBlockHeight.keyBytes)\n",
      "            val maybeBlockHeight = kBlockHeight.parse(blockHeightBytes)\n",
      "            maybeBlockHeight.foreach { h =>\n",
      "              val kBlock     = Keys.blockBytes(h)\n",
      "              val blockBytes = db.get(kBlock.keyBytes)\n",
      "              log.info(s\"BlockId=${maybeBlockId.get} at h=$h: ${Base64.encode(blockBytes)}\")\n",
      "            }\n",
      "          } else log.error(\"No block ID was provided\")\n",
      "\n",
      "        case \"O\" =>\n",
      "          val orderId = Base58.decode(args(2)).toOption.map(ByteStr.apply)\n",
      "          if (orderId.isDefined) {\n",
      "            val kVolumeAndFee = Keys.filledVolumeAndFee(orderId.get)(blockchainHeight)\n",
      "            val bytes1        = db.get(kVolumeAndFee.keyBytes)\n",
      "            val v             = kVolumeAndFee.parse(bytes1)\n",
      "            log.info(s\"OrderId = ${Base58.encode(orderId.get.arr)}: Volume = ${v.volume}, Fee = ${v.fee}\")\n",
      "\n",
      "            val kVolumeAndFeeHistory = Keys.filledVolumeAndFeeHistory(orderId.get)\n",
      "            val bytes2               = db.get(kVolumeAndFeeHistory.keyBytes)\n",
      "            val value2               = kVolumeAndFeeHistory.parse(bytes2)\n",
      "            val value2Str            = value2.mkString(\"[\", \", \", \"]\")\n",
      "            log.info(s\"OrderId = ${Base58.encode(orderId.get.arr)}: History = $value2Str\")\n",
      "            value2.foreach { h =>\n",
      "              val k = Keys.filledVolumeAndFee(orderId.get)(h)\n",
      "              val v = k.parse(db.get(k.keyBytes))\n",
      "              log.info(s\"\\t h = $h: Volume = ${v.volume}, Fee = ${v.fee}\")\n",
      "            }\n",
      "          } else log.error(\"No order ID was provided\")\n",
      "\n",
      "        case \"A\" =>\n",
      "          val address   = Address.fromString(args(2)).explicitGet()\n",
      "          val aid       = Keys.addressId(address)\n",
      "          val addressId = aid.parse(db.get(aid.keyBytes)).get\n",
      "          log.info(s\"Address id = $addressId\")\n",
      "\n",
      "          val kwbh = Keys.wavesBalanceHistory(addressId)\n",
      "          val wbh  = kwbh.parse(db.get(kwbh.keyBytes))\n",
      "\n",
      "          val balances = wbh.map { h =>\n",
      "            val k = Keys.wavesBalance(addressId)(h)\n",
      "            h -> k.parse(db.get(k.keyBytes))\n",
      "          }\n",
      "          balances.foreach(b => log.info(s\"h = ${b._1}: balance = ${b._2}\"))\n",
      "\n",
      "        case \"AC\" =>\n",
      "          val lastAddressId = Keys.lastAddressId.parse(db.get(Keys.lastAddressId.keyBytes))\n",
      "          log.info(s\"Last address id: $lastAddressId\")\n",
      "\n",
      "        case \"AD\" =>\n",
      "          val result        = new util.HashMap[Address, java.lang.Integer]()\n",
      "          val lastAddressId = Keys.lastAddressId.parse(db.get(Keys.lastAddressId.keyBytes))\n",
      "          for (id <- BigInt(1) to lastAddressId.getOrElse(BigInt(0))) {\n",
      "            val k       = Keys.idToAddress(id)\n",
      "            val address = k.parse(db.get(k.keyBytes))\n",
      "            result.compute(address,\n",
      "                           (_, prev) =>\n",
      "                             prev match {\n",
      "                               case null    => 1\n",
      "                               case notNull => 1 + notNull\n",
      "                           })\n",
      "          }\n",
      "\n",
      "          for ((k, v) <- result.asScala if v > 1) {\n",
      "            log.info(s\"$k,$v\")\n",
      "          }\n",
      "\n",
      "        case \"AA\" =>\n",
      "          val secondaryId = args(3)\n",
      "\n",
      "          val address   = Address.fromString(args(2)).explicitGet()\n",
      "          val asset     = ByteStr.decodeBase58(secondaryId).get\n",
      "          val ai        = Keys.addressId(address)\n",
      "          val addressId = ai.parse(db.get(ai.keyBytes)).get\n",
      "          log.info(s\"Address ID = $addressId\")\n",
      "\n",
      "          val kabh = Keys.assetBalanceHistory(addressId, asset)\n",
      "          val abh  = kabh.parse(db.get(kabh.keyBytes))\n",
      "\n",
      "          val balances = abh.map { h =>\n",
      "            val k = Keys.assetBalance(addressId, asset)(h)\n",
      "            h -> k.parse(db.get(k.keyBytes))\n",
      "          }\n",
      "          balances.foreach(b => log.info(s\"h = ${b._1}: balance = ${b._2}\"))\n",
      "\n",
      "        case \"S\" =>\n",
      "          log.info(\"Collecting DB stats\")\n",
      "          val iterator = db.iterator()\n",
      "          val result   = new util.HashMap[Short, Stats]\n",
      "          iterator.seekToFirst()\n",
      "          while (iterator.hasNext) {\n",
      "            val entry     = iterator.next()\n",
      "            val keyPrefix = ByteBuffer.wrap(entry.getKey).getShort\n",
      "            result.compute(\n",
      "              keyPrefix,\n",
      "              (_, maybePrev) =>\n",
      "                maybePrev match {\n",
      "                  case null => Stats(1, entry.getKey.length, entry.getValue.length)\n",
      "                  case prev => Stats(prev.entryCount + 1, prev.totalKeySize + entry.getKey.length, prev.totalValueSize + entry.getValue.length)\n",
      "              }\n",
      "            )\n",
      "          }\n",
      "          iterator.close()\n",
      "\n",
      "          log.info(\"key-space,entry-count,total-key-size,total-value-size\")\n",
      "          for ((prefix, stats) <- result.asScala) {\n",
      "            log.info(s\"${keys(prefix)},${stats.entryCount},${stats.totalKeySize},${stats.totalValueSize}\")\n",
      "          }\n",
      "      }\n",
      "    } finally db.close()\n",
      "  }\n",
      "}\n",
      "\n",
      "Average Line Length: 36.009661835748794\n",
      "\n",
      "Example 1:\n",
      "package helpers\n",
      "\n",
      "import org.specs2.mutable._\n",
      "import com.ruimo.recoeng.RecoEngApi\n",
      "import com.ruimo.recoeng.json.{JsonResponseHeader, OnSalesJsonResponse, SalesItem, TransactionMode, TransactionSalesMode, SortOrder, JsonRequestPaging, Desc, Asc, ScoredItem}\n",
      "import com.ruimo.recoeng.json.RecommendByItemJsonResponse\n",
      "import play.api.libs.json.{JsSuccess, JsResult}\n",
      "import models.LoginSession\n",
      "import org.mockito.Mockito.mock\n",
      "import models.PersistedTransaction\n",
      "import models.TransactionLogItem\n",
      "import models.TransactionLogCoupon\n",
      "import models.TransactionLogHeader\n",
      "import models.TransactionType\n",
      "import models.Address\n",
      "import models.ItemName\n",
      "import helpers.Helper._\n",
      "import com.ruimo.scoins.Scoping._\n",
      "\n",
      "class RecommendEngineSpec extends Specification {\n",
      "  \"Recommend engine\" should {\n",
      "    \"Can send transaction\" in {\n",
      "      val api: RecoEngApi = new RecoEngApi {\n",
      "        def onSales(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          transactionMode: TransactionMode,\n",
      "          transactionTime: Long,\n",
      "          userCode: String,\n",
      "          salesItems: Seq[SalesItem]\n",
      "        ): JsResult[OnSalesJsonResponse] = {\n",
      "          transactionMode === TransactionSalesMode\n",
      "          transactionTime === 23456L\n",
      "          userCode === \"12345\"\n",
      "          salesItems.size === 3\n",
      "          val set = salesItems.toSet\n",
      "          set.contains(SalesItem(\"555\", \"8192\", 3)) must beTrue\n",
      "          set.contains(SalesItem(\"555\", \"8193\", 5)) must beTrue\n",
      "          set.contains(SalesItem(\"666\", \"8194\", 1)) must beTrue\n",
      "\n",
      "          JsSuccess(\n",
      "            OnSalesJsonResponse(\n",
      "              JsonResponseHeader(sequenceNumber = \"1234\", statusCode = \"OK\", message = \"msg\")\n",
      "            )\n",
      "          )\n",
      "        }\n",
      "\n",
      "        def recommendByItem(\n",
      "          requestTime: Long = System.currentTimeMillis,\n",
      "          sequenceNumber: Long,\n",
      "          salesItems: Seq[SalesItem],\n",
      "          sort: SortOrder = Desc(\"score\"),\n",
      "          paging: JsonRequestPaging\n",
      "        ): JsResult[RecommendByItemJsonResponse] = null\n",
      "      }\n",
      "\n",
      "      val login = mock(classOf[LoginSession])\n",
      "      val tran: PersistedTransaction = PersistedTransaction(\n",
      "        header = TransactionLogHeader(\n",
      "          id = None,\n",
      "          userId = 12345L,\n",
      "          transactionTime = 23456L,\n",
      "          currencyId = 111L,\n",
      "          totalAmount = BigDecimal(1234),\n",
      "          taxAmount = BigDecimal(20),\n",
      "          transactionType = TransactionType.NORMAL\n",
      "        ),\n",
      "        tranSiteLog = Map(),\n",
      "        siteTable = Seq(),\n",
      "        shippingTable = Map(),\n",
      "        taxTable = Map(),\n",
      "        itemTable = Map(\n",
      "          555L -> Seq(\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 888L,\n",
      "              itemId = 8192L,\n",
      "              itemPriceHistoryId = 444L,\n",
      "              quantity = 3,\n",
      "              amount = BigDecimal(123),\n",
      "              costPrice = BigDecimal(555555),\n",
      "              taxId = 1232L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]])),\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 889L,\n",
      "              itemId = 8193L,\n",
      "              itemPriceHistoryId = 445L,\n",
      "              quantity = 5,\n",
      "              amount = BigDecimal(124),\n",
      "              costPrice = BigDecimal(555556),\n",
      "              taxId = 1234L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]]))\n",
      "          ),\n",
      "          666L -> Seq(\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 890L,\n",
      "              itemId = 8194L,\n",
      "              itemPriceHistoryId = 446L,\n",
      "              quantity = 1,\n",
      "              amount = BigDecimal(125),\n",
      "              costPrice = BigDecimal(555557),\n",
      "              taxId = 1235L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]]))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      val addr = mock(classOf[Address])\n",
      "      val resp: JsResult[OnSalesJsonResponse] = RecommendEngine.sendOnSales(login, tran, Some(addr), api)\n",
      "      doWith(resp.get.header) { header =>\n",
      "        header.sequenceNumber === \"1234\"\n",
      "        header.statusCode === \"OK\"\n",
      "        header.message === \"msg\"\n",
      "      }\n",
      "    }\n",
      "\n",
      "    \"Can get recommendByItem\" in {\n",
      "      val api: RecoEngApi = new RecoEngApi {\n",
      "        def onSales(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          transactionMode: TransactionMode,\n",
      "          transactionTime: Long,\n",
      "          userCode: String,\n",
      "          itemTable: Seq[SalesItem]\n",
      "        ): JsResult[OnSalesJsonResponse] = null\n",
      "\n",
      "        def recommendByItem(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          salesItems: Seq[SalesItem],\n",
      "          sort: SortOrder,\n",
      "          paging: JsonRequestPaging\n",
      "        ): JsResult[RecommendByItemJsonResponse] = {\n",
      "          salesItems.size === 1\n",
      "          doWith(salesItems(0)) { item =>\n",
      "            item.storeCode === \"11111\"\n",
      "            item.itemCode === \"22222\"\n",
      "          }\n",
      "          sort === Desc(\"score\")\n",
      "          paging.offset === 0\n",
      "          paging.limit === 5\n",
      "\n",
      "          JsSuccess(\n",
      "            RecommendByItemJsonResponse(\n",
      "              JsonResponseHeader(sequenceNumber = \"1234\", statusCode = \"OK\", message = \"msg\"),\n",
      "              salesItems = Seq(\n",
      "                ScoredItem(\n",
      "                  storeCode = \"1212\",\n",
      "                  itemCode = \"2323\",\n",
      "                  score = 12\n",
      "                ),\n",
      "                ScoredItem(\n",
      "                  storeCode = \"3434\",\n",
      "                  itemCode = \"4545\",\n",
      "                  score = 11\n",
      "                )\n",
      "              ),\n",
      "              \"desc(\\\"col\\\")\",\n",
      "              JsonRequestPaging(\n",
      "                offset = 2,\n",
      "                limit = 20\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        }\n",
      "      }\n",
      "\n",
      "      val result: JsResult[RecommendByItemJsonResponse] =\n",
      "        RecommendEngine.sendRecommendByItem(\n",
      "          Seq(SalesItem(storeCode = \"11111\", itemCode = \"22222\", quantity = 1)),\n",
      "          api\n",
      "        )\n",
      "      doWith(result.get) { resp =>\n",
      "        doWith(resp.header) { header =>\n",
      "          header.sequenceNumber === \"1234\"\n",
      "          header.statusCode === \"OK\"\n",
      "          header.message === \"msg\"\n",
      "        }\n",
      "        doWith(resp.salesItems) { salesItems =>\n",
      "          salesItems.size === 2\n",
      "          doWith(salesItems(0)) { item =>\n",
      "            item.storeCode === \"1212\"\n",
      "            item.itemCode === \"2323\"\n",
      "            item.score === 12f\n",
      "          }\n",
      "          doWith(salesItems(1)) { item =>\n",
      "            item.storeCode === \"3434\"\n",
      "            item.itemCode === \"4545\"\n",
      "            item.score === 11f\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Average Line Length: 31.782828282828284\n",
      "\n",
      "Example 2:\n",
      "/***********************************************************************\n",
      " * Copyright (c) 2013-2017 Commonwealth Computer Research, Inc.\n",
      " * All rights reserved. This program and the accompanying materials\n",
      " * are made available under the terms of the Apache License, Version 2.0\n",
      " * which accompanies this distribution and is available at\n",
      " * http://www.opensource.org/licenses/apache2.0.php.\n",
      " ***********************************************************************/\n",
      "\n",
      "package org.locationtech.geomesa.utils.stats\n",
      "\n",
      "import java.util.Date\n",
      "\n",
      "import com.typesafe.scalalogging.LazyLogging\n",
      "import com.vividsolutions.jts.geom.{Coordinate, Geometry}\n",
      "import org.geotools.geometry.jts.JTSFactoryFinder\n",
      "import org.locationtech.geomesa.utils.clearspring.HyperLogLog\n",
      "import org.locationtech.geomesa.utils.stats.MinMax.CardinalityBits\n",
      "import org.opengis.feature.simple.SimpleFeature\n",
      "\n",
      "import scala.collection.immutable.ListMap\n",
      "\n",
      "/**\n",
      " * The MinMax stat merely returns the min/max of an attribute's values.\n",
      " * Works with dates, integers, longs, doubles, and floats.\n",
      " *\n",
      " * @param attribute attribute index for the attribute the histogram is being made for\n",
      " * @tparam T the type of the attribute the stat is targeting (needs to be comparable)\n",
      " */\n",
      "class MinMax[T] private [stats] (val attribute: Int,\n",
      "                                 private [stats] var minValue: T,\n",
      "                                 private [stats] var maxValue: T,\n",
      "                                 private [stats] val hpp: HyperLogLog)\n",
      "                                (implicit val defaults: MinMax.MinMaxDefaults[T])\n",
      "    extends Stat with LazyLogging with Serializable {\n",
      "\n",
      "  // use a secondary constructor instead of companion apply to allow mixin types (i.e. ImmutableStat)\n",
      "  def this(attribute: Int)(implicit defaults: MinMax.MinMaxDefaults[T]) =\n",
      "    this(attribute, defaults.max, defaults.min, HyperLogLog(CardinalityBits))(defaults)\n",
      "\n",
      "  override type S = MinMax[T]\n",
      "\n",
      "  def min: T = if (isEmpty) { maxValue } else { minValue }\n",
      "  def max: T = if (isEmpty) { minValue } else { maxValue }\n",
      "  def bounds: (T, T) = (min, max)\n",
      "  def cardinality: Long = hpp.cardinality()\n",
      "  def tuple: (T, T, Long) = (min, max, cardinality)\n",
      "\n",
      "  override def observe(sf: SimpleFeature): Unit = {\n",
      "    val value = sf.getAttribute(attribute).asInstanceOf[T]\n",
      "    if (value != null) {\n",
      "      try {\n",
      "        minValue = defaults.min(value, minValue)\n",
      "        maxValue = defaults.max(value, maxValue)\n",
      "        hpp.offer(value)\n",
      "      } catch {\n",
      "        case e: Exception => logger.warn(s\"Error observing value '$value': ${e.toString}\")\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  // note: can't unobserve min/max without storing a lot more data\n",
      "  override def unobserve(sf: SimpleFeature): Unit = {}\n",
      "\n",
      "  override def +(other: MinMax[T]): MinMax[T] = {\n",
      "    if (other.isEmpty) {\n",
      "      new MinMax[T](attribute, minValue, maxValue, hpp.merge())\n",
      "    } else if (this.isEmpty) {\n",
      "      new MinMax[T](attribute, other.minValue, other.maxValue, other.hpp.merge())\n",
      "    } else {\n",
      "      val plus = new MinMax[T](attribute, minValue, maxValue, hpp.merge())\n",
      "      plus += other\n",
      "      plus\n",
      "    }\n",
      "  }\n",
      "\n",
      "  override def +=(other: MinMax[T]): Unit = {\n",
      "    if (other.isEmpty) {\n",
      "      // no-op\n",
      "    } else if (isEmpty) {\n",
      "      minValue = other.minValue\n",
      "      maxValue = other.maxValue\n",
      "      hpp += other.hpp\n",
      "    } else {\n",
      "      minValue = defaults.min(minValue, other.minValue)\n",
      "      maxValue = defaults.max(maxValue, other.maxValue)\n",
      "      hpp += other.hpp\n",
      "    }\n",
      "  }\n",
      "\n",
      "  override def toJsonObject: Any =\n",
      "    if (isEmpty) {\n",
      "      ListMap(\"min\" -> null, \"max\" -> null, \"cardinality\" -> 0)\n",
      "    } else {\n",
      "      ListMap(\"min\" -> minValue, \"max\" -> maxValue, \"cardinality\" -> cardinality)\n",
      "    }\n",
      "\n",
      "  override def isEmpty: Boolean = minValue == defaults.max\n",
      "\n",
      "  override def clear(): Unit = {\n",
      "    minValue = defaults.max\n",
      "    maxValue = defaults.min\n",
      "    java.util.Arrays.fill(hpp.registerSet.rawBits, 0)\n",
      "  }\n",
      "\n",
      "  override def isEquivalent(other: Stat): Boolean = other match {\n",
      "    case that: MinMax[T] =>\n",
      "      attribute == that.attribute && minValue == that.minValue &&\n",
      "          maxValue == that.maxValue && cardinality == that.cardinality\n",
      "    case _ => false\n",
      "  }\n",
      "}\n",
      "\n",
      "object MinMax {\n",
      "\n",
      "  val CardinalityBits: Int = 10\n",
      "\n",
      "  trait MinMaxDefaults[T] {\n",
      "    def min: T\n",
      "    def max: T\n",
      "    def min(left: T, right: T): T\n",
      "    def max(left: T, right: T): T\n",
      "  }\n",
      "\n",
      "  object MinMaxDefaults {\n",
      "    def apply[T](binding: Class[_]): MinMaxDefaults[T] = {\n",
      "      if (binding == classOf[String]) {\n",
      "        MinMaxString.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (binding == classOf[Integer]) {\n",
      "        MinMaxInt.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (binding == classOf[java.lang.Long]) {\n",
      "        MinMaxLong.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (binding == classOf[java.lang.Float]) {\n",
      "        MinMaxFloat.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (binding == classOf[java.lang.Double]) {\n",
      "        MinMaxDouble.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (classOf[Date].isAssignableFrom(binding)) {\n",
      "        MinMaxDate.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else if (classOf[Geometry].isAssignableFrom(binding)) {\n",
      "        MinMaxGeometry.asInstanceOf[MinMaxDefaults[T]]\n",
      "      } else {\n",
      "        throw new IllegalArgumentException(s\"No implicit default available for type: $binding\")\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  abstract class ComparableMinMax[T <: Comparable[T]] extends MinMaxDefaults[T] with Serializable {\n",
      "    override def min(left: T, right: T): T = if (left.compareTo(right) > 0) right else left\n",
      "    override def max(left: T, right: T): T = if (left.compareTo(right) < 0) right else left\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxString extends ComparableMinMax[String] with Serializable {\n",
      "    override val min: String = \"\"\n",
      "    override val max: String = \"\\uFFFF\\uFFFF\\uFFFF\"\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxInt extends ComparableMinMax[Integer] with Serializable {\n",
      "    override val min: Integer = Integer.MIN_VALUE\n",
      "    override val max: Integer = Integer.MAX_VALUE\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxLong extends ComparableMinMax[java.lang.Long] with Serializable {\n",
      "    override val min: java.lang.Long = java.lang.Long.MIN_VALUE\n",
      "    override val max: java.lang.Long = java.lang.Long.MAX_VALUE\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxFloat extends ComparableMinMax[java.lang.Float] with Serializable {\n",
      "    override val min: java.lang.Float = 0f - java.lang.Float.MAX_VALUE\n",
      "    override val max: java.lang.Float = java.lang.Float.MAX_VALUE\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxDouble extends ComparableMinMax[java.lang.Double] with Serializable  {\n",
      "    override val min: java.lang.Double = 0d - java.lang.Double.MAX_VALUE\n",
      "    override val max: java.lang.Double = java.lang.Double.MAX_VALUE\n",
      "  }\n",
      "\n",
      "  implicit object MinMaxDate extends ComparableMinMax[Date] with Serializable {\n",
      "    override val min: Date = new Date(java.lang.Long.MIN_VALUE)\n",
      "    override val max: Date = new Date(java.lang.Long.MAX_VALUE)\n",
      "  }\n",
      "\n",
      "  /**\n",
      "    * Geometry min/max tracks the bounding box of each geometry, not the geometries themselves.\n",
      "    */\n",
      "  implicit object MinMaxGeometry extends MinMaxDefaults[Geometry] with Serializable {\n",
      "\n",
      "    private val gf = JTSFactoryFinder.getGeometryFactory\n",
      "\n",
      "    override val min: Geometry = gf.createPoint(new Coordinate(-180.0, -90.0))\n",
      "    override val max: Geometry = gf.createPoint(new Coordinate(180.0, 90.0))\n",
      "\n",
      "    override def min(left: Geometry, right: Geometry): Geometry = {\n",
      "      val (lx, ly) = { val e = left.getEnvelopeInternal; (e.getMinX, e.getMinY) }\n",
      "      val (rx, ry) = { val e = right.getEnvelopeInternal; (e.getMinX, e.getMinY) }\n",
      "\n",
      "      val x = math.min(lx, rx)\n",
      "      val y = math.min(ly, ry)\n",
      "\n",
      "      if (x == lx && y == ly) {\n",
      "        left\n",
      "      } else if (x == rx && y == ry) {\n",
      "        right\n",
      "      } else {\n",
      "        gf.createPoint(new Coordinate(x, y))\n",
      "      }\n",
      "    }\n",
      "\n",
      "    override def max(left: Geometry, right: Geometry): Geometry = {\n",
      "      val (lx, ly) = { val e = left.getEnvelopeInternal; (e.getMaxX, e.getMaxY) }\n",
      "      val (rx, ry) = { val e = right.getEnvelopeInternal; (e.getMaxX, e.getMaxY) }\n",
      "\n",
      "      val x = math.max(lx, rx)\n",
      "      val y = math.max(ly, ry)\n",
      "\n",
      "      if (x == lx && y == ly) {\n",
      "        left\n",
      "      } else if (x == rx && y == ry) {\n",
      "        right\n",
      "      } else {\n",
      "        gf.createPoint(new Coordinate(x, y))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Average Line Length: 35.455357142857146\n",
      "\n",
      "Example 3:\n",
      "package cqrs.queries\n",
      "\n",
      "import java.time.Instant\n",
      "import java.util.UUID\n",
      "\n",
      "import io.circe.{Decoder, Encoder}\n",
      "import io.circe.generic.semiauto.{deriveDecoder, deriveEncoder}\n",
      "import io.circe.java8.time._\n",
      "\n",
      "import scala.collection.immutable.SortedMap\n",
      "\n",
      "/**\n",
      "  * This is the model used for querying.\n",
      "  */\n",
      "// TODO Add useful stats\n",
      "case class Meter(id: UUID, label: String, timeSeries: SortedMap[Instant, BigDecimal])\n",
      "\n",
      "object Meter {\n",
      "\n",
      "  implicit def decodeSortedMap[A : Decoder : Ordering, B : Decoder]: Decoder[SortedMap[A, B]] =\n",
      "    Decoder[Seq[(A, B)]].map(entries => (SortedMap.newBuilder[A, B] ++= entries).result())\n",
      "\n",
      "  implicit def encodeSortedMap[A : Encoder, B : Encoder]: Encoder[SortedMap[A, B]] =\n",
      "    Encoder.encodeList[(A, B)].contramap[SortedMap[A, B]](_.to[List])\n",
      "\n",
      "  implicit val decoder: Decoder[Meter] = deriveDecoder\n",
      "  implicit val encoder: Encoder[Meter] = deriveEncoder\n",
      "\n",
      "}\n",
      "\n",
      "Average Line Length: 28.333333333333332\n",
      "\n",
      "Example 4:\n",
      "/*\n",
      "Copyright 2012 Twitter, Inc.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "*/\n",
      "\n",
      "package com.twitter.algebird\n",
      "\n",
      "class VectorSpaceProperties extends CheckProperties {\n",
      "  import com.twitter.algebird.BaseVectorSpaceProperties._\n",
      "\n",
      "  // TODO: we won't need this when we have an Equatable trait\n",
      "  def mapEqFn(a: Map[Int, Double], b: Map[Int, Double]) = {\n",
      "    (a.keySet ++ b.keySet).forall { key =>\n",
      "      (a.get(key), b.get(key)) match {\n",
      "        case (Some(aVal), Some(bVal)) => beCloseTo(aVal, bVal)\n",
      "        case (Some(aVal), None) => beCloseTo(aVal, 0.0)\n",
      "        case (None, Some(bVal)) => beCloseTo(bVal, 0.0)\n",
      "        case _ => true\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  property(\"map int double scaling\") {\n",
      "    vectorSpaceLaws[Double, ({ type x[a] = Map[Int, a] })#x](mapEqFn(_, _))\n",
      "  }\n",
      "}\n",
      "\n",
      "Average Line Length: 31.710526315789473\n",
      "\n",
      "Example 5:\n",
      "/*\n",
      " * Tencent is pleased to support the open source community by making Angel available.\n",
      " *\n",
      " * Copyright (C) 2017 THL A29 Limited, a Tencent company. All rights reserved.\n",
      " *\n",
      " * Licensed under the BSD 3-Clause License (the \"License\"); you may not use this file except in\n",
      " * compliance with the License. You may obtain a copy of the License at\n",
      " *\n",
      " * https://opensource.org/licenses/BSD-3-Clause\n",
      " *\n",
      " * Unless required by applicable law or agreed to in writing, software distributed under the License\n",
      " * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n",
      " * or implied. See the License for the specific language governing permissions and limitations under\n",
      " * the License.\n",
      " */\n",
      "\n",
      "package org.apache.spark.ml.classification.ps\n",
      "\n",
      "import scala.collection.JavaConverters._\n",
      "import scala.language.existentials\n",
      "import scala.util.Random\n",
      "import scala.util.control.Breaks._\n",
      "\n",
      "import org.apache.spark.SparkException\n",
      "import org.apache.spark.ml.attribute.NominalAttribute\n",
      "import org.apache.spark.ml.classification.LogisticRegressionSuite.{generateLogisticInput, generateMultinomialLogisticInput}\n",
      "import org.apache.spark.ml.classification.ProbabilisticClassifierSuite\n",
      "import org.apache.spark.ml.feature.{Instance, LabeledPoint}\n",
      "import org.apache.spark.ml.linalg.{DenseMatrix, Matrices, SparseMatrix, SparseVector, Vector, Vectors}\n",
      "import org.apache.spark.ml.param.ParamsSuite\n",
      "import org.apache.spark.ml.util.TestingUtils._\n",
      "import org.apache.spark.ml.util.{DefaultReadWriteTest, MLTestingUtils}\n",
      "import org.apache.spark.mllib.util.MLlibTestSparkContext\n",
      "import org.apache.spark.sql.functions.{col, lit, rand}\n",
      "import org.apache.spark.sql.types.LongType\n",
      "import org.apache.spark.sql.{Dataset, Row, SparkSession}\n",
      "import org.scalatest.{FunSuite, _}\n",
      "\n",
      "@Ignore\n",
      "class LogisticRegressionSuite extends FunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n",
      "\n",
      "  import testImplicits._\n",
      "  private val seed = 42\n",
      "  @transient var smallBinaryDataset: Dataset[_] = _\n",
      "  @transient var smallMultinomialDataset: Dataset[_] = _\n",
      "  @transient var binaryDataset: Dataset[_] = _\n",
      "  @transient var multinomialDataset: Dataset[_] = _\n",
      "  private val eps: Double = 1e-5\n",
      "\n",
      "  override def beforeAll(): Unit = {\n",
      "    super.beforeAll()\n",
      "    val spark = SparkSession.builder().getOrCreate()\n",
      "    import spark.implicits._\n",
      "\n",
      "    smallBinaryDataset = generateLogisticInput(1.0, 1.0, nPoints = 100, seed = seed).toDF()\n",
      "\n",
      "    smallMultinomialDataset = {\n",
      "      val nPoints = 100\n",
      "      val coefficients = Array(\n",
      "        -0.57997, 0.912083, -0.371077,\n",
      "        -0.16624, -0.84355, -0.048509)\n",
      "\n",
      "      val xMean = Array(5.843, 3.057)\n",
      "      val xVariance = Array(0.6856, 0.1899)\n",
      "\n",
      "      val testData = generateMultinomialLogisticInput(\n",
      "        coefficients, xMean, xVariance, addIntercept = true, nPoints, seed)\n",
      "\n",
      "      val df = sc.parallelize(testData, 4).toDF()\n",
      "      df.cache()\n",
      "      df\n",
      "    }\n",
      "\n",
      "    binaryDataset = {\n",
      "      val nPoints = 500\n",
      "      val coefficients = Array(-0.57997, 0.912083, -0.371077, -0.819866, 2.688191)\n",
      "      val xMean = Array(5.843, 3.057, 3.758, 1.199)\n",
      "      val xVariance = Array(0.6856, 0.1899, 3.116, 0.581)\n",
      "\n",
      "      val testData =\n",
      "        generateMultinomialLogisticInput(coefficients, xMean, xVariance,\n",
      "          addIntercept = true, nPoints, seed)\n",
      "\n",
      "      sc.parallelize(testData, 4).toDF().withColumn(\"weight\", rand(seed))\n",
      "    }\n",
      "\n",
      "    multinomialDataset = {\n",
      "      val nPoints = 10000\n",
      "      val coefficients = Array(\n",
      "        -0.57997, 0.912083, -0.371077, -0.819866, 2.688191,\n",
      "        -0.16624, -0.84355, -0.048509, -0.301789, 4.170682)\n",
      "\n",
      "      val xMean = Array(5.843, 3.057, 3.758, 1.199)\n",
      "      val xVariance = Array(0.6856, 0.1899, 3.116, 0.581)\n",
      "\n",
      "      val testData = generateMultinomialLogisticInput(\n",
      "        coefficients, xMean, xVariance, addIntercept = true, nPoints, seed)\n",
      "\n",
      "      val df = sc.parallelize(testData, 4).toDF().withColumn(\"weight\", rand(seed))\n",
      "      df.cache()\n",
      "      df\n",
      "    }\n",
      "  }\n",
      "\n",
      "  /**\n",
      "   * Enable the ignored test to export the dataset into CSV format,\n",
      "   * so we can validate the training accuracy compared with R's glmnet package.\n",
      "   */\n",
      "  ignore(\"export test data into CSV format\") {\n",
      "    binaryDataset.rdd.map { case Row(label: Double, features: Vector, weight: Double) =>\n",
      "      label + \",\" + weight + \",\" + features.toArray.mkString(\",\")\n",
      "    }.repartition(1).saveAsTextFile(\"target/tmp/LogisticRegressionSuite/binaryDataset\")\n",
      "    multinomialDataset.rdd.map { case Row(label: Double, features: Vector, weight: Double) =>\n",
      "      label + \",\" + weight + \",\" + features.toArray.mkString(\",\")\n",
      "    }.repartition(1).saveAsTextFile(\"target/tmp/LogisticRegressionSuite/multinomialDataset\")\n",
      "  }\n",
      "\n",
      "  test(\"params\") {\n",
      "    ParamsSuite.checkParams(new LogisticRegression)\n",
      "    val model = new LogisticRegressionModel(\"logReg\", Vectors.dense(0.0), 0.0)\n",
      "    ParamsSuite.checkParams(model)\n",
      "  }\n",
      "\n",
      "  test(\"empty probabilityCol\") {\n",
      "    val lr = new LogisticRegression().setProbabilityCol(\"\")\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    assert(model.hasSummary)\n",
      "    // Validate that we re-insert a probability column for evaluation\n",
      "    val fieldNames = model.summary.predictions.schema.fieldNames\n",
      "    assert(smallBinaryDataset.schema.fieldNames.toSet.subsetOf(\n",
      "      fieldNames.toSet))\n",
      "    assert(fieldNames.exists(s => s.startsWith(\"probability_\")))\n",
      "  }\n",
      "\n",
      "  test(\"setThreshold, getThreshold\") {\n",
      "    val lr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    // default\n",
      "    assert(lr.getThreshold === 0.5, \"LogisticRegression.threshold should default to 0.5\")\n",
      "    withClue(\"LogisticRegression should not have thresholds set by default.\") {\n",
      "      intercept[java.util.NoSuchElementException] { // Note: The exception type may change in future\n",
      "        lr.getThresholds\n",
      "      }\n",
      "    }\n",
      "    // Set via threshold.\n",
      "    // Intuition: Large threshold or large thresholds(1) makes class 0 more likely.\n",
      "    lr.setThreshold(1.0)\n",
      "    assert(lr.getThresholds === Array(0.0, 1.0))\n",
      "    lr.setThreshold(0.0)\n",
      "    assert(lr.getThresholds === Array(1.0, 0.0))\n",
      "    lr.setThreshold(0.5)\n",
      "    assert(lr.getThresholds === Array(0.5, 0.5))\n",
      "    // Set via thresholds\n",
      "    val lr2 = new LogisticRegression().setFamily(\"binomial\")\n",
      "    lr2.setThresholds(Array(0.3, 0.7))\n",
      "    val expectedThreshold = 1.0 / (1.0 + 0.3 / 0.7)\n",
      "    assert(lr2.getThreshold ~== expectedThreshold relTol 1E-7)\n",
      "    // thresholds and threshold must be consistent\n",
      "    lr2.setThresholds(Array(0.1, 0.2, 0.3))\n",
      "    withClue(\"getThreshold should throw error if thresholds has length != 2.\") {\n",
      "      intercept[IllegalArgumentException] {\n",
      "        lr2.getThreshold\n",
      "      }\n",
      "    }\n",
      "    // thresholds and threshold must be consistent: values\n",
      "    withClue(\"fit with ParamMap should throw error if threshold, thresholds do not match.\") {\n",
      "      intercept[IllegalArgumentException] {\n",
      "        lr2.fit(smallBinaryDataset,\n",
      "          lr2.thresholds -> Array(0.3, 0.7), lr2.threshold -> (expectedThreshold / 2.0))\n",
      "      }\n",
      "    }\n",
      "    withClue(\"fit with ParamMap should throw error if threshold, thresholds do not match.\") {\n",
      "      intercept[IllegalArgumentException] {\n",
      "        val lr2model = lr2.fit(smallBinaryDataset,\n",
      "          lr2.thresholds -> Array(0.3, 0.7), lr2.threshold -> (expectedThreshold / 2.0))\n",
      "        lr2model.getThreshold\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  test(\"thresholds prediction\") {\n",
      "    val blr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    val binaryModel = blr.fit(smallBinaryDataset)\n",
      "\n",
      "    binaryModel.setThreshold(1.0)\n",
      "    val binaryZeroPredictions =\n",
      "      binaryModel.transform(smallBinaryDataset).select(\"prediction\").collect()\n",
      "    assert(binaryZeroPredictions.forall(_.getDouble(0) === 0.0))\n",
      "\n",
      "    binaryModel.setThreshold(0.0)\n",
      "    val binaryOnePredictions =\n",
      "      binaryModel.transform(smallBinaryDataset).select(\"prediction\").collect()\n",
      "    assert(binaryOnePredictions.forall(_.getDouble(0) === 1.0))\n",
      "\n",
      "\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    val model = mlr.fit(smallMultinomialDataset)\n",
      "    val basePredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "\n",
      "    // should predict all zeros\n",
      "    model.setThresholds(Array(1, 1000, 1000))\n",
      "    val zeroPredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(zeroPredictions.forall(_.getDouble(0) === 0.0))\n",
      "\n",
      "    // should predict all ones\n",
      "    model.setThresholds(Array(1000, 1, 1000))\n",
      "    val onePredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(onePredictions.forall(_.getDouble(0) === 1.0))\n",
      "\n",
      "    // should predict all twos\n",
      "    model.setThresholds(Array(1000, 1000, 1))\n",
      "    val twoPredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(twoPredictions.forall(_.getDouble(0) === 2.0))\n",
      "\n",
      "    // constant threshold scaling is the same as no thresholds\n",
      "    model.setThresholds(Array(1000, 1000, 1000))\n",
      "    val scaledPredictions = model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(scaledPredictions.zip(basePredictions).forall { case (scaled, base) =>\n",
      "      scaled.getDouble(0) === base.getDouble(0)\n",
      "    })\n",
      "\n",
      "    // force it to use the predict method\n",
      "    model.setRawPredictionCol(\"\").setProbabilityCol(\"\").setThresholds(Array(0, 1, 1))\n",
      "    val predictionsWithPredict =\n",
      "      model.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    assert(predictionsWithPredict.forall(_.getDouble(0) === 0.0))\n",
      "  }\n",
      "\n",
      "  test(\"logistic regression doesn't fit intercept when fitIntercept is off\") {\n",
      "    val lr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    lr.setFitIntercept(false)\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    assert(model.intercept === 0.0)\n",
      "\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    mlr.setFitIntercept(false)\n",
      "    val mlrModel = mlr.fit(smallMultinomialDataset)\n",
      "    assert(mlrModel.interceptVector === Vectors.sparse(3, Seq()))\n",
      "  }\n",
      "\n",
      "  test(\"logistic regression with setters\") {\n",
      "    // Set params, train, and check as many params as we can.\n",
      "    val lr = new LogisticRegression()\n",
      "      .setMaxIter(10)\n",
      "      .setRegParam(1.0)\n",
      "      .setThreshold(0.6)\n",
      "      .setProbabilityCol(\"myProbability\")\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    val parent = model.parent.asInstanceOf[LogisticRegression]\n",
      "    assert(parent.getMaxIter === 10)\n",
      "    assert(parent.getRegParam === 1.0)\n",
      "    assert(parent.getThreshold === 0.6)\n",
      "    assert(model.getThreshold === 0.6)\n",
      "\n",
      "    // Modify model params, and check that the params worked.\n",
      "    model.setThreshold(1.0)\n",
      "    val predAllZero = model.transform(smallBinaryDataset)\n",
      "      .select(\"prediction\", \"myProbability\")\n",
      "      .collect()\n",
      "      .map { case Row(pred: Double, prob: Vector) => pred }\n",
      "    assert(predAllZero.forall(_ === 0),\n",
      "      s\"With threshold=1.0, expected predictions to be all 0, but only\" +\n",
      "      s\" ${predAllZero.count(_ === 0)} of ${smallBinaryDataset.count()} were 0.\")\n",
      "    // Call transform with params, and check that the params worked.\n",
      "    val predNotAllZero =\n",
      "      model.transform(smallBinaryDataset, model.threshold -> 0.0,\n",
      "        model.probabilityCol -> \"myProb\")\n",
      "        .select(\"prediction\", \"myProb\")\n",
      "        .collect()\n",
      "        .map { case Row(pred: Double, prob: Vector) => pred }\n",
      "    assert(predNotAllZero.exists(_ !== 0.0))\n",
      "\n",
      "    // Call fit() with new params, and check as many params as we can.\n",
      "    lr.setThresholds(Array(0.6, 0.4))\n",
      "    val model2 = lr.fit(smallBinaryDataset, lr.maxIter -> 5, lr.regParam -> 0.1,\n",
      "      lr.probabilityCol -> \"theProb\")\n",
      "    val parent2 = model2.parent.asInstanceOf[LogisticRegression]\n",
      "    assert(parent2.getMaxIter === 5)\n",
      "    assert(parent2.getRegParam === 0.1)\n",
      "    assert(parent2.getThreshold === 0.4)\n",
      "    assert(model2.getThreshold === 0.4)\n",
      "    assert(model2.getProbabilityCol === \"theProb\")\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression: Predictor, Classifier methods\") {\n",
      "    val sqlContext = smallMultinomialDataset.sqlContext\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "\n",
      "    val model = mlr.fit(smallMultinomialDataset)\n",
      "    assert(model.numClasses === 3)\n",
      "    val numFeatures = smallMultinomialDataset.select(\"features\").first().getAs[Vector](0).size\n",
      "    assert(model.numFeatures === numFeatures)\n",
      "\n",
      "    val results = model.transform(smallMultinomialDataset)\n",
      "    // check that raw prediction is coefficients dot features + intercept\n",
      "    results.select(\"rawPrediction\", \"features\").collect().foreach {\n",
      "      case Row(raw: Vector, features: Vector) =>\n",
      "        assert(raw.size === 3)\n",
      "        val margins = Array.tabulate(3) { k =>\n",
      "          var margin = 0.0\n",
      "          features.foreachActive { (index, value) =>\n",
      "            margin += value * model.coefficientMatrix(k, index)\n",
      "          }\n",
      "          margin += model.interceptVector(k)\n",
      "          margin\n",
      "        }\n",
      "        assert(raw ~== Vectors.dense(margins) relTol eps)\n",
      "    }\n",
      "\n",
      "    // Compare rawPrediction with probability\n",
      "    results.select(\"rawPrediction\", \"probability\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector) =>\n",
      "        assert(raw.size === 3)\n",
      "        assert(prob.size === 3)\n",
      "        val max = raw.toArray.max\n",
      "        val subtract = if (max > 0) max else 0.0\n",
      "        val sum = raw.toArray.map(x => math.exp(x - subtract)).sum\n",
      "        val probFromRaw0 = math.exp(raw(0) - subtract) / sum\n",
      "        val probFromRaw1 = math.exp(raw(1) - subtract) / sum\n",
      "        assert(prob(0) ~== probFromRaw0 relTol eps)\n",
      "        assert(prob(1) ~== probFromRaw1 relTol eps)\n",
      "        assert(prob(2) ~== 1.0 - probFromRaw1 - probFromRaw0 relTol eps)\n",
      "    }\n",
      "\n",
      "    // Compare prediction with probability\n",
      "    results.select(\"prediction\", \"probability\").collect().foreach {\n",
      "      case Row(pred: Double, prob: Vector) =>\n",
      "        val predFromProb = prob.toArray.zipWithIndex.maxBy(_._1)._2\n",
      "        assert(pred == predFromProb)\n",
      "    }\n",
      "\n",
      "    // force it to use probability2prediction\n",
      "    model.setProbabilityCol(\"\")\n",
      "    val resultsUsingProb2Predict =\n",
      "      model.transform(smallMultinomialDataset).select(\"prediction\").as[Double].collect()\n",
      "    resultsUsingProb2Predict.zip(results.select(\"prediction\").as[Double].collect()).foreach {\n",
      "      case (pred1, pred2) => assert(pred1 === pred2)\n",
      "    }\n",
      "\n",
      "    // force it to use predict\n",
      "    model.setRawPredictionCol(\"\").setProbabilityCol(\"\")\n",
      "    val resultsUsingPredict =\n",
      "      model.transform(smallMultinomialDataset).select(\"prediction\").as[Double].collect()\n",
      "    resultsUsingPredict.zip(results.select(\"prediction\").as[Double].collect()).foreach {\n",
      "      case (pred1, pred2) => assert(pred1 === pred2)\n",
      "    }\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression: Predictor, Classifier methods\") {\n",
      "    val sqlContext = smallBinaryDataset.sqlContext\n",
      "    val lr = new LogisticRegression().setFamily(\"binomial\")\n",
      "\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    assert(model.numClasses === 2)\n",
      "    val numFeatures = smallBinaryDataset.select(\"features\").first().getAs[Vector](0).size\n",
      "    assert(model.numFeatures === numFeatures)\n",
      "\n",
      "    val results = model.transform(smallBinaryDataset)\n",
      "\n",
      "    // Compare rawPrediction with probability\n",
      "    results.select(\"rawPrediction\", \"probability\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector) =>\n",
      "        assert(raw.size === 2)\n",
      "        assert(prob.size === 2)\n",
      "        val probFromRaw1 = 1.0 / (1.0 + math.exp(-raw(1)))\n",
      "        assert(prob(1) ~== probFromRaw1 relTol eps)\n",
      "        assert(prob(0) ~== 1.0 - probFromRaw1 relTol eps)\n",
      "    }\n",
      "\n",
      "    // Compare prediction with probability\n",
      "    results.select(\"prediction\", \"probability\").collect().foreach {\n",
      "      case Row(pred: Double, prob: Vector) =>\n",
      "        val predFromProb = prob.toArray.zipWithIndex.maxBy(_._1)._2\n",
      "        assert(pred == predFromProb)\n",
      "    }\n",
      "\n",
      "    // force it to use probability2prediction\n",
      "    model.setProbabilityCol(\"\")\n",
      "    val resultsUsingProb2Predict =\n",
      "      model.transform(smallBinaryDataset).select(\"prediction\").as[Double].collect()\n",
      "    resultsUsingProb2Predict.zip(results.select(\"prediction\").as[Double].collect()).foreach {\n",
      "      case (pred1, pred2) => assert(pred1 === pred2)\n",
      "    }\n",
      "\n",
      "    // force it to use predict\n",
      "    model.setRawPredictionCol(\"\").setProbabilityCol(\"\")\n",
      "    val resultsUsingPredict =\n",
      "      model.transform(smallBinaryDataset).select(\"prediction\").as[Double].collect()\n",
      "    resultsUsingPredict.zip(results.select(\"prediction\").as[Double].collect()).foreach {\n",
      "      case (pred1, pred2) => assert(pred1 === pred2)\n",
      "    }\n",
      "  }\n",
      "\n",
      "  test(\"coefficients and intercept methods\") {\n",
      "    val mlr = new LogisticRegression().setMaxIter(1).setFamily(\"multinomial\")\n",
      "    val mlrModel = mlr.fit(smallMultinomialDataset)\n",
      "    val thrownCoef = intercept[SparkException] {\n",
      "      mlrModel.coefficients\n",
      "    }\n",
      "    val thrownIntercept = intercept[SparkException] {\n",
      "      mlrModel.intercept\n",
      "    }\n",
      "    assert(thrownCoef.getMessage().contains(\"use coefficientMatrix instead\"))\n",
      "    assert(thrownIntercept.getMessage().contains(\"use interceptVector instead\"))\n",
      "\n",
      "    val blr = new LogisticRegression().setMaxIter(1).setFamily(\"binomial\")\n",
      "    val blrModel = blr.fit(smallBinaryDataset)\n",
      "    assert(blrModel.coefficients.size === 1)\n",
      "    assert(blrModel.intercept !== 0.0)\n",
      "  }\n",
      "\n",
      "  test(\"overflow prediction for multiclass\") {\n",
      "    val spark = SparkSession.builder().getOrCreate()\n",
      "    import spark.implicits._\n",
      "    val model = new LogisticRegressionModel(\"mLogReg\",\n",
      "      Matrices.dense(3, 2, Array(0.0, 0.0, 0.0, 1.0, 2.0, 3.0)),\n",
      "      Vectors.dense(0.0, 0.0, 0.0), 3, true)\n",
      "    val overFlowData = Seq(\n",
      "      LabeledPoint(1.0, Vectors.dense(0.0, 1000.0)),\n",
      "      LabeledPoint(1.0, Vectors.dense(0.0, -1.0))\n",
      "    ).toDF()\n",
      "    val results = model.transform(overFlowData).select(\"rawPrediction\", \"probability\").collect()\n",
      "\n",
      "    // probabilities are correct when margins have to be adjusted\n",
      "    val raw1 = results(0).getAs[Vector](0)\n",
      "    val prob1 = results(0).getAs[Vector](1)\n",
      "    assert(raw1 === Vectors.dense(1000.0, 2000.0, 3000.0))\n",
      "    assert(prob1 ~== Vectors.dense(0.0, 0.0, 1.0) absTol eps)\n",
      "\n",
      "    // probabilities are correct when margins don't have to be adjusted\n",
      "    val raw2 = results(1).getAs[Vector](0)\n",
      "    val prob2 = results(1).getAs[Vector](1)\n",
      "    assert(raw2 === Vectors.dense(-1.0, -2.0, -3.0))\n",
      "    assert(prob2 ~== Vectors.dense(0.66524096, 0.24472847, 0.09003057) relTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"MultiClassSummarizer\") {\n",
      "    val summarizer1 = (new MultiClassSummarizer)\n",
      "      .add(0.0).add(3.0).add(4.0).add(3.0).add(6.0)\n",
      "    assert(summarizer1.histogram === Array[Double](1, 0, 0, 2, 1, 0, 1))\n",
      "    assert(summarizer1.countInvalid === 0)\n",
      "    assert(summarizer1.numClasses === 7)\n",
      "\n",
      "    val summarizer2 = (new MultiClassSummarizer)\n",
      "      .add(1.0).add(5.0).add(3.0).add(0.0).add(4.0).add(1.0)\n",
      "    assert(summarizer2.histogram === Array[Double](1, 2, 0, 1, 1, 1))\n",
      "    assert(summarizer2.countInvalid === 0)\n",
      "    assert(summarizer2.numClasses === 6)\n",
      "\n",
      "    val summarizer3 = (new MultiClassSummarizer)\n",
      "      .add(0.0).add(1.3).add(5.2).add(2.5).add(2.0).add(4.0).add(4.0).add(4.0).add(1.0)\n",
      "    assert(summarizer3.histogram === Array[Double](1, 1, 1, 0, 3))\n",
      "    assert(summarizer3.countInvalid === 3)\n",
      "    assert(summarizer3.numClasses === 5)\n",
      "\n",
      "    val summarizer4 = (new MultiClassSummarizer)\n",
      "      .add(3.1).add(4.3).add(2.0).add(1.0).add(3.0)\n",
      "    assert(summarizer4.histogram === Array[Double](0, 1, 1, 1))\n",
      "    assert(summarizer4.countInvalid === 2)\n",
      "    assert(summarizer4.numClasses === 4)\n",
      "\n",
      "    val summarizer5 = new MultiClassSummarizer\n",
      "    assert(summarizer5.histogram.isEmpty)\n",
      "    assert(summarizer5.numClasses === 0)\n",
      "\n",
      "    // small map merges large one\n",
      "    val summarizerA = summarizer1.merge(summarizer2)\n",
      "    assert(summarizerA.hashCode() === summarizer2.hashCode())\n",
      "    assert(summarizerA.histogram === Array[Double](2, 2, 0, 3, 2, 1, 1))\n",
      "    assert(summarizerA.countInvalid === 0)\n",
      "    assert(summarizerA.numClasses === 7)\n",
      "\n",
      "    // large map merges small one\n",
      "    val summarizerB = summarizer3.merge(summarizer4)\n",
      "    assert(summarizerB.hashCode() === summarizer3.hashCode())\n",
      "    assert(summarizerB.histogram === Array[Double](1, 2, 2, 1, 3))\n",
      "    assert(summarizerB.countInvalid === 5)\n",
      "    assert(summarizerB.numClasses === 5)\n",
      "  }\n",
      "\n",
      "  test(\"MultiClassSummarizer with weighted samples\") {\n",
      "    val summarizer1 = (new MultiClassSummarizer)\n",
      "      .add(label = 0.0, weight = 0.2).add(3.0, 0.8).add(4.0, 3.2).add(3.0, 1.3).add(6.0, 3.1)\n",
      "    assert(Vectors.dense(summarizer1.histogram) ~==\n",
      "      Vectors.dense(Array(0.2, 0, 0, 2.1, 3.2, 0, 3.1)) absTol 1E-10)\n",
      "    assert(summarizer1.countInvalid === 0)\n",
      "    assert(summarizer1.numClasses === 7)\n",
      "\n",
      "    val summarizer2 = (new MultiClassSummarizer)\n",
      "      .add(1.0, 1.1).add(5.0, 2.3).add(3.0).add(0.0).add(4.0).add(1.0).add(2, 0.0)\n",
      "    assert(Vectors.dense(summarizer2.histogram) ~==\n",
      "      Vectors.dense(Array[Double](1.0, 2.1, 0.0, 1, 1, 2.3)) absTol 1E-10)\n",
      "    assert(summarizer2.countInvalid === 0)\n",
      "    assert(summarizer2.numClasses === 6)\n",
      "\n",
      "    val summarizer = summarizer1.merge(summarizer2)\n",
      "    assert(Vectors.dense(summarizer.histogram) ~==\n",
      "      Vectors.dense(Array(1.2, 2.1, 0.0, 3.1, 4.2, 2.3, 3.1)) absTol 1E-10)\n",
      "    assert(summarizer.countInvalid === 0)\n",
      "    assert(summarizer.numClasses === 7)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept without regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setStandardization(true)\n",
      "      .setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true).setStandardization(false)\n",
      "      .setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 0))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                          s0\n",
      "      (Intercept)  2.7355261\n",
      "      data.V3     -0.5734389\n",
      "      data.V4      0.8911736\n",
      "      data.V5     -0.3878645\n",
      "      data.V6     -0.8060570\n",
      "\n",
      "     */\n",
      "    val coefficientsR = Vectors.dense(-0.5734389, 0.8911736, -0.3878645, -0.8060570)\n",
      "    val interceptR = 2.7355261\n",
      "\n",
      "    assert(model1.intercept ~== interceptR relTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsR relTol 1E-3)\n",
      "\n",
      "    // Without regularization, with or without standardization will converge to the same solution.\n",
      "    assert(model2.intercept ~== interceptR relTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR relTol 1E-3)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression without intercept without regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false).setStandardization(true)\n",
      "      .setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false).setStandardization(false)\n",
      "      .setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 0, intercept=FALSE))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                          s0\n",
      "      (Intercept)  .\n",
      "      data.V3     -0.3448461\n",
      "      data.V4      1.2776453\n",
      "      data.V5     -0.3539178\n",
      "      data.V6     -0.7469384\n",
      "\n",
      "     */\n",
      "    val coefficientsR = Vectors.dense(-0.3448461, 1.2776453, -0.3539178, -0.7469384)\n",
      "\n",
      "    assert(model1.intercept ~== 0.0 relTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsR relTol 1E-2)\n",
      "\n",
      "    // Without regularization, with or without standardization should converge to the same solution.\n",
      "    assert(model2.intercept ~== 0.0 relTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR relTol 1E-2)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept with L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.12).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.12).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1,\n",
      "      lambda = 0.12, standardize=T))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept) -0.06775980\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.03933146\n",
      "      data.V6     -0.03047580\n",
      "\n",
      "     */\n",
      "    val coefficients = Array(-0.57997, 0.912083, -0.371077, -0.819866, 2.688191)\n",
      "    val coefficientsRStd = Vectors.dense(0.0, 0.0, -0.03933146, -0.03047580)\n",
      "    val interceptRStd = -0.06775980\n",
      "\n",
      "    assert(model1.intercept ~== interceptRStd relTol 1E-2)\n",
      "    assert(model1.coefficients ~= coefficientsRStd absTol 2E-2)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1,\n",
      "      lambda = 0.12, standardize=F))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                          s0\n",
      "      (Intercept)  0.3544768\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.1626191\n",
      "      data.V6      .\n",
      "\n",
      "     */\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.0, -0.1626191, 0.0)\n",
      "    val interceptR = 0.3544768\n",
      "\n",
      "    println(s\"coff ${model2.coefficientMatrix.toArray.mkString(\" \")}\")\n",
      "    println(s\"intercept ${model2.interceptVector.toArray.mkString(\" \")}\")\n",
      "\n",
      "    assert(model2.intercept ~== interceptR relTol 1E-2)\n",
      "    assert(model2.coefficients ~== coefficientsR absTol 1E-3)\n",
      "    // TODO: move this to a standalone test of compression after SPARK-17471\n",
      "    assert(model2.coefficients.isInstanceOf[SparseVector])\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression without intercept with L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.12).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.12).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1,\n",
      "      lambda = 0.12, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1,\n",
      "      lambda = 0.12, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.04967635\n",
      "      data.V6     -0.04757757\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.08433195\n",
      "      data.V6      .\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(0.0, 0.0, -0.04967635, -0.04757757)\n",
      "\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.0, -0.08433195, 0.0)\n",
      "\n",
      "    assert(model1.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsRStd absTol 1E-3)\n",
      "    assert(model2.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR absTol 1E-3)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept with L2 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(1.37).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(1.37).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 1.37, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 1.37, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  0.12707703\n",
      "      data.V3     -0.06980967\n",
      "      data.V4      0.10803933\n",
      "      data.V5     -0.04800404\n",
      "      data.V6     -0.10165096\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  0.46613016\n",
      "      data.V3     -0.04944529\n",
      "      data.V4      0.02326772\n",
      "      data.V5     -0.11362772\n",
      "      data.V6     -0.06312848\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(-0.06980967, 0.10803933, -0.04800404, -0.10165096)\n",
      "    val interceptRStd = 0.12707703\n",
      "    val coefficientsR = Vectors.dense(-0.04944529, 0.02326772, -0.11362772, -0.06312848)\n",
      "    val interceptR = 0.46613016\n",
      "\n",
      "    assert(model1.intercept ~== interceptRStd relTol 1E-2)\n",
      "    assert(model1.coefficients ~= coefficientsRStd relTol 1E-2)\n",
      "    assert(model2.intercept ~== interceptR relTol 3E-2)\n",
      "    assert(model2.coefficients ~= coefficientsR relTol 1E-2)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression without intercept with L2 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(1.37).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(1.37).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 1.37, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0,\n",
      "      lambda = 1.37, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3     -0.06000152\n",
      "      data.V4      0.12598737\n",
      "      data.V5     -0.04669009\n",
      "      data.V6     -0.09941025\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                            s0\n",
      "      (Intercept)  .\n",
      "      data.V3     -0.005482255\n",
      "      data.V4      0.048106338\n",
      "      data.V5     -0.093411640\n",
      "      data.V6     -0.054149798\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(-0.06000152, 0.12598737, -0.04669009, -0.09941025)\n",
      "    val coefficientsR = Vectors.dense(-0.005482255, 0.048106338, -0.093411640, -0.054149798)\n",
      "\n",
      "    assert(model1.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsRStd relTol 1E-2)\n",
      "    assert(model2.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR relTol 1E-2)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept with ElasticNet regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setMaxIter(200)\n",
      "      .setElasticNetParam(0.38).setRegParam(0.21).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.38).setRegParam(0.21).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0.38,\n",
      "      lambda = 0.21, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0.38,\n",
      "      lambda = 0.21, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  0.49991996\n",
      "      data.V3     -0.04131110\n",
      "      data.V4      .\n",
      "      data.V5     -0.08585233\n",
      "      data.V6     -0.15875400\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                          s0\n",
      "      (Intercept)  0.5024256\n",
      "      data.V3      .\n",
      "      data.V4      .\n",
      "      data.V5     -0.1846038\n",
      "      data.V6     -0.0559614\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(-0.04131110, 0.0, -0.08585233, -0.15875400)\n",
      "    val interceptRStd = 0.49991996\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.0, -0.1846038, -0.0559614)\n",
      "    val interceptR = 0.5024256\n",
      "\n",
      "    assert(model1.intercept ~== interceptRStd relTol 6E-3)\n",
      "    assert(model1.coefficients ~== coefficientsRStd absTol 5E-3)\n",
      "    assert(model2.intercept ~== interceptR relTol 6E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR absTol 1E-3)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression without intercept with ElasticNet regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.38).setRegParam(0.21).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.38).setRegParam(0.21).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0.38,\n",
      "      lambda = 0.21, intercept=FALSE, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 0.38,\n",
      "      lambda = 0.21, intercept=FALSE, standardize=F))\n",
      "      coefficientsStd\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3      .\n",
      "      data.V4      0.06859390\n",
      "      data.V5     -0.07900058\n",
      "      data.V6     -0.14684320\n",
      "\n",
      "      coefficients\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "      (Intercept)  .\n",
      "      data.V3      .\n",
      "      data.V4      0.03060637\n",
      "      data.V5     -0.11126742\n",
      "      data.V6      .\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = Vectors.dense(0.0, 0.06859390, -0.07900058, -0.14684320)\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.03060637, -0.11126742, 0.0)\n",
      "\n",
      "    assert(model1.intercept ~== 0.0 relTol 1E-3)\n",
      "    assert(model1.coefficients ~= coefficientsRStd absTol 1E-2)\n",
      "    assert(model2.intercept ~== 0.0 absTol 1E-3)\n",
      "    assert(model2.coefficients ~= coefficientsR absTol 1E-2)\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with intercept with strong L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(1.0).setRegParam(6.0).setStandardization(true)\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(1.0).setRegParam(6.0).setStandardization(false)\n",
      "\n",
      "    val model1 = trainer1.fit(binaryDataset)\n",
      "    val model2 = trainer2.fit(binaryDataset)\n",
      "\n",
      "    val histogram = binaryDataset.as[Instance].rdd.map { i => (i.label, i.weight)}\n",
      "      .treeAggregate(new MultiClassSummarizer)(\n",
      "        seqOp = (c, v) => (c, v) match {\n",
      "          case (classSummarizer: MultiClassSummarizer, (label: Double, weight: Double)) =>\n",
      "            classSummarizer.add(label, weight)\n",
      "        },\n",
      "        combOp = (c1, c2) => (c1, c2) match {\n",
      "          case (classSummarizer1: MultiClassSummarizer, classSummarizer2: MultiClassSummarizer) =>\n",
      "            classSummarizer1.merge(classSummarizer2)\n",
      "        }).histogram\n",
      "\n",
      "    /*\n",
      "       For binary logistic regression with strong L1 regularization, all the coefficients\n",
      "       will be zeros. As a result,\n",
      "       {{{\n",
      "       P(0) = 1 / (1 + \\exp(b)), and\n",
      "       P(1) = \\exp(b) / (1 + \\exp(b))\n",
      "       }}}, hence\n",
      "       {{{\n",
      "       b = \\log{P(1) / P(0)} = \\log{count_1 / count_0}\n",
      "       }}}\n",
      "     */\n",
      "    val interceptTheory = math.log(histogram(1) / histogram(0))\n",
      "    val coefficientsTheory = Vectors.dense(0.0, 0.0, 0.0, 0.0)\n",
      "\n",
      "    assert(model1.intercept ~== interceptTheory relTol 1E-5)\n",
      "    assert(model1.coefficients ~= coefficientsTheory absTol 1E-6)\n",
      "\n",
      "    assert(model2.intercept ~== interceptTheory relTol 1E-5)\n",
      "    assert(model2.coefficients ~= coefficientsTheory absTol 1E-6)\n",
      "\n",
      "    /*\n",
      "       Using the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "       library(\"glmnet\")\n",
      "       data <- read.csv(\"path\", header=FALSE)\n",
      "       label = factor(data$V1)\n",
      "       w = data$V2\n",
      "       features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "       coefficients = coef(glmnet(features, label, weights=w, family=\"binomial\", alpha = 1.0,\n",
      "       lambda = 6.0))\n",
      "       coefficients\n",
      "\n",
      "       5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                           s0\n",
      "       (Intercept) -0.2516986\n",
      "       data.V3      0.0000000\n",
      "       data.V4      .\n",
      "       data.V5      .\n",
      "       data.V6      .\n",
      "     */\n",
      "    val interceptR = -0.2516986\n",
      "    val coefficientsR = Vectors.dense(0.0, 0.0, 0.0, 0.0)\n",
      "\n",
      "    assert(model1.intercept ~== interceptR relTol 1E-5)\n",
      "    assert(model1.coefficients ~== coefficientsR absTol 1E-6)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept with strong L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(1.0).setRegParam(6.0).setStandardization(true)\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(1.0).setRegParam(6.0).setStandardization(false)\n",
      "\n",
      "    val sqlContext = multinomialDataset.sqlContext\n",
      "    import sqlContext.implicits._\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "\n",
      "    val histogram = multinomialDataset.as[Instance].rdd.map(i => (i.label, i.weight))\n",
      "      .treeAggregate(new MultiClassSummarizer)(\n",
      "        seqOp = (c, v) => (c, v) match {\n",
      "          case (classSummarizer: MultiClassSummarizer, (label: Double, weight: Double)) =>\n",
      "            classSummarizer.add(label, weight)\n",
      "        },\n",
      "        combOp = (c1, c2) => (c1, c2) match {\n",
      "          case (classSummarizer1: MultiClassSummarizer, classSummarizer2: MultiClassSummarizer) =>\n",
      "            classSummarizer1.merge(classSummarizer2)\n",
      "        }).histogram\n",
      "    val numFeatures = multinomialDataset.as[Instance].first().features.size\n",
      "    val numClasses = histogram.length\n",
      "\n",
      "    /*\n",
      "       For multinomial logistic regression with strong L1 regularization, all the coefficients\n",
      "       will be zeros. As a result, the intercepts will be proportional to the log counts in the\n",
      "       histogram.\n",
      "       {{{\n",
      "         \\exp(b_k) = count_k * \\exp(\\lambda)\n",
      "         b_k = \\log(count_k) * \\lambda\n",
      "       }}}\n",
      "       \\lambda is a free parameter, so choose the phase \\lambda such that the\n",
      "       mean is centered. This yields\n",
      "       {{{\n",
      "         b_k = \\log(count_k)\n",
      "         b_k' = b_k - \\mean(b_k)\n",
      "       }}}\n",
      "     */\n",
      "    val rawInterceptsTheory = histogram.map(c => math.log(c + 1)) // add 1 for smoothing\n",
      "    val rawMean = rawInterceptsTheory.sum / rawInterceptsTheory.length\n",
      "    val interceptsTheory = Vectors.dense(rawInterceptsTheory.map(_ - rawMean))\n",
      "    val coefficientsTheory = new DenseMatrix(numClasses, numFeatures,\n",
      "      Array.fill[Double](numClasses * numFeatures)(0.0), isTransposed = true)\n",
      "\n",
      "    assert(model1.interceptVector ~== interceptsTheory relTol 1E-3)\n",
      "    assert(model1.coefficientMatrix ~= coefficientsTheory absTol 1E-6)\n",
      "\n",
      "    assert(model2.interceptVector ~== interceptsTheory relTol 1E-3)\n",
      "    assert(model2.coefficientMatrix ~= coefficientsTheory absTol 1E-6)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept without regularization\") {\n",
      "\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.0).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.0).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\",\n",
      "      alpha = 0, lambda = 0))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -2.10320093\n",
      "      data.V3  0.24337896\n",
      "      data.V4 -0.05916156\n",
      "      data.V5  0.14446790\n",
      "      data.V6  0.35976165\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               0.3394473\n",
      "      data.V3 -0.3443375\n",
      "      data.V4  0.9181331\n",
      "      data.V5 -0.2283959\n",
      "      data.V6 -0.4388066\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               1.76375361\n",
      "      data.V3  0.10095851\n",
      "      data.V4 -0.85897154\n",
      "      data.V5  0.08392798\n",
      "      data.V6  0.07904499\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.24337896, -0.05916156, 0.14446790, 0.35976165,\n",
      "      -0.3443375, 0.9181331, -0.2283959, -0.4388066,\n",
      "      0.10095851, -0.85897154, 0.08392798, 0.07904499), isTransposed = true)\n",
      "    val interceptsR = Vectors.dense(-2.10320093, 0.3394473, 1.76375361)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model1.coefficientMatrix.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model1.interceptVector ~== interceptsR relTol 0.05)\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model2.coefficientMatrix.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.interceptVector ~== interceptsR relTol 0.05)\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression without intercept without regularization\") {\n",
      "\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.0).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.0).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0,\n",
      "      lambda = 0, intercept=F))\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  0.07276291\n",
      "      data.V4 -0.36325496\n",
      "      data.V5  0.12015088\n",
      "      data.V6  0.31397340\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3 -0.3180040\n",
      "      data.V4  0.9679074\n",
      "      data.V5 -0.2252219\n",
      "      data.V6 -0.4319914\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3  0.2452411\n",
      "      data.V4 -0.6046524\n",
      "      data.V5  0.1050710\n",
      "      data.V6  0.1180180\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.07276291, -0.36325496, 0.12015088, 0.31397340,\n",
      "      -0.3180040, 0.9679074, -0.2252219, -0.4319914,\n",
      "      0.2452411, -0.6046524, 0.1050710, 0.1180180), isTransposed = true)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model1.coefficientMatrix.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model1.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model2.coefficientMatrix.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept with L1 regularization\") {\n",
      "\n",
      "    // use tighter constraints because OWL-QN solver takes longer to converge\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.05).setStandardization(true)\n",
      "      .setMaxIter(300).setTol(1e-10).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.05).setStandardization(false)\n",
      "      .setMaxIter(300).setTol(1e-10).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\",\n",
      "      alpha = 1, lambda = 0.05, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 1,\n",
      "      lambda = 0.05, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -0.62244703\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  0.08419825\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -0.2804845\n",
      "      data.V3 -0.1336960\n",
      "      data.V4  0.3717091\n",
      "      data.V5 -0.1530363\n",
      "      data.V6 -0.2035286\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               0.9029315\n",
      "      data.V3  .\n",
      "      data.V4 -0.4629737\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -0.44215290\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  0.01767089\n",
      "      data.V6  0.02542866\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               0.76308326\n",
      "      data.V3 -0.06818576\n",
      "      data.V4  .\n",
      "      data.V5 -0.20446351\n",
      "      data.V6 -0.13017924\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -0.3209304\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.08419825,\n",
      "      -0.1336960, 0.3717091, -0.1530363, -0.2035286,\n",
      "      0.0, -0.4629737, 0.0, 0.0), isTransposed = true)\n",
      "    val interceptsRStd = Vectors.dense(-0.62244703, -0.2804845, 0.9029315)\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.01767089, 0.02542866,\n",
      "      -0.06818576, 0.0, -0.20446351, -0.13017924,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "    val interceptsR = Vectors.dense(-0.44215290, 0.76308326, -0.3209304)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.2)\n",
      "    assert(model1.interceptVector ~== interceptsRStd relTol 0.1)\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.02)\n",
      "    assert(model2.interceptVector ~== interceptsR relTol 0.1)\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression without intercept with L1 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.05).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(1.0).setRegParam(0.05).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 1,\n",
      "      lambda = 0.05, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 1,\n",
      "      lambda = 0.05, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              .\n",
      "      data.V3 .\n",
      "      data.V4 .\n",
      "      data.V5 .\n",
      "      data.V6 0.01144225\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3 -0.1678787\n",
      "      data.V4  0.5385351\n",
      "      data.V5 -0.1573039\n",
      "      data.V6 -0.2471624\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  0.1929409\n",
      "      data.V5 -0.1889121\n",
      "      data.V6 -0.1010413\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.01144225,\n",
      "      -0.1678787, 0.5385351, -0.1573039, -0.2471624,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.0,\n",
      "      0.0, 0.1929409, -0.1889121, -0.1010413,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.01)\n",
      "    assert(model2.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept with L2 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.1).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.1).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame( data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\",\n",
      "      alpha = 0, lambda = 0.1, intercept=T, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0,\n",
      "      lambda = 0.1, intercept=T, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                         s0\n",
      "              -1.5898288335\n",
      "      data.V3  0.1691226336\n",
      "      data.V4  0.0002983651\n",
      "      data.V5  0.1001732896\n",
      "      data.V6  0.2554575585\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               0.2125746\n",
      "      data.V3 -0.2304586\n",
      "      data.V4  0.6153492\n",
      "      data.V5 -0.1537017\n",
      "      data.V6 -0.2975443\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               1.37725427\n",
      "      data.V3  0.06133600\n",
      "      data.V4 -0.61564761\n",
      "      data.V5  0.05352840\n",
      "      data.V6  0.04208671\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -1.5681088\n",
      "      data.V3  0.1508182\n",
      "      data.V4  0.0121955\n",
      "      data.V5  0.1217930\n",
      "      data.V6  0.2162850\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               1.1217130\n",
      "      data.V3 -0.2028984\n",
      "      data.V4  0.2862431\n",
      "      data.V5 -0.1843559\n",
      "      data.V6 -0.2481218\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               0.44639579\n",
      "      data.V3  0.05208012\n",
      "      data.V4 -0.29843864\n",
      "      data.V5  0.06256289\n",
      "      data.V6  0.03183676\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.1691226336, 0.0002983651, 0.1001732896, 0.2554575585,\n",
      "      -0.2304586, 0.6153492, -0.1537017, -0.2975443,\n",
      "      0.06133600, -0.61564761, 0.05352840, 0.04208671), isTransposed = true)\n",
      "    val interceptsRStd = Vectors.dense(-1.5898288335, 0.2125746, 1.37725427)\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.1508182, 0.0121955, 0.1217930, 0.2162850,\n",
      "      -0.2028984, 0.2862431, -0.1843559, -0.2481218,\n",
      "      0.05208012, -0.29843864, 0.06256289, 0.03183676), isTransposed = true)\n",
      "    val interceptsR = Vectors.dense(-1.5681088, 1.1217130, 0.44639579)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.001)\n",
      "    assert(model1.interceptVector ~== interceptsRStd relTol 0.05)\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR relTol 0.05)\n",
      "    assert(model2.interceptVector ~== interceptsR relTol 0.05)\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression without intercept with L2 regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.1).setStandardization(true).setWeightCol(\"weight\")\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false)\n",
      "      .setElasticNetParam(0.0).setRegParam(0.1).setStandardization(false).setWeightCol(\"weight\")\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0,\n",
      "      lambda = 0.1, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0,\n",
      "      lambda = 0.1, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  0.04048126\n",
      "      data.V4 -0.23075758\n",
      "      data.V5  0.08228864\n",
      "      data.V6  0.22277648\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3 -0.2149745\n",
      "      data.V4  0.6478666\n",
      "      data.V5 -0.1515158\n",
      "      data.V6 -0.2930498\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  0.17449321\n",
      "      data.V4 -0.41710901\n",
      "      data.V5  0.06922716\n",
      "      data.V6  0.07027332\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                        s0\n",
      "               .\n",
      "      data.V3 -0.003949652\n",
      "      data.V4 -0.142982415\n",
      "      data.V5  0.091439598\n",
      "      data.V6  0.179286241\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3 -0.09071124\n",
      "      data.V4  0.39752531\n",
      "      data.V5 -0.16233832\n",
      "      data.V6 -0.22206059\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  0.09466090\n",
      "      data.V4 -0.25454290\n",
      "      data.V5  0.07089872\n",
      "      data.V6  0.04277435\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.04048126, -0.23075758, 0.08228864, 0.22277648,\n",
      "      -0.2149745, 0.6478666, -0.1515158, -0.2930498,\n",
      "      0.17449321, -0.41710901, 0.06922716, 0.07027332), isTransposed = true)\n",
      "\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      -0.003949652, -0.142982415, 0.091439598, 0.179286241,\n",
      "      -0.09071124, 0.39752531, -0.16233832, -0.22206059,\n",
      "      0.09466090, -0.25454290, 0.07089872, 0.04277435), isTransposed = true)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.01)\n",
      "    assert(model2.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression with intercept with elasticnet regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(0.5).setRegParam(0.1).setStandardization(true)\n",
      "      .setMaxIter(300).setTol(1e-10)\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(true).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(0.5).setRegParam(0.1).setStandardization(false)\n",
      "      .setMaxIter(300).setTol(1e-10)\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0.5,\n",
      "      lambda = 0.1, intercept=T, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0.5,\n",
      "      lambda = 0.1, intercept=T, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -0.50133383\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  0.08351653\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -0.3151913\n",
      "      data.V3 -0.1058702\n",
      "      data.V4  0.3183251\n",
      "      data.V5 -0.1212969\n",
      "      data.V6 -0.1629778\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               0.8165252\n",
      "      data.V3  .\n",
      "      data.V4 -0.3943069\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              -0.38857157\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  0.02384198\n",
      "      data.V6  0.03127749\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               0.62492165\n",
      "      data.V3 -0.04949061\n",
      "      data.V4  .\n",
      "      data.V5 -0.18584462\n",
      "      data.V6 -0.08952455\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              -0.2363501\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.08351653,\n",
      "      -0.1058702, 0.3183251, -0.1212969, -0.1629778,\n",
      "      0.0, -0.3943069, 0.0, 0.0), isTransposed = true)\n",
      "    val interceptsRStd = Vectors.dense(-0.50133383, -0.3151913, 0.8165252)\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.02384198, 0.03127749,\n",
      "      -0.04949061, 0.0, -0.18584462, -0.08952455,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "    val interceptsR = Vectors.dense(-0.38857157, 0.62492165, -0.2363501)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector ~== interceptsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.01)\n",
      "    assert(model2.interceptVector ~== interceptsR absTol 0.01)\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"multinomial logistic regression without intercept with elasticnet regularization\") {\n",
      "    val trainer1 = (new LogisticRegression).setFitIntercept(false).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(0.5).setRegParam(0.1).setStandardization(true)\n",
      "      .setMaxIter(300).setTol(1e-10)\n",
      "    val trainer2 = (new LogisticRegression).setFitIntercept(false).setWeightCol(\"weight\")\n",
      "      .setElasticNetParam(0.5).setRegParam(0.1).setStandardization(false)\n",
      "      .setMaxIter(300).setTol(1e-10)\n",
      "\n",
      "    val model1 = trainer1.fit(multinomialDataset)\n",
      "    val model2 = trainer2.fit(multinomialDataset)\n",
      "    /*\n",
      "      Use the following R code to load the data and train the model using glmnet package.\n",
      "\n",
      "      library(\"glmnet\")\n",
      "      data <- read.csv(\"path\", header=FALSE)\n",
      "      label = as.factor(data$V1)\n",
      "      w = data$V2\n",
      "      features = as.matrix(data.frame(data$V3, data$V4, data$V5, data$V6))\n",
      "      coefficientsStd = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0.5,\n",
      "      lambda = 0.1, intercept=F, standardize=T))\n",
      "      coefficients = coef(glmnet(features, label, weights=w, family=\"multinomial\", alpha = 0.5,\n",
      "      lambda = 0.1, intercept=F, standardize=F))\n",
      "      coefficientsStd\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "              .\n",
      "      data.V3 .\n",
      "      data.V4 .\n",
      "      data.V5 .\n",
      "      data.V6 0.03238285\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                      s0\n",
      "               .\n",
      "      data.V3 -0.1328284\n",
      "      data.V4  0.4219321\n",
      "      data.V5 -0.1247544\n",
      "      data.V6 -0.1893318\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "              .\n",
      "      data.V3 0.004572312\n",
      "      data.V4 .\n",
      "      data.V5 .\n",
      "      data.V6 .\n",
      "\n",
      "\n",
      "      coefficients\n",
      "      $`0`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "      $`1`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                       s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  0.14571623\n",
      "      data.V5 -0.16456351\n",
      "      data.V6 -0.05866264\n",
      "\n",
      "      $`2`\n",
      "      5 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "              s0\n",
      "               .\n",
      "      data.V3  .\n",
      "      data.V4  .\n",
      "      data.V5  .\n",
      "      data.V6  .\n",
      "\n",
      "\n",
      "     */\n",
      "    val coefficientsRStd = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.03238285,\n",
      "      -0.1328284, 0.4219321, -0.1247544, -0.1893318,\n",
      "      0.004572312, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "\n",
      "    val coefficientsR = new DenseMatrix(3, 4, Array(\n",
      "      0.0, 0.0, 0.0, 0.0,\n",
      "      0.0, 0.14571623, -0.16456351, -0.05866264,\n",
      "      0.0, 0.0, 0.0, 0.0), isTransposed = true)\n",
      "\n",
      "    assert(model1.coefficientMatrix ~== coefficientsRStd absTol 0.01)\n",
      "    assert(model1.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model1.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "    assert(model2.coefficientMatrix ~== coefficientsR absTol 0.01)\n",
      "    assert(model2.interceptVector.toArray === Array.fill(3)(0.0))\n",
      "    assert(model2.interceptVector.toArray.sum ~== 0.0 absTol eps)\n",
      "  }\n",
      "\n",
      "  test(\"evaluate on test set\") {\n",
      "    // TODO: add for multiclass when model summary becomes available\n",
      "    // Evaluate on test set should be same as that of the transformed training data.\n",
      "    val lr = new LogisticRegression()\n",
      "      .setMaxIter(10)\n",
      "      .setRegParam(1.0)\n",
      "      .setThreshold(0.6)\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    val summary = model.summary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
      "\n",
      "    val sameSummary =\n",
      "      model.evaluate(smallBinaryDataset).asInstanceOf[BinaryLogisticRegressionSummary]\n",
      "    assert(summary.areaUnderROC === sameSummary.areaUnderROC)\n",
      "    assert(summary.roc.collect() === sameSummary.roc.collect())\n",
      "    assert(summary.pr.collect === sameSummary.pr.collect())\n",
      "    assert(\n",
      "      summary.fMeasureByThreshold.collect() === sameSummary.fMeasureByThreshold.collect())\n",
      "    assert(summary.recallByThreshold.collect() === sameSummary.recallByThreshold.collect())\n",
      "    assert(\n",
      "      summary.precisionByThreshold.collect() === sameSummary.precisionByThreshold.collect())\n",
      "  }\n",
      "\n",
      "  test(\"evaluate with labels that are not doubles\") {\n",
      "    // Evaluate a test set with Label that is a numeric type other than Double\n",
      "    val lr = new LogisticRegression()\n",
      "      .setMaxIter(1)\n",
      "      .setRegParam(1.0)\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    val summary = model.evaluate(smallBinaryDataset).asInstanceOf[BinaryLogisticRegressionSummary]\n",
      "\n",
      "    val longLabelData = smallBinaryDataset.select(col(model.getLabelCol).cast(LongType),\n",
      "      col(model.getFeaturesCol))\n",
      "    val longSummary = model.evaluate(longLabelData).asInstanceOf[BinaryLogisticRegressionSummary]\n",
      "\n",
      "    assert(summary.areaUnderROC === longSummary.areaUnderROC)\n",
      "  }\n",
      "\n",
      "  test(\"statistics on training data\") {\n",
      "    // Test that loss is monotonically decreasing.\n",
      "    val lr = new LogisticRegression()\n",
      "      .setMaxIter(10)\n",
      "      .setRegParam(1.0)\n",
      "      .setThreshold(0.6)\n",
      "    val model = lr.fit(smallBinaryDataset)\n",
      "    assert(\n",
      "      model.summary\n",
      "        .objectiveHistory\n",
      "        .sliding(2)\n",
      "        .forall(x => x(0) >= x(1)))\n",
      "  }\n",
      "\n",
      "  test(\"set family\") {\n",
      "    val lr = new LogisticRegression().setMaxIter(1)\n",
      "    // don't set anything for binary classification\n",
      "    val model1 = lr.fit(binaryDataset)\n",
      "    assert(model1.coefficientMatrix.numRows === 1 && model1.coefficientMatrix.numCols === 4)\n",
      "    assert(model1.interceptVector.size === 1)\n",
      "\n",
      "    // set to multinomial for binary classification\n",
      "    val model2 = lr.setFamily(\"multinomial\").fit(binaryDataset)\n",
      "    assert(model2.coefficientMatrix.numRows === 2 && model2.coefficientMatrix.numCols === 4)\n",
      "    assert(model2.interceptVector.size === 2)\n",
      "\n",
      "    // set to binary for binary classification\n",
      "    val model3 = lr.setFamily(\"binomial\").fit(binaryDataset)\n",
      "    assert(model3.coefficientMatrix.numRows === 1 && model3.coefficientMatrix.numCols === 4)\n",
      "    assert(model3.interceptVector.size === 1)\n",
      "\n",
      "    // don't set anything for multiclass classification\n",
      "    val mlr = new LogisticRegression().setMaxIter(1)\n",
      "    val model4 = mlr.fit(multinomialDataset)\n",
      "    assert(model4.coefficientMatrix.numRows === 3 && model4.coefficientMatrix.numCols === 4)\n",
      "    assert(model4.interceptVector.size === 3)\n",
      "\n",
      "    // set to binary for multiclass classification\n",
      "    mlr.setFamily(\"binomial\")\n",
      "    val thrown = intercept[IllegalArgumentException] {\n",
      "      mlr.fit(multinomialDataset)\n",
      "    }\n",
      "    assert(thrown.getMessage.contains(\"Binomial family only supports 1 or 2 outcome classes\"))\n",
      "\n",
      "    // set to multinomial for multiclass\n",
      "    mlr.setFamily(\"multinomial\")\n",
      "    val model5 = mlr.fit(multinomialDataset)\n",
      "    assert(model5.coefficientMatrix.numRows === 3 && model5.coefficientMatrix.numCols === 4)\n",
      "    assert(model5.interceptVector.size === 3)\n",
      "  }\n",
      "\n",
      "  test(\"set initial model\") {\n",
      "    val lr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    val model1 = lr.fit(smallBinaryDataset)\n",
      "    val lr2 = new LogisticRegression().setInitialModel(model1).setMaxIter(5).setFamily(\"binomial\")\n",
      "    val model2 = lr2.fit(smallBinaryDataset)\n",
      "    val predictions1 = model1.transform(smallBinaryDataset).select(\"prediction\").collect()\n",
      "    val predictions2 = model2.transform(smallBinaryDataset).select(\"prediction\").collect()\n",
      "    predictions1.zip(predictions2).foreach { case (Row(p1: Double), Row(p2: Double)) =>\n",
      "      assert(p1 === p2)\n",
      "    }\n",
      "    assert(model2.summary.totalIterations === 1)\n",
      "\n",
      "    val lr3 = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    val model3 = lr3.fit(smallMultinomialDataset)\n",
      "    val lr4 = new LogisticRegression()\n",
      "      .setInitialModel(model3).setMaxIter(5).setFamily(\"multinomial\")\n",
      "    val model4 = lr4.fit(smallMultinomialDataset)\n",
      "    val predictions3 = model3.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    val predictions4 = model4.transform(smallMultinomialDataset).select(\"prediction\").collect()\n",
      "    predictions3.zip(predictions4).foreach { case (Row(p1: Double), Row(p2: Double)) =>\n",
      "      assert(p1 === p2)\n",
      "    }\n",
      "    // TODO: check that it converges in a single iteration when model summary is available\n",
      "  }\n",
      "\n",
      "  test(\"binary logistic regression with all labels the same\") {\n",
      "    val sameLabels = smallBinaryDataset\n",
      "      .withColumn(\"zeroLabel\", lit(0.0))\n",
      "      .withColumn(\"oneLabel\", lit(1.0))\n",
      "\n",
      "    // fitIntercept=true\n",
      "    val lrIntercept = new LogisticRegression()\n",
      "      .setFitIntercept(true)\n",
      "      .setMaxIter(3)\n",
      "      .setFamily(\"binomial\")\n",
      "\n",
      "    val allZeroInterceptModel = lrIntercept\n",
      "      .setLabelCol(\"zeroLabel\")\n",
      "      .fit(sameLabels)\n",
      "    assert(allZeroInterceptModel.coefficients ~== Vectors.dense(0.0) absTol 1E-3)\n",
      "    assert(allZeroInterceptModel.intercept === Double.NegativeInfinity)\n",
      "    assert(allZeroInterceptModel.summary.totalIterations === 0)\n",
      "\n",
      "    val allOneInterceptModel = lrIntercept\n",
      "      .setLabelCol(\"oneLabel\")\n",
      "      .fit(sameLabels)\n",
      "    assert(allOneInterceptModel.coefficients ~== Vectors.dense(0.0) absTol 1E-3)\n",
      "    assert(allOneInterceptModel.intercept === Double.PositiveInfinity)\n",
      "    assert(allOneInterceptModel.summary.totalIterations === 0)\n",
      "\n",
      "    // fitIntercept=false\n",
      "    val lrNoIntercept = new LogisticRegression()\n",
      "      .setFitIntercept(false)\n",
      "      .setMaxIter(3)\n",
      "      .setFamily(\"binomial\")\n",
      "\n",
      "    val allZeroNoInterceptModel = lrNoIntercept\n",
      "      .setLabelCol(\"zeroLabel\")\n",
      "      .fit(sameLabels)\n",
      "    assert(allZeroNoInterceptModel.intercept === 0.0)\n",
      "    assert(allZeroNoInterceptModel.summary.totalIterations > 0)\n",
      "\n",
      "    val allOneNoInterceptModel = lrNoIntercept\n",
      "      .setLabelCol(\"oneLabel\")\n",
      "      .fit(sameLabels)\n",
      "    assert(allOneNoInterceptModel.intercept === 0.0)\n",
      "    assert(allOneNoInterceptModel.summary.totalIterations > 0)\n",
      "  }\n",
      "\n",
      "  test(\"multiclass logistic regression with all labels the same\") {\n",
      "    val spark = SparkSession.builder().getOrCreate()\n",
      "    import spark.implicits._\n",
      "\n",
      "    val constantData = Seq(\n",
      "      LabeledPoint(4.0, Vectors.dense(0.0)),\n",
      "      LabeledPoint(4.0, Vectors.dense(1.0)),\n",
      "      LabeledPoint(4.0, Vectors.dense(2.0))).toDF()\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    val model = mlr.fit(constantData)\n",
      "    val results = model.transform(constantData)\n",
      "    results.select(\"rawPrediction\", \"probability\", \"prediction\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector, pred: Double) =>\n",
      "        assert(raw === Vectors.dense(Array(0.0, 0.0, 0.0, 0.0, Double.PositiveInfinity)))\n",
      "        assert(prob === Vectors.dense(Array(0.0, 0.0, 0.0, 0.0, 1.0)))\n",
      "        assert(pred === 4.0)\n",
      "    }\n",
      "\n",
      "    // force the model to be trained with only one class\n",
      "    val constantZeroData = Seq(\n",
      "      LabeledPoint(0.0, Vectors.dense(0.0)),\n",
      "      LabeledPoint(0.0, Vectors.dense(1.0)),\n",
      "      LabeledPoint(0.0, Vectors.dense(2.0))).toDF()\n",
      "    val modelZeroLabel = mlr.setFitIntercept(false).fit(constantZeroData)\n",
      "    val resultsZero = modelZeroLabel.transform(constantZeroData)\n",
      "    resultsZero.select(\"rawPrediction\", \"probability\", \"prediction\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector, pred: Double) =>\n",
      "        assert(prob === Vectors.dense(Array(1.0)))\n",
      "        assert(pred === 0.0)\n",
      "    }\n",
      "\n",
      "    // ensure that the correct value is predicted when numClasses passed through metadata\n",
      "    val labelMeta = NominalAttribute.defaultAttr.withName(\"label\").withNumValues(6).toMetadata()\n",
      "    val constantDataWithMetadata = constantData\n",
      "      .select(constantData(\"label\").as(\"label\", labelMeta), constantData(\"features\"))\n",
      "    val modelWithMetadata = mlr.setFitIntercept(true).fit(constantDataWithMetadata)\n",
      "    val resultsWithMetadata = modelWithMetadata.transform(constantDataWithMetadata)\n",
      "    resultsWithMetadata.select(\"rawPrediction\", \"probability\", \"prediction\").collect().foreach {\n",
      "      case Row(raw: Vector, prob: Vector, pred: Double) =>\n",
      "        assert(raw === Vectors.dense(Array(0.0, 0.0, 0.0, 0.0, Double.PositiveInfinity, 0.0)))\n",
      "        assert(prob === Vectors.dense(Array(0.0, 0.0, 0.0, 0.0, 1.0, 0.0)))\n",
      "        assert(pred === 4.0)\n",
      "    }\n",
      "    // TODO: check num iters is zero when it become available in the model\n",
      "  }\n",
      "\n",
      "  test(\"compressed storage\") {\n",
      "    val spark = SparkSession.builder().getOrCreate()\n",
      "    import spark.implicits._\n",
      "\n",
      "    val moreClassesThanFeatures = Seq(\n",
      "      LabeledPoint(4.0, Vectors.dense(0.0, 0.0, 0.0)),\n",
      "      LabeledPoint(4.0, Vectors.dense(1.0, 1.0, 1.0)),\n",
      "      LabeledPoint(4.0, Vectors.dense(2.0, 2.0, 2.0))).toDF()\n",
      "    val mlr = new LogisticRegression().setFamily(\"multinomial\")\n",
      "    val model = mlr.fit(moreClassesThanFeatures)\n",
      "    assert(model.coefficientMatrix.isInstanceOf[SparseMatrix])\n",
      "    assert(model.coefficientMatrix.asInstanceOf[SparseMatrix].colPtrs.length === 4)\n",
      "    val moreFeaturesThanClasses = Seq(\n",
      "      LabeledPoint(1.0, Vectors.dense(0.0, 0.0, 0.0)),\n",
      "      LabeledPoint(1.0, Vectors.dense(1.0, 1.0, 1.0)),\n",
      "      LabeledPoint(1.0, Vectors.dense(2.0, 2.0, 2.0))).toDF()\n",
      "    val model2 = mlr.fit(moreFeaturesThanClasses)\n",
      "    assert(model2.coefficientMatrix.isInstanceOf[SparseMatrix])\n",
      "    assert(model2.coefficientMatrix.asInstanceOf[SparseMatrix].colPtrs.length === 3)\n",
      "\n",
      "    val blr = new LogisticRegression().setFamily(\"binomial\")\n",
      "    val blrModel = blr.fit(moreFeaturesThanClasses)\n",
      "    assert(blrModel.coefficientMatrix.isInstanceOf[SparseMatrix])\n",
      "    assert(blrModel.coefficientMatrix.asInstanceOf[SparseMatrix].colPtrs.length === 2)\n",
      "  }\n",
      "\n",
      "  test(\"numClasses specified in metadata/inferred\") {\n",
      "    val lr = new LogisticRegression().setMaxIter(1).setFamily(\"multinomial\")\n",
      "\n",
      "    // specify more classes than unique label values\n",
      "    val labelMeta = NominalAttribute.defaultAttr.withName(\"label\").withNumValues(4).toMetadata()\n",
      "    val df = smallMultinomialDataset.select(smallMultinomialDataset(\"label\").as(\"label\", labelMeta),\n",
      "      smallMultinomialDataset(\"features\"))\n",
      "    val model1 = lr.fit(df)\n",
      "    assert(model1.numClasses === 4)\n",
      "    assert(model1.interceptVector.size === 4)\n",
      "\n",
      "    // specify two classes when there are really three\n",
      "    val labelMeta1 = NominalAttribute.defaultAttr.withName(\"label\").withNumValues(2).toMetadata()\n",
      "    val df1 = smallMultinomialDataset\n",
      "      .select(smallMultinomialDataset(\"label\").as(\"label\", labelMeta1),\n",
      "        smallMultinomialDataset(\"features\"))\n",
      "    val thrown = intercept[IllegalArgumentException] {\n",
      "      lr.fit(df1)\n",
      "    }\n",
      "    assert(thrown.getMessage.contains(\"less than the number of unique labels\"))\n",
      "\n",
      "    // lr should infer the number of classes if not specified\n",
      "    val model3 = lr.fit(smallMultinomialDataset)\n",
      "    assert(model3.numClasses === 3)\n",
      "  }\n",
      "\n",
      "  test(\"should support all NumericType labels and not support other types\") {\n",
      "    val lr = new LogisticRegression().setMaxIter(1)\n",
      "    MLTestingUtils.checkNumericTypes[LogisticRegressionModel, LogisticRegression](\n",
      "      lr, spark) { (expected, actual) =>\n",
      "        assert(expected.intercept === actual.intercept)\n",
      "        assert(expected.coefficients.toArray === actual.coefficients.toArray)\n",
      "      }\n",
      "  }\n",
      "}\n",
      "\n",
      "object LogisticRegressionSuite {\n",
      "\n",
      "  /**\n",
      "   * Mapping from all Params to valid settings which differ from the defaults.\n",
      "   * This is useful for tests which need to exercise all Params, such as save/load.\n",
      "   * This excludes input columns to simplify some tests.\n",
      "   */\n",
      "  val allParamSettings: Map[String, Any] = ProbabilisticClassifierSuite.allParamSettings ++ Map(\n",
      "    \"probabilityCol\" -> \"myProbability\",\n",
      "    \"thresholds\" -> Array(0.4, 0.6),\n",
      "    \"regParam\" -> 0.01,\n",
      "    \"elasticNetParam\" -> 0.1,\n",
      "    \"maxIter\" -> 2,  // intentionally small\n",
      "    \"fitIntercept\" -> true,\n",
      "    \"tol\" -> 0.8,\n",
      "    \"standardization\" -> false,\n",
      "    \"threshold\" -> 0.6\n",
      "  )\n",
      "\n",
      "  def generateLogisticInputAsList(\n",
      "    offset: Double,\n",
      "    scale: Double,\n",
      "    nPoints: Int,\n",
      "    seed: Int): java.util.List[LabeledPoint] = {\n",
      "    generateLogisticInput(offset, scale, nPoints, seed).asJava\n",
      "  }\n",
      "\n",
      "  // Generate input of the form Y = logistic(offset + scale*X)\n",
      "  def generateLogisticInput(\n",
      "      offset: Double,\n",
      "      scale: Double,\n",
      "      nPoints: Int,\n",
      "      seed: Int): Seq[LabeledPoint] = {\n",
      "    val rnd = new Random(seed)\n",
      "    val x1 = Array.fill[Double](nPoints)(rnd.nextGaussian())\n",
      "\n",
      "    val y = (0 until nPoints).map { i =>\n",
      "      val p = 1.0 / (1.0 + math.exp(-(offset + scale * x1(i))))\n",
      "      if (rnd.nextDouble() < p) 1.0 else 0.0\n",
      "    }\n",
      "\n",
      "    val testData = (0 until nPoints).map(i => LabeledPoint(y(i), Vectors.dense(Array(x1(i)))))\n",
      "    testData\n",
      "  }\n",
      "\n",
      "  /**\n",
      "   * Generates `k` classes multinomial synthetic logistic input in `n` dimensional space given the\n",
      "   * model weights and mean/variance of the features. The synthetic data will be drawn from\n",
      "   * the probability distribution constructed by weights using the following formula.\n",
      "   *\n",
      "   * P(y = 0 | x) = 1 / norm\n",
      "   * P(y = 1 | x) = exp(x * w_1) / norm\n",
      "   * P(y = 2 | x) = exp(x * w_2) / norm\n",
      "   * ...\n",
      "   * P(y = k-1 | x) = exp(x * w_{k-1}) / norm\n",
      "   * where norm = 1 + exp(x * w_1) + exp(x * w_2) + ... + exp(x * w_{k-1})\n",
      "   *\n",
      "   * @param weights matrix is flatten into a vector; as a result, the dimension of weights vector\n",
      "   *                will be (k - 1) * (n + 1) if `addIntercept == true`, and\n",
      "   *                if `addIntercept != true`, the dimension will be (k - 1) * n.\n",
      "   * @param xMean the mean of the generated features. Lots of time, if the features are not properly\n",
      "   *              standardized, the algorithm with poor implementation will have difficulty\n",
      "   *              to converge.\n",
      "   * @param xVariance the variance of the generated features.\n",
      "   * @param addIntercept whether to add intercept.\n",
      "   * @param nPoints the number of instance of generated data.\n",
      "   * @param seed the seed for random generator. For consistent testing result, it will be fixed.\n",
      "   */\n",
      "  def generateMultinomialLogisticInput(\n",
      "      weights: Array[Double],\n",
      "      xMean: Array[Double],\n",
      "      xVariance: Array[Double],\n",
      "      addIntercept: Boolean,\n",
      "      nPoints: Int,\n",
      "      seed: Int): Seq[LabeledPoint] = {\n",
      "    val rnd = new Random(seed)\n",
      "\n",
      "    val xDim = xMean.length\n",
      "    val xWithInterceptsDim = if (addIntercept) xDim + 1 else xDim\n",
      "    val nClasses = weights.length / xWithInterceptsDim + 1\n",
      "\n",
      "    val x = Array.fill[Vector](nPoints)(Vectors.dense(Array.fill[Double](xDim)(rnd.nextGaussian())))\n",
      "\n",
      "    x.foreach { vector =>\n",
      "      // This doesn't work if `vector` is a sparse vector.\n",
      "      val vectorArray = vector.toArray\n",
      "      var i = 0\n",
      "      val len = vectorArray.length\n",
      "      while (i < len) {\n",
      "        vectorArray(i) = vectorArray(i) * math.sqrt(xVariance(i)) + xMean(i)\n",
      "        i += 1\n",
      "      }\n",
      "    }\n",
      "\n",
      "    val y = (0 until nPoints).map { idx =>\n",
      "      val xArray = x(idx).toArray\n",
      "      val margins = Array.ofDim[Double](nClasses)\n",
      "      val probs = Array.ofDim[Double](nClasses)\n",
      "\n",
      "      for (i <- 0 until nClasses - 1) {\n",
      "        for (j <- 0 until xDim) margins(i + 1) += weights(i * xWithInterceptsDim + j) * xArray(j)\n",
      "        if (addIntercept) margins(i + 1) += weights((i + 1) * xWithInterceptsDim - 1)\n",
      "      }\n",
      "      // Preventing the overflow when we compute the probability\n",
      "      val maxMargin = margins.max\n",
      "      if (maxMargin > 0) for (i <- 0 until nClasses) margins(i) -= maxMargin\n",
      "\n",
      "      // Computing the probabilities for each class from the margins.\n",
      "      val norm = {\n",
      "        var temp = 0.0\n",
      "        for (i <- 0 until nClasses) {\n",
      "          probs(i) = math.exp(margins(i))\n",
      "          temp += probs(i)\n",
      "        }\n",
      "        temp\n",
      "      }\n",
      "      for (i <- 0 until nClasses) probs(i) /= norm\n",
      "\n",
      "      // Compute the cumulative probability so we can generate a random number and assign a label.\n",
      "      for (i <- 1 until nClasses) probs(i) += probs(i - 1)\n",
      "      val p = rnd.nextDouble()\n",
      "      var y = 0\n",
      "      breakable {\n",
      "        for (i <- 0 until nClasses) {\n",
      "          if (p < probs(i)) {\n",
      "            y = i\n",
      "            break\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      y\n",
      "    }\n",
      "\n",
      "    val testData = (0 until nPoints).map(i => LabeledPoint(y(i), x(i)))\n",
      "    testData\n",
      "  }\n",
      "}\n",
      "\n",
      "Average Line Length: 37.32495344506518\n",
      "\n",
      "Example 6:\n",
      "<gh_stars>0\n",
      "/*\n",
      " * Copyright 2016-2021 47 Degrees Open Source <https://www.47deg.com>\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not use this file except in compliance with the License.\n",
      " * You may obtain a copy of the License at\n",
      " *\n",
      " *     http://www.apache.org/licenses/LICENSE-2.0\n",
      " *\n",
      " * Unless required by applicable law or agreed to in writing, software\n",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      " * See the License for the specific language governing permissions and\n",
      " * limitations under the License.\n",
      " */\n",
      "\n",
      "package github4s\n",
      "\n",
      "import cats.data.NonEmptyList\n",
      "import cats.syntax.all._\n",
      "import github4s.domain._\n",
      "import io.circe.Decoder.Result\n",
      "import io.circe._\n",
      "import io.circe.generic.semiauto.deriveDecoder\n",
      "\n",
      "/**\n",
      " * Implicit circe decoders of domains objects\n",
      " */\n",
      "object Decoders {\n",
      "  final case class Author(\n",
      "      login: Option[String],\n",
      "      avatar_url: Option[String],\n",
      "      html_url: Option[String]\n",
      "  )\n",
      "\n",
      "  implicit val decodeAuthor: Decoder[Author] = deriveDecoder[Author]\n",
      "\n",
      "  implicit val decodeCommit: Decoder[Commit] = Decoder.instance { c =>\n",
      "    for {\n",
      "      sha     <- c.downField(\"sha\").as[String]\n",
      "      message <- c.downField(\"commit\").downField(\"message\").as[String]\n",
      "      date    <- c.downField(\"commit\").downField(\"author\").downField(\"date\").as[String]\n",
      "      url     <- c.downField(\"html_url\").as[String]\n",
      "      author  <- c.downField(\"author\").as[Option[Author]]\n",
      "    } yield Commit(\n",
      "      sha = sha,\n",
      "      message = message,\n",
      "      date = date,\n",
      "      url = url,\n",
      "      login = author.flatMap(_.login),\n",
      "      avatar_url = author.flatMap(_.avatar_url),\n",
      "      author_url = author.flatMap(_.html_url)\n",
      "    )\n",
      "  }\n",
      "\n",
      "  implicit val decodeBranch: Decoder[Branch] = Decoder.instance { c =>\n",
      "    for {\n",
      "      name            <- c.downField(\"name\").as[String]\n",
      "      commit          <- c.downField(\"commit\").as[BranchCommit]\n",
      "      branchProtected <- c.downField(\"protected\").as[Option[Boolean]]\n",
      "      protection_url  <- c.downField(\"protection_url\").as[Option[String]]\n",
      "    } yield Branch(\n",
      "      name = name,\n",
      "      commit = commit,\n",
      "      `protected` = branchProtected,\n",
      "      protection_url = protection_url\n",
      "    )\n",
      "  }\n",
      "\n",
      "  implicit val decodeBranchCommit: Decoder[BranchCommit] = Decoder.instance { c =>\n",
      "    for {\n",
      "      url <- c.downField(\"url\").as[String]\n",
      "      sha <- c.downField(\"sha\").as[String]\n",
      "    } yield BranchCommit(\n",
      "      url = url,\n",
      "      sha = sha\n",
      "    )\n",
      "  }\n",
      "\n",
      "  def readRepoUrls(c: HCursor): Either[DecodingFailure, Map[String, String]] =\n",
      "    RepoUrlKeys.allFields\n",
      "      .traverse(name => c.downField(name).as[Option[String]].map(_.map(value => name -> value)))\n",
      "      .map(_.flatten.toMap)\n",
      "\n",
      "  implicit val decodeStatusRepository: Decoder[StatusRepository] = {\n",
      "    Decoder.instance { c =>\n",
      "      for {\n",
      "        id          <- c.downField(\"id\").as[Long]\n",
      "        name        <- c.downField(\"name\").as[String]\n",
      "        full_name   <- c.downField(\"full_name\").as[String]\n",
      "        owner       <- c.downField(\"owner\").as[Option[User]]\n",
      "        priv        <- c.downField(\"private\").as[Boolean]\n",
      "        description <- c.downField(\"description\").as[Option[String]]\n",
      "        fork        <- c.downField(\"fork\").as[Boolean]\n",
      "        repoUrls    <- readRepoUrls(c)\n",
      "      } yield StatusRepository(\n",
      "        id = id,\n",
      "        name = name,\n",
      "        full_name = full_name,\n",
      "        owner = owner,\n",
      "        `private` = priv,\n",
      "        description = description,\n",
      "        fork = fork,\n",
      "        urls = repoUrls\n",
      "      )\n",
      "    }\n",
      "  }\n",
      "\n",
      "  implicit val decodeRepositoryBase: Decoder[RepositoryBase] = {\n",
      "\n",
      "    Decoder.instance { c =>\n",
      "      for {\n",
      "        id                <- c.downField(\"id\").as[Long]\n",
      "        name              <- c.downField(\"name\").as[String]\n",
      "        full_name         <- c.downField(\"full_name\").as[String]\n",
      "        owner             <- c.downField(\"owner\").as[User]\n",
      "        priv              <- c.downField(\"private\").as[Boolean]\n",
      "        description       <- c.downField(\"description\").as[Option[String]]\n",
      "        fork              <- c.downField(\"fork\").as[Boolean]\n",
      "        archived          <- c.downField(\"archived\").as[Boolean]\n",
      "        created_at        <- c.downField(\"created_at\").as[String]\n",
      "        updated_at        <- c.downField(\"updated_at\").as[String]\n",
      "        pushed_at         <- c.downField(\"pushed_at\").as[String]\n",
      "        homepage          <- c.downField(\"homepage\").as[Option[String]]\n",
      "        language          <- c.downField(\"language\").as[Option[String]]\n",
      "        organization      <- c.downField(\"organization\").as[Option[User]]\n",
      "        size              <- c.downField(\"size\").as[Int]\n",
      "        stargazers_count  <- c.downField(\"stargazers_count\").as[Int]\n",
      "        watchers_count    <- c.downField(\"watchers_count\").as[Int]\n",
      "        forks_count       <- c.downField(\"forks_count\").as[Int]\n",
      "        open_issues_count <- c.downField(\"open_issues_count\").as[Int]\n",
      "        open_issues       <- c.downField(\"open_issues\").as[Option[Int]]\n",
      "        watchers          <- c.downField(\"watchers\").as[Option[Int]]\n",
      "        network_count     <- c.downField(\"network_count\").as[Option[Int]]\n",
      "        subscribers_count <- c.downField(\"subscribers_count\").as[Option[Int]]\n",
      "        has_issues        <- c.downField(\"has_issues\").as[Boolean]\n",
      "        has_downloads     <- c.downField(\"has_downloads\").as[Boolean]\n",
      "        has_wiki          <- c.downField(\"has_wiki\").as[Boolean]\n",
      "        has_pages         <- c.downField(\"has_pages\").as[Boolean]\n",
      "        url               <- c.downField(\"url\").as[String]\n",
      "        html_url          <- c.downField(\"html_url\").as[String]\n",
      "        git_url           <- c.downField(\"git_url\").as[String]\n",
      "        ssh_url           <- c.downField(\"ssh_url\").as[String]\n",
      "        clone_url         <- c.downField(\"clone_url\").as[String]\n",
      "        svn_url           <- c.downField(\"svn_url\").as[String]\n",
      "        permissions       <- c.downField(\"permissions\").as[Option[RepoPermissions]]\n",
      "        repoUrls          <- readRepoUrls(c)\n",
      "      } yield RepositoryBase(\n",
      "        id = id,\n",
      "        name = name,\n",
      "        full_name = full_name,\n",
      "        owner = owner,\n",
      "        `private` = priv,\n",
      "        description = description,\n",
      "        fork = fork,\n",
      "        archived = archived,\n",
      "        created_at = created_at,\n",
      "        updated_at = updated_at,\n",
      "        pushed_at = pushed_at,\n",
      "        homepage = homepage,\n",
      "        language = language,\n",
      "        organization = organization,\n",
      "        permissions = permissions,\n",
      "        status = RepoStatus(\n",
      "          size = size,\n",
      "          stargazers_count = stargazers_count,\n",
      "          watchers_count = watchers_count,\n",
      "          forks_count = forks_count,\n",
      "          open_issues_count = open_issues_count,\n",
      "          open_issues = open_issues,\n",
      "          watchers = watchers,\n",
      "          network_count = network_count,\n",
      "          subscribers_count = subscribers_count,\n",
      "          has_issues = has_issues,\n",
      "          has_downloads = has_downloads,\n",
      "          has_wiki = has_wiki,\n",
      "          has_pages = has_pages\n",
      "        ),\n",
      "        urls = RepoUrls(\n",
      "          url = url,\n",
      "          html_url = html_url,\n",
      "          git_url = git_url,\n",
      "          ssh_url = ssh_url,\n",
      "          clone_url = clone_url,\n",
      "          svn_url = svn_url,\n",
      "          otherUrls = repoUrls\n",
      "        )\n",
      "      )\n",
      "    }\n",
      "  }\n",
      "\n",
      "  implicit val decodeRepository: Decoder[Repository] = for {\n",
      "    base   <- decodeRepositoryBase\n",
      "    parent <- Decoder[Option[RepositoryBase]].at(\"parent\")\n",
      "    source <- Decoder[Option[RepositoryBase]].at(\"source\")\n",
      "  } yield Repository.fromBaseRepos(base, parent, source)\n",
      "\n",
      "  implicit val decodePRStatus: Decoder[PullRequestReviewState] =\n",
      "    Decoder.decodeString.emap {\n",
      "      case PRRStateApproved.value         => PRRStateApproved.asRight\n",
      "      case PRRStateChangesRequested.value => PRRStateChangesRequested.asRight\n",
      "      case PRRStateCommented.value        => PRRStateCommented.asRight\n",
      "      case PRRStatePending.value          => PRRStatePending.asRight\n",
      "      case PRRStateDismissed.value        => PRRStateDismissed.asRight\n",
      "      case other                          => s\"Unknown pull request review state: $other\".asLeft\n",
      "    }\n",
      "\n",
      "  implicit val decodeGistFile: Decoder[GistFile] = Decoder.instance { c =>\n",
      "    for {\n",
      "      content <- c.downField(\"content\").as[String]\n",
      "    } yield GistFile(\n",
      "      content = content\n",
      "    )\n",
      "  }\n",
      "\n",
      "  implicit val decodeGist: Decoder[Gist] = Decoder.instance { c =>\n",
      "    for {\n",
      "      url         <- c.downField(\"url\").as[String]\n",
      "      id          <- c.downField(\"id\").as[String]\n",
      "      description <- c.downField(\"description\").as[String]\n",
      "      public      <- c.downField(\"public\").as[Boolean]\n",
      "      files       <- c.downField(\"files\").as[Map[String, GistFile]]\n",
      "    } yield Gist(\n",
      "      url = url,\n",
      "      id = id,\n",
      "      description = description,\n",
      "      public = public,\n",
      "      files = files\n",
      "    )\n",
      "  }\n",
      "\n",
      "  implicit val decodeStarredRepository: Decoder[StarredRepository] =\n",
      "    Decoder[Repository]\n",
      "      .map(StarredRepository(_))\n",
      "      .or(\n",
      "        Decoder.instance(c =>\n",
      "          for {\n",
      "            starred_at <- c.downField(\"starred_at\").as[String]\n",
      "            repo       <- c.downField(\"repo\").as[Repository]\n",
      "          } yield StarredRepository(repo, Some(starred_at))\n",
      "        )\n",
      "      )\n",
      "\n",
      "  implicit val decoderCreatePullRequestData: Decoder[CreatePullRequestData] =\n",
      "    deriveDecoder[CreatePullRequestData]\n",
      "  implicit val decoderCreatePullRequestIssue: Decoder[CreatePullRequestIssue] =\n",
      "    deriveDecoder[CreatePullRequestIssue]\n",
      "  implicit val decoderNewBlobRequest: Decoder[NewBlobRequest]   = deriveDecoder[NewBlobRequest]\n",
      "  implicit val decoderNewGistRequest: Decoder[NewGistRequest]   = deriveDecoder[NewGistRequest]\n",
      "  implicit val decoderNewIssueRequest: Decoder[NewIssueRequest] = deriveDecoder[NewIssueRequest]\n",
      "  implicit val decoderNewReleaseRequest: Decoder[NewReleaseRequest] =\n",
      "    deriveDecoder[NewReleaseRequest]\n",
      "  implicit val decoderSubscriptionRequest: Decoder[SubscriptionRequest] =\n",
      "    deriveDecoder[SubscriptionRequest]\n",
      "  implicit val decoderTreeData: Decoder[TreeData] = {\n",
      "    val sha  = deriveDecoder[TreeDataSha]\n",
      "    val blob = deriveDecoder[TreeDataBlob]\n",
      "    sha.widen[TreeData] or blob.widen[TreeData]\n",
      "  }\n",
      "  implicit val decoderUpdateReferenceRequest: Decoder[UpdateReferenceRequest] =\n",
      "    deriveDecoder[UpdateReferenceRequest]\n",
      "  implicit val decoderWriteFileRequest: Decoder[WriteFileRequest] = deriveDecoder[WriteFileRequest]\n",
      "  implicit val decoderReviewersRequest: Decoder[ReviewersRequest] = deriveDecoder[ReviewersRequest]\n",
      "  implicit val decoderNewStatusRequest: Decoder[NewStatusRequest] = deriveDecoder[NewStatusRequest]\n",
      "  implicit val decoderNewTagRequest: Decoder[NewTagRequest]       = deriveDecoder[NewTagRequest]\n",
      "  implicit val decoderNewTreeRequest: Decoder[NewTreeRequest]     = deriveDecoder[NewTreeRequest]\n",
      "  implicit val decoderNewCommitRequest: Decoder[NewCommitRequest] = deriveDecoder[NewCommitRequest]\n",
      "\n",
      "  implicit def decodeNonEmptyList[T](implicit D: Decoder[T]): Decoder[NonEmptyList[T]] = {\n",
      "\n",
      "    def decodeCursors(cursors: List[HCursor]): Result[NonEmptyList[T]] =\n",
      "      cursors.toNel\n",
      "        .toRight(DecodingFailure(\"Empty Response\", Nil))\n",
      "        .flatMap(nelCursors => nelCursors.traverse(_.as[T]))\n",
      "\n",
      "    Decoder.instance { c =>\n",
      "      c.as[T] match {\n",
      "        case Right(r) => Right(NonEmptyList(r, Nil))\n",
      "        case Left(_)  => c.as[List[HCursor]] flatMap decodeCursors\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  implicit val decoderCommitter: Decoder[Committer] = deriveDecoder[Committer]\n",
      "  implicit val decoderWriteResponseCommit: Decoder[WriteResponseCommit] =\n",
      "    deriveDecoder[WriteResponseCommit]\n",
      "  implicit val decoderWriteFileResponse: Decoder[WriteFileResponse] =\n",
      "    deriveDecoder[WriteFileResponse]\n",
      "  implicit val decoderPullRequestFile: Decoder[PullRequestFile] = deriveDecoder[PullRequestFile]\n",
      "  implicit val decoderPullRequestReview: Decoder[PullRequestReview] =\n",
      "    deriveDecoder[PullRequestReview]\n",
      "  implicit val decoderUser: Decoder[User] = deriveDecoder[User]\n",
      "\n",
      "  implicit val decoderRepoPermissions: Decoder[RepoPermissions] = deriveDecoder[RepoPermissions]\n",
      "  implicit val decoderPullRequestBase: Decoder[PullRequestBase] = deriveDecoder[PullRequestBase]\n",
      "\n",
      "  implicit val decoderPullRequest: Decoder[PullRequest]           = deriveDecoder[PullRequest]\n",
      "  implicit val decoderRefObject: Decoder[RefObject]               = deriveDecoder[RefObject]\n",
      "  implicit val decoderRef: Decoder[Ref]                           = deriveDecoder[Ref]\n",
      "  implicit val decoderRefAuthor: Decoder[RefAuthor]               = deriveDecoder[RefAuthor]\n",
      "  implicit val decoderRefCommit: Decoder[RefCommit]               = deriveDecoder[RefCommit]\n",
      "  implicit val decoderRefInfo: Decoder[RefInfo]                   = deriveDecoder[RefInfo]\n",
      "  implicit val decoderTreeDataResult: Decoder[TreeDataResult]     = deriveDecoder[TreeDataResult]\n",
      "  implicit val decoderTreeResult: Decoder[TreeResult]             = deriveDecoder[TreeResult]\n",
      "  implicit val decoderTag: Decoder[Tag]                           = deriveDecoder[Tag]\n",
      "  implicit val decoderLabel: Decoder[Label]                       = deriveDecoder[Label]\n",
      "  implicit val decoderIssuePullRequest: Decoder[IssuePullRequest] = deriveDecoder[IssuePullRequest]\n",
      "  implicit val decoderIssue: Decoder[Issue]                       = deriveDecoder[Issue]\n",
      "  implicit val decoderSearchIssuesResult: Decoder[SearchIssuesResult] =\n",
      "    deriveDecoder[SearchIssuesResult]\n",
      "  implicit val decoderSearchReposResult: Decoder[SearchReposResult] = deriveDecoder\n",
      "  implicit val decoderComment: Decoder[Comment]                     = deriveDecoder[Comment]\n",
      "  implicit val decoderStatus: Decoder[Status]                       = deriveDecoder[Status]\n",
      "  implicit val decoderCombinedStatus: Decoder[CombinedStatus]       = deriveDecoder[CombinedStatus]\n",
      "  implicit val decoderContent: Decoder[Content]                     = deriveDecoder[Content]\n",
      "  implicit val decoderBlobContent: Decoder[BlobContent]             = deriveDecoder[BlobContent]\n",
      "  implicit val decoderSubscription: Decoder[Subscription]           = deriveDecoder[Subscription]\n",
      "  implicit val decoderOAuthToken: Decoder[OAuthToken]               = deriveDecoder[OAuthToken]\n",
      "  implicit val decoderRelease: Decoder[Release]                     = deriveDecoder[Release]\n",
      "  implicit val decoderUserRepoPermission: Decoder[UserRepoPermission] =\n",
      "    deriveDecoder[UserRepoPermission]\n",
      "\n",
      "  implicit val decodeStargazer: Decoder[Stargazer] =\n",
      "    decoderUser\n",
      "      .map(Stargazer(_))\n",
      "      .or(\n",
      "        Decoder.instance(c =>\n",
      "          for {\n",
      "            starred_at <- c.downField(\"starred_at\").as[String]\n",
      "            user       <- c.downField(\"user\").as[User]\n",
      "          } yield Stargazer(user, Some(starred_at))\n",
      "        )\n",
      "      )\n",
      "\n",
      "  implicit val decodeTeam: Decoder[Team]           = deriveDecoder[Team]\n",
      "  implicit val decodeCreator: Decoder[Creator]     = deriveDecoder[Creator]\n",
      "  implicit val decodeMilestone: Decoder[Milestone] = deriveDecoder[Milestone]\n",
      "  implicit val decodeProject: Decoder[Project]     = deriveDecoder[Project]\n",
      "  implicit val decodeColumn: Decoder[Column]       = deriveDecoder[Column]\n",
      "  implicit val decodeCard: Decoder[Card]           = deriveDecoder[Card]\n",
      "\n",
      "  implicit val decodeReviewers: Decoder[ReviewersResponse] =\n",
      "    deriveDecoder[ReviewersResponse]\n",
      "  implicit val decoderCommentData: Decoder[CommentData] = deriveDecoder[CommentData]\n",
      "  implicit val decoderPullRequestReviewEvent: Decoder[PullRequestReviewEvent] =\n",
      "    Decoder[String].emap {\n",
      "      case s if s == PRREventApprove.value        => Right(PRREventApprove)\n",
      "      case s if s == PRREventRequestChanges.value => Right(PRREventRequestChanges)\n",
      "      case s if s == PRREventComment.value        => Right(PRREventComment)\n",
      "      case s if s == PRREventPending.value        => Right(PRREventPending)\n",
      "      case other                                  => Left(s\"Bad event: $other\")\n",
      "    }\n",
      "  implicit val decoderCreateReviewComment: Decoder[CreateReviewComment] =\n",
      "    deriveDecoder[CreateReviewComment]\n",
      "  implicit val decoderCreatePRReviewRequest: Decoder[CreatePRReviewRequest] =\n",
      "    deriveDecoder[CreatePRReviewRequest]\n",
      "  implicit val decoderCreatePullRequest: Decoder[CreatePullRequest] = {\n",
      "    val data  = deriveDecoder[CreatePullRequestData]\n",
      "    val issue = deriveDecoder[CreatePullRequestIssue]\n",
      "    data.widen[CreatePullRequest] or issue.widen[CreatePullRequest]\n",
      "  }\n",
      "  implicit val decoderCreateReferenceRequest: Decoder[CreateReferenceRequest] =\n",
      "    deriveDecoder[CreateReferenceRequest]\n",
      "  implicit val decoderDeleteFileRequest: Decoder[DeleteFileRequest] =\n",
      "    deriveDecoder[DeleteFileRequest]\n",
      "  implicit val decoderEditGistFile: Decoder[EditGistFile]         = deriveDecoder[EditGistFile]\n",
      "  implicit val decoderEditGistRequest: Decoder[EditGistRequest]   = deriveDecoder[EditGistRequest]\n",
      "  implicit val decoderEditIssueRequest: Decoder[EditIssueRequest] = deriveDecoder[EditIssueRequest]\n",
      "  implicit val decoderMilestoneData: Decoder[MilestoneData]       = deriveDecoder[MilestoneData]\n",
      "}\n",
      "\n",
      "Average Line Length: 44.12096774193548\n",
      "\n",
      "Example 7:\n",
      "import dotty.tools.sbtplugin.DottyPlugin.autoImport._\n",
      "import sbt._\n",
      "import Keys._\n",
      "import com.typesafe.tools.mima.plugin.MimaKeys.{mimaPreviousArtifacts, mimaCurrentClassfiles, mimaBinaryIssueFilters}\n",
      "import com.typesafe.tools.mima.core._\n",
      "import com.typesafe.tools.mima.core.ProblemFilters._\n",
      "import com.typesafe.sbt.osgi.OsgiKeys\n",
      "import com.typesafe.sbt.osgi.SbtOsgi\n",
      "import com.typesafe.sbt.osgi.SbtOsgi.autoImport._\n",
      "import org.scalajs.sbtplugin.ScalaJSPlugin\n",
      "import org.scalajs.sbtplugin.ScalaJSPlugin.autoImport.{scalaJSLinkerConfig, jsEnv}\n",
      "\n",
      "trait DottyBuild { this: BuildCommons =>\n",
      "\n",
      "  // List of available night build at https://repo1.maven.org/maven2/ch/epfl/lamp/dotty-compiler_0.27/\n",
      "  // lazy val dottyVersion = dottyLatestNightlyBuild.get\n",
      "  lazy val dottyVersion = System.getProperty(\"scalatest.dottyVersion\", \"3.0.0-RC2\")\n",
      "  lazy val dottySettings = List(\n",
      "    scalaVersion := dottyVersion,\n",
      "    scalacOptions ++= List(\"-language:implicitConversions\", \"-noindent\", \"-Xprint-suspension\")\n",
      "  )\n",
      "\n",
      "  // https://github.com/sbt/sbt/issues/2205#issuecomment-144375501\n",
      "  private lazy val packageManagedSources =\n",
      "    mappings in (Compile, packageSrc) ++= { // publish generated sources\n",
      "      val srcs = (managedSources in Compile).value\n",
      "      val sdirs = (managedSourceDirectories in Compile).value\n",
      "      val base = baseDirectory.value\n",
      "      import Path._\n",
      "      (srcs --- sdirs --- base) pair (relativeTo(sdirs) | relativeTo(base) | flat)\n",
      "    }\n",
      "\n",
      "  lazy val scalacticDotty = project.in(file(\"dotty/scalactic\"))\n",
      "    .enablePlugins(SbtOsgi)\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(scalacticDocSettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Scalactic\",\n",
      "      organization := \"org.scalactic\",\n",
      "      moduleName := \"scalactic\",\n",
      "      initialCommands in console := \"import org.scalactic._\",\n",
      "      packageManagedSources,\n",
      "      sourceGenerators in Compile += {\n",
      "        Def.task {\n",
      "          // From scalactic-macro\n",
      "          GenScalacticDotty.genMacroScala((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "          ScalacticGenResourcesJVM.genResources((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenAnyVals.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\" / \"anyvals\", version.value, scalaVersion.value, true) ++\n",
      "          GenEvery.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenColCompatHelper.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          // end from scalactic-macro\n",
      "          GenScalacticDotty.genScala((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "          GenVersions.genScalacticVersions((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          ScalacticGenResourcesJVM.genFailureMessages((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenArrayHelper.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value)\n",
      "        }.taskValue\n",
      "      },\n",
      "      resourceGenerators in Compile += Def.task {\n",
      "        GenScalacticDotty.genResource((resourceManaged in Compile).value)\n",
      "      }.taskValue,\n",
      "      //scalacticDocSourcesSetting,\n",
      "      //docTaskSetting,\n",
      "      publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "      mimaPreviousArtifacts := Set(organization.value %% name.value % previousReleaseVersion),\n",
      "      mimaCurrentClassfiles := (classDirectory in Compile).value.getParentFile / (name.value + \"_\" + scalaBinaryVersion.value + \"-\" + releaseVersion + \".jar\")\n",
      "    ).settings(osgiSettings: _*).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\n",
      "      \"org.scalactic\",\n",
      "      \"org.scalactic.anyvals\",\n",
      "      \"org.scalactic.exceptions\",\n",
      "      \"org.scalactic.source\"\n",
      "    ),\n",
      "    OsgiKeys.importPackage := Seq(\n",
      "      \"org.scalatest.*\",\n",
      "      \"org.scalactic.*\",\n",
      "      \"scala.util.parsing.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.xml.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.*;version=\\\"$<range;[==,=+);$<replace;\"+scalaBinaryVersion.value+\";-;.>>\\\"\",\n",
      "      \"*;resolution:=optional\"\n",
      "    ),\n",
      "    OsgiKeys.additionalHeaders:= Map(\n",
      "      \"Bundle-Name\" -> \"Scalactic\",\n",
      "      \"Bundle-Description\" -> \"Scalactic is an open-source library for Scala projects.\",\n",
      "      \"Bundle-DocURL\" -> \"http://www.scalactic.org/\",\n",
      "      \"Bundle-Vendor\" -> \"Artima, Inc.\"\n",
      "    )\n",
      "  )\n",
      "\n",
      "  lazy val scalacticDottyJS = project.in(file(\"dotty/scalactic.js\"))\n",
      "    .enablePlugins(SbtOsgi)\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(scalacticDocSettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Scalactic\",\n",
      "      organization := \"org.scalactic\",\n",
      "      moduleName := \"scalactic\",\n",
      "      initialCommands in console := \"import org.scalactic._\",\n",
      "      packageManagedSources,\n",
      "      sourceGenerators in Compile += {\n",
      "        Def.task {\n",
      "          // From scalactic-macro\n",
      "          GenScalacticDotty.genMacroScala((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "          ScalacticGenResourcesJSVM.genResources((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenAnyVals.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\" / \"anyvals\", version.value, scalaVersion.value, true) ++\n",
      "          GenEvery.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenColCompatHelper.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          // end from scalactic-macro\n",
      "          GenScalacticDotty.genScalaJS((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "          GenVersions.genScalacticVersions((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          ScalacticGenResourcesJSVM.genFailureMessages((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value) ++\n",
      "          GenArrayHelper.genMain((sourceManaged in Compile).value / \"org\" / \"scalactic\", version.value, scalaVersion.value)\n",
      "        }.taskValue\n",
      "      },\n",
      "      resourceGenerators in Compile += Def.task {\n",
      "        GenScalacticDotty.genResource((resourceManaged in Compile).value)\n",
      "      }.taskValue,\n",
      "      //scalacticDocSourcesSetting,\n",
      "      //docTaskSetting,\n",
      "      publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "      mimaPreviousArtifacts := Set(organization.value %% name.value % previousReleaseVersion),\n",
      "      mimaCurrentClassfiles := (classDirectory in Compile).value.getParentFile / (name.value + \"_\" + scalaBinaryVersion.value + \"-\" + releaseVersion + \".jar\")\n",
      "    ).settings(osgiSettings: _*).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\n",
      "      \"org.scalactic\",\n",
      "      \"org.scalactic.anyvals\",\n",
      "      \"org.scalactic.exceptions\",\n",
      "      \"org.scalactic.source\"\n",
      "    ),\n",
      "    OsgiKeys.importPackage := Seq(\n",
      "      \"org.scalatest.*\",\n",
      "      \"org.scalactic.*\",\n",
      "      \"scala.util.parsing.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.xml.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.*;version=\\\"$<range;[==,=+);$<replace;\"+scalaBinaryVersion.value+\";-;.>>\\\"\",\n",
      "      \"*;resolution:=optional\"\n",
      "    ),\n",
      "    OsgiKeys.additionalHeaders:= Map(\n",
      "      \"Bundle-Name\" -> \"Scalactic\",\n",
      "      \"Bundle-Description\" -> \"Scalactic is an open-source library for Scala projects.\",\n",
      "      \"Bundle-DocURL\" -> \"http://www.scalactic.org/\",\n",
      "      \"Bundle-Vendor\" -> \"Artima, Inc.\"\n",
      "    )\n",
      "  ).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestCoreDotty = project.in(file(\"dotty/core\"))\n",
      "    .enablePlugins(SbtOsgi)\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Core Dotty\",\n",
      "      organization := \"org.scalatest\",\n",
      "      moduleName := \"scalatest-core\",\n",
      "      initialCommands in console := \"\"\"|import org.scalatest._\n",
      "                                       |import org.scalactic._\n",
      "                                       |import Matchers._\"\"\".stripMargin,\n",
      "      libraryDependencies ++= scalaXmlDependency(scalaVersion.value),\n",
      "      libraryDependencies ++= scalatestLibraryDependencies,\n",
      "      packageManagedSources,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenModulesDotty.genScalaTestCore((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenVersions.genScalaTestVersions((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        ScalaTestGenResourcesJVM.genResources((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        ScalaTestGenResourcesJVM.genFailureMessages((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)  ++\n",
      "        GenConfigMap.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      javaSourceManaged := target.value / \"java\",\n",
      "      managedSourceDirectories in Compile += javaSourceManaged.value,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenScalaTestDotty.genJava((javaSourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      resourceGenerators in Compile += Def.task {\n",
      "          GenScalaTestDotty.genHtml((resourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenTable.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        GenCompatibleClasses.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\" / \"tools\", version.value, scalaVersion.value)\n",
      "        //GenSafeStyles.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      //scalatestJSDocTaskSetting,\n",
      "      publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "      mimaPreviousArtifacts := Set(organization.value %% name.value % previousReleaseVersion),\n",
      "      mimaCurrentClassfiles := (classDirectory in Compile).value.getParentFile / (name.value + \"_\" + scalaBinaryVersion.value + \"-\" + releaseVersion + \".jar\"),\n",
      "      mimaBinaryIssueFilters ++= {\n",
      "        Seq(\n",
      "          exclude[MissingClassProblem](\"org.scalatest.tools.SbtCommandParser$\"),\n",
      "          exclude[MissingClassProblem](\"org.scalatest.tools.SbtCommandParser\")\n",
      "        )\n",
      "      }\n",
      "    ).settings(osgiSettings: _*).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\n",
      "      \"org.scalatest\", \n",
      "        \"org.scalatest.concurrent\",  \n",
      "        \"org.scalatest.enablers\",  \n",
      "        \"org.scalatest.exceptions\",  \n",
      "        \"org.scalatest.events\", \n",
      "        \"org.scalatest.fixture\",  \n",
      "        \"org.scalatest.prop\", \n",
      "        \"org.scalatest.tags\", \n",
      "        \"org.scalatest.tagobjects\", \n",
      "        \"org.scalatest.time\", \n",
      "        \"org.scalatest.tools\",  \n",
      "        \"org.scalatest.verbs\"\n",
      "    ),\n",
      "    OsgiKeys.importPackage := Seq(\n",
      "      \"org.scalatest.*\",\n",
      "      \"org.scalactic.*\",\n",
      "      \"scala.util.parsing.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.xml.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.*;version=\\\"$<range;[==,=+);$<replace;\"+scalaBinaryVersion.value+\";-;.>>\\\"\",\n",
      "      \"*;resolution:=optional\"\n",
      "    ),\n",
      "    OsgiKeys.additionalHeaders:= Map(\n",
      "      \"Bundle-Name\" -> \"ScalaTest Core Dotty\",\n",
      "      \"Bundle-Description\" -> \"ScalaTest is an open-source test framework for the Javascript Platform designed to increase your productivity by letting you write fewer lines of test code that more clearly reveal your intent.\",\n",
      "      \"Bundle-DocURL\" -> \"http://www.scalatest.org/\",\n",
      "      \"Bundle-Vendor\" -> \"Artima, Inc.\",\n",
      "      \"Main-Class\" -> \"org.scalatest.tools.Runner\"\n",
      "    )\n",
      "  ).dependsOn(scalacticDotty, scalatestCompatible)\n",
      "\n",
      "  lazy val scalatestCoreDottyJS = project.in(file(\"dotty/core.js\"))\n",
      "    .enablePlugins(SbtOsgi)\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Core Dotty\",\n",
      "      organization := \"org.scalatest\",\n",
      "      moduleName := \"scalatest-core\",\n",
      "      initialCommands in console := \"\"\"|import org.scalatest._\n",
      "                                       |import org.scalactic._\n",
      "                                       |import Matchers._\"\"\".stripMargin,\n",
      "      libraryDependencies ++= scalaXmlDependency(scalaVersion.value),\n",
      "      libraryDependencies += (\"org.scala-js\" %% \"scalajs-test-interface\" % scalaJSVersion).withDottyCompat(dottyVersion), \n",
      "      packageManagedSources,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenModulesDotty.genScalaTestCoreJS((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenScalaTestDotty.genScalaJS((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenVersions.genScalaTestVersions((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        ScalaTestGenResourcesJSVM.genResources((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value) ++\n",
      "        ScalaTestGenResourcesJSVM.genFailureMessages((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)  ++\n",
      "        GenConfigMap.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      javaSourceManaged := target.value / \"java\",\n",
      "      managedSourceDirectories in Compile += javaSourceManaged.value,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenScalaTestDotty.genJava((javaSourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      resourceGenerators in Compile += Def.task {\n",
      "          GenScalaTestDotty.genHtml((resourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenTable.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "        //GenSafeStyles.genMain((sourceManaged in Compile).value / \"org\" / \"scalatest\", version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      //scalatestJSDocTaskSetting,\n",
      "      publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "      mimaPreviousArtifacts := Set(organization.value %% name.value % previousReleaseVersion),\n",
      "      mimaCurrentClassfiles := (classDirectory in Compile).value.getParentFile / (name.value + \"_\" + scalaBinaryVersion.value + \"-\" + releaseVersion + \".jar\"),\n",
      "      mimaBinaryIssueFilters ++= {\n",
      "        Seq(\n",
      "          exclude[MissingClassProblem](\"org.scalatest.tools.SbtCommandParser$\"),\n",
      "          exclude[MissingClassProblem](\"org.scalatest.tools.SbtCommandParser\")\n",
      "        )\n",
      "      }\n",
      "    ).settings(osgiSettings: _*).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\n",
      "      \"org.scalatest\", \n",
      "      \"org.scalatest.compatible\", \n",
      "      \"org.scalatest.concurrent\",  \n",
      "      \"org.scalatest.enablers\",  \n",
      "      \"org.scalatest.exceptions\",  \n",
      "      \"org.scalatest.events\", \n",
      "      \"org.scalatest.fixture\",  \n",
      "      \"org.scalatest.prop\", \n",
      "      \"org.scalatest.tags\", \n",
      "      \"org.scalatest.tagobjects\", \n",
      "      \"org.scalatest.time\", \n",
      "      \"org.scalatest.tools\",  \n",
      "      \"org.scalatest.verbs\"\n",
      "    ),\n",
      "    OsgiKeys.importPackage := Seq(\n",
      "      \"org.scalatest.*\",\n",
      "      \"org.scalactic.*\",\n",
      "      \"scala.util.parsing.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.xml.*;version=\\\"$<range;[==,=+);$<replace;1.0.4;-;.>>\\\"\",\n",
      "      \"scala.*;version=\\\"$<range;[==,=+);$<replace;\"+scalaBinaryVersion.value+\";-;.>>\\\"\",\n",
      "      \"*;resolution:=optional\"\n",
      "    ),\n",
      "    OsgiKeys.additionalHeaders:= Map(\n",
      "      \"Bundle-Name\" -> \"ScalaTest Core Dotty\",\n",
      "      \"Bundle-Description\" -> \"ScalaTest is an open-source test framework for the Javascript Platform designed to increase your productivity by letting you write fewer lines of test code that more clearly reveal your intent.\",\n",
      "      \"Bundle-DocURL\" -> \"http://www.scalatest.org/\",\n",
      "      \"Bundle-Vendor\" -> \"Artima, Inc.\",\n",
      "      \"Main-Class\" -> \"org.scalatest.tools.Runner\"\n",
      "    )\n",
      "  ).dependsOn(scalacticDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  private implicit class DottyProjectEx(private val p: Project) {\n",
      "    /** common settings for all scalatest modules */\n",
      "    def scalatestModule(name: String, title: String): Project = p\n",
      "      .enablePlugins(SbtOsgi)\n",
      "      .settings(sharedSettings: _*)\n",
      "      .settings(dottySettings: _*)\n",
      "      .settings(\n",
      "        projectTitle := title,\n",
      "        organization := \"org.scalatest\",\n",
      "        moduleName := name,\n",
      "        packageManagedSources,\n",
      "        publishArtifact in (Compile, packageDoc) := false, // Temporary disable publishing of doc, can't get it to build.\n",
      "        osgiSettings,\n",
      "        OsgiKeys.additionalHeaders := Map(\n",
      "          \"Bundle-Name\" -> title,\n",
      "          \"Bundle-Description\" -> \"ScalaTest is an open-source test framework for the Javascript Platform designed to increase your productivity by letting you write fewer lines of test code that more clearly reveal your intent.\",\n",
      "          \"Bundle-DocURL\" -> \"http://www.scalatest.org/\",\n",
      "          \"Bundle-Vendor\" -> \"Artima, Inc.\"\n",
      "        ),\n",
      "      )\n",
      "\n",
      "    /** common settings for all scalatest sub modules (all modules, except the `scalatest` module) */\n",
      "    def scalatestSubModule(name: String, title: String, gen: GenModulesDotty.GenFn): Project =\n",
      "      scalatestModule(name, title).settings(\n",
      "        sourceGenerators in Compile += Def.task {\n",
      "          gen((sourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "        }.taskValue,\n",
      "        OsgiKeys.importPackage := Seq(\n",
      "          \"org.scalatest.*\",\n",
      "          \"*;resolution:=optional\"\n",
      "        ),\n",
      "      )\n",
      "\n",
      "    /** common settings for all scalatest `style` modules such as `featurespec`, `funsuite`,.. */\n",
      "    def scalatestStyleModule(style: String, title: String): Project =\n",
      "      scalatestSubModule(s\"scalatest-$style\", title, GenModulesDotty(style))\n",
      "        .settings(\n",
      "          OsgiKeys.exportPackage := Seq(s\"org.scalatest.$style\"),\n",
      "        ).dependsOn(scalatestCoreDotty)\n",
      "\n",
      "    /** common settings for all scalatest js `style` modules such as `featurespec`, `funsuite`,.. */\n",
      "    def scalatestStyleModuleJS(style: String, title: String): Project =\n",
      "      scalatestSubModule(s\"scalatest-$style\", title, GenModulesDotty.applyJS(style))\n",
      "        .settings(\n",
      "          OsgiKeys.exportPackage := Seq(s\"org.scalatest.$style\"),\n",
      "        ).dependsOn(scalatestCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "        \n",
      "  }\n",
      "  \n",
      "  lazy val scalatestFeatureSpecDotty = project.in(file(\"dotty/featurespec\"))\n",
      "    .scalatestStyleModule(\"featurespec\", \"ScalaTest FeatureSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestFeatureSpecDottyJS = project.in(file(\"dotty/featurespec.js\"))\n",
      "    .scalatestStyleModuleJS(\"featurespec\", \"ScalaTest FeatureSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestFlatSpecDotty = project.in(file(\"dotty/flatspec\"))\n",
      "    .scalatestStyleModule(\"flatspec\", \"ScalaTest FlatSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestFlatSpecDottyJS = project.in(file(\"dotty/flatspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"flatspec\", \"ScalaTest FlatSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestFreeSpecDotty = project.in(file(\"dotty/freespec\"))\n",
      "    .scalatestStyleModule(\"freespec\", \"ScalaTest FreeSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestFreeSpecDottyJS = project.in(file(\"dotty/freespec.js\"))\n",
      "    .scalatestStyleModuleJS(\"freespec\", \"ScalaTest FreeSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestFunSuiteDotty = project.in(file(\"dotty/funsuite\"))\n",
      "    .scalatestStyleModule(\"funsuite\", \"ScalaTest FunSuite Dotty\")\n",
      "\n",
      "  lazy val scalatestFunSuiteDottyJS = project.in(file(\"dotty/funsuite.js\"))\n",
      "    .scalatestStyleModuleJS(\"funsuite\", \"ScalaTest FunSuite Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestFunSpecDotty = project.in(file(\"dotty/funspec\"))\n",
      "    .scalatestStyleModule(\"funspec\", \"ScalaTest FunSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestFunSpecDottyJS = project.in(file(\"dotty/funspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"funspec\", \"ScalaTest FunSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestPropSpecDotty = project.in(file(\"dotty/propspec\"))\n",
      "    .scalatestStyleModule(\"propspec\", \"ScalaTest PropSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestPropSpecDottyJS = project.in(file(\"dotty/propspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"propspec\", \"ScalaTest PropSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestRefSpecDotty = project.in(file(\"dotty/refspec\"))\n",
      "    .scalatestStyleModule(\"refspec\", \"ScalaTest RefSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestRefSpecDottyJS = project.in(file(\"dotty/refspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"refspec\", \"ScalaTest RefSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestWordSpecDotty = project.in(file(\"dotty/wordspec\"))\n",
      "    .scalatestStyleModule(\"wordspec\", \"ScalaTest WordSpec Dotty\")\n",
      "\n",
      "  lazy val scalatestWordSpecDottyJS = project.in(file(\"dotty/wordspec.js\"))\n",
      "    .scalatestStyleModuleJS(\"wordspec\", \"ScalaTest WordSpec Dotty JS\")  \n",
      "\n",
      "  lazy val scalatestDiagramsDotty = project.in(file(\"dotty/diagrams\"))\n",
      "    .scalatestStyleModule(\"diagrams\", \"ScalaTest Diagrams Dotty\")\n",
      "\n",
      "  lazy val scalatestDiagramsDottyJS = project.in(file(\"dotty/diagrams.js\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-diagrams\", \n",
      "      \"ScalaTest Diagrams Dotty JS\", \n",
      "      (targetDir, version, scalaVersion) =>\n",
      "        GenScalaTestDotty.genDiagramsScalaJS(targetDir / \"org\" / \"scalatest\", version, scalaVersion)\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\n",
      "        \"org.scalatest\", \n",
      "        \"org.scalatest.diagrams\"\n",
      "      ),\n",
      "    ).dependsOn(scalatestCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestMatchersCoreDotty = project.in(file(\"dotty/matchers-core\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-matchers-core\",\n",
      "      \"ScalaTest Matchers Core Dotty\",\n",
      "      (targetDir, version, scalaVersion) => {\n",
      "        GenModulesDotty.genScalaTestMatchersCore(targetDir, version, scalaVersion) ++\n",
      "          GenFactoriesDotty.genMain(targetDir / \"org\" / \"scalatest\" / \"matchers\" / \"dsl\", version, scalaVersion)\n",
      "      }\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\n",
      "        \"org.scalatest.matchers\",\n",
      "        \"org.scalatest.matchers.dsl\"\n",
      "      ),\n",
      "    ).dependsOn(scalatestCoreDotty)\n",
      "\n",
      "  lazy val scalatestMatchersCoreDottyJS = project.in(file(\"dotty/matchers-core.js\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-matchers-core\",\n",
      "      \"ScalaTest Matchers Core Dotty JS\",\n",
      "      (targetDir, version, scalaVersion) => {\n",
      "        GenModulesDotty.genScalaTestMatchersCoreJS(targetDir, version, scalaVersion) ++\n",
      "        GenScalaTestDotty.genMatchersCoreScalaJS(targetDir, version, scalaVersion) ++\n",
      "        GenFactoriesDotty.genMain(targetDir / \"org\" / \"scalatest\" / \"matchers\" / \"dsl\", version, scalaVersion)\n",
      "      }\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\n",
      "        \"org.scalatest.matchers\",\n",
      "        \"org.scalatest.matchers.dsl\"\n",
      "      ),\n",
      "    ).dependsOn(scalatestCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestShouldMatchersDotty = project.in(file(\"dotty/shouldmatchers\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-shouldmatchers\",\n",
      "      \"ScalaTest Should Matchers Dotty\",\n",
      "      GenModulesDotty.genScalaTestShouldMatchers\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\"org.scalatest.matchers.should\"),\n",
      "    ).dependsOn(scalatestMatchersCoreDotty)\n",
      "\n",
      "  lazy val scalatestShouldMatchersDottyJS = project.in(file(\"dotty/shouldmatchers.js\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-shouldmatchers\",\n",
      "      \"ScalaTest Should Matchers Dotty JS\",\n",
      "      (targetDir, version, scalaVersion) => {\n",
      "        GenModulesDotty.genScalaTestShouldMatchersJS(targetDir, version, scalaVersion) ++ \n",
      "        GenScalaTestDotty.genShouldMatchersScalaJS(targetDir, version, scalaVersion)\n",
      "      }\n",
      "    ).settings(\n",
      "      OsgiKeys.exportPackage := Seq(\"org.scalatest.matchers.should\"),\n",
      "    ).dependsOn(scalatestMatchersCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestMustMatchersDotty = project.in(file(\"dotty/mustmatchers\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-mustmatchers\",\n",
      "      \"ScalaTest Must Matchers Dotty\",\n",
      "      (targetDir, version, scalaVersion) =>\n",
      "        GenMatchers.genMainForDotty(targetDir / \"org\" / \"scalatest\", version, scalaVersion)\n",
      "    ).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\"org.scalatest.matchers.must\"),\n",
      "  ).dependsOn(scalatestMatchersCoreDotty)\n",
      "\n",
      "  lazy val scalatestMustMatchersDottyJS = project.in(file(\"dotty/mustmatchers.js\"))\n",
      "    .scalatestSubModule(\n",
      "      \"scalatest-mustmatchers\",\n",
      "      \"ScalaTest Must Matchers DottyJS \",\n",
      "      (targetDir, version, scalaVersion) =>\n",
      "        GenMatchers.genMainForDottyJS(targetDir / \"org\" / \"scalatest\", version, scalaVersion) ++ \n",
      "        GenScalaTestDotty.genMustMatchersScalaJS(targetDir, version, scalaVersion)\n",
      "    ).settings(\n",
      "    OsgiKeys.exportPackage := Seq(\"org.scalatest.matchers.must\"),\n",
      "  ).dependsOn(scalatestMatchersCoreDottyJS).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestModulesDotty = project.in(file(\"modules/dotty/modules-aggregation\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(\n",
      "      noPublishSettings,\n",
      "      scalacOptions in (Compile, doc) := List.empty\n",
      "    ).aggregate(\n",
      "      scalatestCoreDotty, \n",
      "      scalatestFeatureSpecDotty, \n",
      "      scalatestFlatSpecDotty, \n",
      "      scalatestFreeSpecDotty, \n",
      "      scalatestFunSuiteDotty, \n",
      "      scalatestFunSpecDotty, \n",
      "      scalatestPropSpecDotty, \n",
      "      scalatestRefSpecDotty, \n",
      "      scalatestWordSpecDotty, \n",
      "      scalatestDiagramsDotty, \n",
      "      scalatestMatchersCoreDotty, \n",
      "      scalatestShouldMatchersDotty, \n",
      "      scalatestMustMatchersDotty\n",
      "    )\n",
      "\n",
      "  lazy val scalatestDotty = project.in(file(\"dotty/scalatest\"))\n",
      "    .scalatestModule(\"scalatest\", \"ScalaTest Dotty\")\n",
      "    .settings(\n",
      "      // Little trick to get rid of bnd error when publish.\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        (crossTarget.value / \"classes\").mkdirs()\n",
      "        Seq.empty[File]\n",
      "      }.taskValue,\n",
      "      OsgiKeys.privatePackage := Seq.empty, \n",
      "    ).dependsOn(\n",
      "      scalatestCoreDotty, \n",
      "      scalatestFeatureSpecDotty, \n",
      "      scalatestFlatSpecDotty, \n",
      "      scalatestFreeSpecDotty, \n",
      "      scalatestFunSuiteDotty, \n",
      "      scalatestFunSpecDotty, \n",
      "      scalatestPropSpecDotty, \n",
      "      scalatestRefSpecDotty, \n",
      "      scalatestWordSpecDotty, \n",
      "      scalatestDiagramsDotty, \n",
      "      scalatestMatchersCoreDotty, \n",
      "      scalatestShouldMatchersDotty, \n",
      "      scalatestMustMatchersDotty\n",
      "    ).aggregate(\n",
      "      scalatestCoreDotty, \n",
      "      scalatestFeatureSpecDotty, \n",
      "      scalatestFlatSpecDotty, \n",
      "      scalatestFreeSpecDotty, \n",
      "      scalatestFunSuiteDotty, \n",
      "      scalatestFunSpecDotty, \n",
      "      scalatestPropSpecDotty, \n",
      "      scalatestRefSpecDotty, \n",
      "      scalatestWordSpecDotty, \n",
      "      scalatestDiagramsDotty, \n",
      "      scalatestMatchersCoreDotty, \n",
      "      scalatestShouldMatchersDotty, \n",
      "      scalatestMustMatchersDotty\n",
      "    )\n",
      "\n",
      "  lazy val scalatestDottyJS = project.in(file(\"dotty/scalatest.js\"))\n",
      "    .scalatestModule(\"scalatest\", \"ScalaTest Dotty JS\")\n",
      "    .settings(\n",
      "      // Little trick to get rid of bnd error when publish.\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        (crossTarget.value / \"classes\").mkdirs()\n",
      "        Seq.empty[File]\n",
      "      }.taskValue,\n",
      "      OsgiKeys.privatePackage := Seq.empty, \n",
      "    ).dependsOn(\n",
      "      scalatestCoreDottyJS, \n",
      "      scalatestFeatureSpecDottyJS, \n",
      "      scalatestFlatSpecDottyJS, \n",
      "      scalatestFreeSpecDottyJS, \n",
      "      scalatestFunSuiteDottyJS, \n",
      "      scalatestFunSpecDottyJS, \n",
      "      scalatestPropSpecDottyJS, \n",
      "      scalatestRefSpecDottyJS, \n",
      "      scalatestWordSpecDottyJS, \n",
      "      scalatestDiagramsDottyJS, \n",
      "      scalatestMatchersCoreDottyJS, \n",
      "      scalatestShouldMatchersDottyJS, \n",
      "      scalatestMustMatchersDottyJS\n",
      "    ).aggregate(\n",
      "      scalatestCoreDottyJS, \n",
      "      scalatestFeatureSpecDottyJS, \n",
      "      scalatestFlatSpecDottyJS, \n",
      "      scalatestFreeSpecDottyJS, \n",
      "      scalatestFunSuiteDottyJS, \n",
      "      scalatestFunSpecDottyJS, \n",
      "      scalatestPropSpecDottyJS, \n",
      "      scalatestRefSpecDottyJS, \n",
      "      scalatestWordSpecDottyJS, \n",
      "      scalatestDiagramsDottyJS, \n",
      "      scalatestMatchersCoreDottyJS, \n",
      "      scalatestShouldMatchersDottyJS, \n",
      "      scalatestMustMatchersDottyJS\n",
      "    ).enablePlugins(ScalaJSPlugin) \n",
      "\n",
      "  private lazy val noPublishSettings = Seq(\n",
      "    publishArtifact := false,\n",
      "    publish := {},\n",
      "    publishLocal := {},\n",
      "  )\n",
      "\n",
      "  lazy val commonTestDotty = project.in(file(\"dotty/common-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Common test classes used by scalactic and scalatest\",\n",
      "      libraryDependencies ++= crossBuildTestLibraryDependencies.value,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenCommonTestDotty.genMain((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenGen.genMain((sourceManaged in Compile).value / \"scala\" / \"org\" / \"scalatest\" / \"prop\", version.value, scalaVersion.value) ++\n",
      "        GenCompatibleClasses.genTest((sourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      noPublishSettings,\n",
      "    ).dependsOn(scalacticDotty, LocalProject(\"scalatestDotty\"))\n",
      "\n",
      "  lazy val commonTestDottyJS = project.in(file(\"dotty/common-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Common test classes used by scalactic and scalatest\",\n",
      "      libraryDependencies ++= crossBuildTestLibraryDependencies.value,\n",
      "      sourceGenerators in Compile += Def.task {\n",
      "        GenCommonTestDotty.genMainJS((sourceManaged in Compile).value, version.value, scalaVersion.value) ++\n",
      "        GenGen.genMain((sourceManaged in Compile).value / \"scala\" / \"org\" / \"scalatest\" / \"prop\", version.value, scalaVersion.value) ++\n",
      "        GenCompatibleClasses.genTest((sourceManaged in Compile).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "      noPublishSettings,\n",
      "    ).dependsOn(scalacticDottyJS, LocalProject(\"scalatestDottyJS\")).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalacticTestDotty = project.in(file(\"dotty/scalactic-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Scalactic Test\",\n",
      "      organization := \"org.scalactic\",\n",
      "      testOptions in Test ++=\n",
      "        Seq(Tests.Argument(TestFrameworks.ScalaTest,\n",
      "          \"-oDIF\",\n",
      "          \"-W\", \"120\", \"60\")),    \n",
      "      logBuffered in Test := false,\n",
      "      noPublishSettings,\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalacticDotty.genTest((sourceManaged in Test).value, version.value, scalaVersion.value) /*++\n",
      "        GenAnyVals.genTest((sourceManaged in Test).value / \"scala\" / \"org\" / \"scalactic\" / \"anyvals\", version.value, scalaVersion.value)*/\n",
      "      }.taskValue\n",
      "    ).dependsOn(scalacticDotty, scalatestDotty % \"test\", commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalacticTestDottyJS = project.in(file(\"dotty/scalactic-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(\n",
      "      projectTitle := \"Scalactic Test JS\",\n",
      "      organization := \"org.scalactic\",\n",
      "      scalaJSLinkerConfig ~= { _.withOptimizer(false) },\n",
      "      testOptions in Test ++=\n",
      "        Seq(Tests.Argument(TestFrameworks.ScalaTest, \"-oDIF\")),\n",
      "      jsEnv := {\n",
      "        import org.scalajs.jsenv.nodejs.NodeJSEnv\n",
      "        new NodeJSEnv(\n",
      "          NodeJSEnv.Config()\n",
      "            .withArgs(List(\"--max_old_space_size=3000\")))\n",
      "      }, \n",
      "      parallelExecution in Test := false,\n",
      "      fork in Test := false,\n",
      "      logBuffered in Test := false,\n",
      "      noPublishSettings,\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalacticDotty.genTestJS((sourceManaged in Test).value, version.value, scalaVersion.value) /*++\n",
      "        GenAnyVals.genTest((sourceManaged in Test).value / \"scala\" / \"org\" / \"scalactic\" / \"anyvals\", version.value, scalaVersion.value)*/\n",
      "      }.taskValue\n",
      "    ).dependsOn(scalacticDottyJS, scalatestDottyJS % \"test\", commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  def sharedTestSettingsDotty: Seq[Setting[_]] = \n",
      "    Seq(\n",
      "      organization := \"org.scalatest\",\n",
      "      libraryDependencies ++= scalatestLibraryDependencies,\n",
      "      libraryDependencies ++= \n",
      "        Seq(\n",
      "          \"org.scalatestplus\" %% \"testng-6-7\" % plusTestNGVersion % \"test\",\n",
      "          \"org.scalatestplus\" %% \"junit-4-13\" % plusJUnitVersion % \"test\"\n",
      "        ),\n",
      "      testOptions in Test := scalatestTestOptions,\n",
      "      logBuffered in Test := false,\n",
      "      //fork in Test := true,\n",
      "      //parallelExecution in Test := true,\n",
      "      //testForkedParallel in Test := true,\n",
      "      baseDirectory in Test := file(\"./\"),\n",
      "    ) ++ noPublishSettings\n",
      "\n",
      "  lazy val scalatestTestDotty = project.in(file(\"dotty/scalatest-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Test\",\n",
      "      javaSourceManaged := target.value / \"java\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenRegularTests4.genJava((javaSourceManaged in Compile).value) ++\n",
      "        GenScalaTestDotty.genTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\").aggregate(\n",
      "      scalatestDiagramsTestDotty, \n",
      "      scalatestFeatureSpecTestDotty, \n",
      "      scalatestFlatSpecTestDotty, \n",
      "      scalatestFreeSpecTestDotty, \n",
      "      scalatestFunSpecTestDotty, \n",
      "      scalatestFunSuiteTestDotty, \n",
      "      scalatestPropSpecTestDotty, \n",
      "      scalatestWordSpecTestDotty\n",
      "    )\n",
      "\n",
      "  def scalatestTestDottyJSOptions =\n",
      "    Seq(Tests.Argument(TestFrameworks.ScalaTest,\n",
      "      \"-l\", \"org.scalatest.tags.Slow\",\n",
      "      \"-m\", \"org.scalatest\",\n",
      "      \"-m\", \"org.scalactic\",\n",
      "      \"-m\", \"org.scalactic.anyvals\",\n",
      "      \"-m\", \"org.scalactic.algebra\",\n",
      "      \"-m\", \"org.scalactic.enablers\",\n",
      "      \"-m\", \"org.scalatest.fixture\",\n",
      "      \"-m\", \"org.scalatest.concurrent\",\n",
      "      \"-m\", \"org.scalatest.events\",\n",
      "      \"-m\", \"org.scalatest.prop\",\n",
      "      \"-m\", \"org.scalatest.tools\",\n",
      "      \"-m\", \"org.scalatest.matchers\",\n",
      "      \"-m\", \"org.scalatest.matchers\",\n",
      "      \"-m\", \"org.scalatest.matchers.should\",\n",
      "      \"-m\", \"org.scalatest.matchers.must\",\n",
      "      \"-m\", \"org.scalatest.matchers.dsl\",\n",
      "      \"-m\", \"org.scalatest.verbs\",\n",
      "      \"-m\", \"org.scalatest.suiteprop\",\n",
      "      \"-m\", \"org.scalatest.path\",\n",
      "      \"-m\", \"org.scalatest.exceptions\",\n",
      "      \"-m\", \"org.scalatest.time\",\n",
      "      \"-m\", \"org.scalatest.words\",\n",
      "      \"-m\", \"org.scalatest.enablers\",\n",
      "      \"-m\", \"org.scalatest.expectations\",\n",
      "      \"-m\", \"org.scalatest.diagrams\",\n",
      "      \"-m\", \"org.scalatest.featurespec\",\n",
      "      \"-m\", \"org.scalatest.flatspec\",\n",
      "      \"-m\", \"org.scalatest.freespec\",\n",
      "      \"-m\", \"org.scalatest.funspec\",\n",
      "      \"-m\", \"org.scalatest.funsuite\",\n",
      "      \"-m\", \"org.scalatest.propspec\",\n",
      "      \"-m\", \"org.scalatest.wordspec\",\n",
      "      \"-oDIF\"))    \n",
      "\n",
      "  def sharedTestSettingsDottyJS: Seq[Setting[_]] = \n",
      "    Seq(\n",
      "      organization := \"org.scalatest\",\n",
      "      //jsDependencies += RuntimeDOM % \"test\",\n",
      "      scalaJSLinkerConfig ~= { _.withOptimizer(false) },\n",
      "      //jsEnv := NodeJSEnv(executable = \"node\").value,\n",
      "      //jsEnv := PhantomJSEnv().value,\n",
      "      jsEnv := {\n",
      "        import org.scalajs.jsenv.nodejs.NodeJSEnv\n",
      "        new NodeJSEnv(\n",
      "          NodeJSEnv.Config()\n",
      "            .withArgs(List(/*\"--max_new_space_size=3000\", */\"--max_old_space_size=3000\")))\n",
      "      },\n",
      "      //Seq(Compile, Test).flatMap(c => inConfig(c)(jsEnv := RhinoJSEnv().value)), // to use rhino\n",
      "      testOptions in Test := scalatestTestDottyJSOptions,\n",
      "      parallelExecution in Test := false,\n",
      "      fork in Test := false,\n",
      "      publishArtifact := false,\n",
      "      publish := {},\n",
      "      publishLocal := {},\n",
      "      scalacOptions ++= (if (scalaBinaryVersion.value == \"2.10\" || scalaVersion.value.startsWith(\"2.13\")) Seq.empty[String] else Seq(\"-Ypartial-unification\"))\n",
      "    )  \n",
      "\n",
      "  lazy val scalatestTestDottyJS = project.in(file(\"dotty/scalatest-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Test\",\n",
      "      scalaJSLinkerConfig ~= { _.withOptimizer(false).withSemantics(_.withStrictFloats(true)) },\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        //GenRegularTests4.genJava((javaSourceManaged in Compile).value) ++\n",
      "        GenScalaTestDotty.genTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(scalacticDottyJS, scalatestDottyJS % \"test\", commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "     .aggregate(\n",
      "       scalatestDiagramsTestDottyJS, \n",
      "       scalatestFeatureSpecTestDottyJS, \n",
      "       scalatestFlatSpecTestDottyJS, \n",
      "       scalatestFreeSpecTestDottyJS, \n",
      "       scalatestFunSpecTestDottyJS, \n",
      "       scalatestFunSuiteTestDottyJS, \n",
      "       scalatestPropSpecTestDottyJS, \n",
      "       scalatestWordSpecTestDottyJS\n",
      "     ).enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "\n",
      "  lazy val scalatestDiagramsTestDotty = project.in(file(\"dotty/diagrams-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Diagrams Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genDiagramsTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestDiagramsTestDottyJS = project.in(file(\"dotty/diagrams-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest Diagrams Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genDiagramsTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFeatureSpecTestDotty = project.in(file(\"dotty/featurespec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FeatureSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFeatureSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFeatureSpecTestDottyJS = project.in(file(\"dotty/featurespec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FeatureSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFeatureSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFlatSpecTestDotty = project.in(file(\"dotty/flatspec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FlatSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFlatSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFlatSpecTestDottyJS = project.in(file(\"dotty/flatspec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FlatSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFlatSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFreeSpecTestDotty = project.in(file(\"dotty/freespec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FreeSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFreeSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFreeSpecTestDottyJS = project.in(file(\"dotty/freespec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FreeSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFreeSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFunSpecTestDotty = project.in(file(\"dotty/funspec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FunSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFunSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFunSpecTestDottyJS = project.in(file(\"dotty/funspec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FunSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFunSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestFunSuiteTestDotty = project.in(file(\"dotty/funsuite-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FunSuite Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFunSuiteTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestFunSuiteTestDottyJS = project.in(file(\"dotty/funsuite-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest FunSuite Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genFunSuiteTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)\n",
      "\n",
      "  lazy val scalatestPropSpecTestDotty = project.in(file(\"dotty/propspec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest PropSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genPropSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestPropSpecTestDottyJS = project.in(file(\"dotty/propspec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest PropSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genPropSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin)             \n",
      "\n",
      "  lazy val scalatestWordSpecTestDotty = project.in(file(\"dotty/wordspec-test\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDotty)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest WordSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genWordSpecTest((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDotty % \"test\")\n",
      "\n",
      "  lazy val scalatestWordSpecTestDottyJS = project.in(file(\"dotty/wordspec-test.js\"))\n",
      "    .settings(sharedSettings: _*)\n",
      "    .settings(dottySettings: _*)\n",
      "    .settings(sharedTestSettingsDottyJS)\n",
      "    .settings(\n",
      "      projectTitle := \"ScalaTest WordSpec Test\",\n",
      "      sourceGenerators in Test += Def.task {\n",
      "        GenScalaTestDotty.genWordSpecTestJS((sourceManaged in Test).value, version.value, scalaVersion.value)\n",
      "      }.taskValue,\n",
      "    ).dependsOn(commonTestDottyJS % \"test\").enablePlugins(ScalaJSPlugin) \n",
      "\n",
      "}\n",
      "\n",
      "Average Line Length: 45.38501560874089\n",
      "\n",
      "Example 8:\n",
      "package is.hail.utils.richUtils\n",
      "\n",
      "import is.hail.expr._\n",
      "import is.hail.annotations.Region\n",
      "import is.hail.asm4s.Code\n",
      "import is.hail.expr.types._\n",
      "\n",
      "class RichCodeRegion(val region: Code[Region]) extends AnyVal {\n",
      "  def size: Code[Long] = region.invoke[Long](\"size\")\n",
      "\n",
      "  def copyFrom(other: Code[Region], readStart: Code[Long], writeStart: Code[Long], n: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Region, Long, Long, Long, Unit](\"copyFrom\", other, readStart, writeStart, n)\n",
      "  }\n",
      "\n",
      "  def storeInt(off: Code[Long], v: Code[Int]): Code[Unit] = {\n",
      "    region.invoke[Long,Int,Unit](\"storeInt\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeLong(off: Code[Long], v: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long,Long,Unit](\"storeLong\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeFloat(off: Code[Long], v: Code[Float]): Code[Unit] = {\n",
      "    region.invoke[Long,Float,Unit](\"storeFloat\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeDouble(off: Code[Long], v: Code[Double]): Code[Unit] = {\n",
      "    region.invoke[Long,Double,Unit](\"storeDouble\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeAddress(off: Code[Long], a: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long,Long,Unit](\"storeAddress\", off, a)\n",
      "  }\n",
      "\n",
      "  def storeByte(off: Code[Long], b: Code[Byte]): Code[Unit] = {\n",
      "    region.invoke[Long, Byte, Unit](\"storeByte\", off, b)\n",
      "  }\n",
      "\n",
      "  def storeBytes(off: Code[Long], bytes: Code[Array[Byte]]): Code[Unit] = {\n",
      "    region.invoke[Long, Array[Byte], Unit](\"storeBytes\", off, bytes)\n",
      "  }\n",
      "\n",
      "  def allocate(alignment: Code[Long], n: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long, Long](\"allocate\", alignment, n)\n",
      "  }\n",
      "\n",
      "  def loadBoolean(off: Code[Long]): Code[Boolean] = {\n",
      "    region.invoke[Long, Boolean](\"loadBoolean\", off)\n",
      "  }\n",
      "\n",
      "  def loadByte(off: Code[Long]): Code[Byte] = {\n",
      "    region.invoke[Long, Byte](\"loadByte\", off)\n",
      "  }\n",
      "\n",
      "  def loadInt(off: Code[Long]): Code[Int] = {\n",
      "    region.invoke[Long, Int](\"loadInt\", off)\n",
      "  }\n",
      "\n",
      "  def loadLong(off: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"loadLong\", off)\n",
      "  }\n",
      "\n",
      "  def loadFloat(off: Code[Long]): Code[Float] = {\n",
      "    region.invoke[Long, Float](\"loadFloat\", off)\n",
      "  }\n",
      "\n",
      "  def loadDouble(off: Code[Long]): Code[Double] = {\n",
      "    region.invoke[Long, Double](\"loadDouble\", off)\n",
      "  }\n",
      "\n",
      "  def loadAddress(off: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"loadAddress\", off)\n",
      "  }\n",
      "\n",
      "  def loadBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Boolean] = {\n",
      "    region.invoke[Long, Long, Boolean](\"loadBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def loadIRIntermediate(typ: Type): Code[Long] => Code[_] = typ.fundamentalType match {\n",
      "    case _: TBoolean => loadBoolean\n",
      "    case _: TInt32 => loadInt\n",
      "    case _: TInt64 => loadLong\n",
      "    case _: TFloat32 => loadFloat\n",
      "    case _: TFloat64 => loadDouble\n",
      "    case _: TArray => loadAddress\n",
      "    case _: TBinary => loadAddress\n",
      "    case _: TBaseStruct => off => off\n",
      "  }\n",
      "\n",
      "  def setBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"setBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def setBit(byteOff: Code[Long], bitOff: Long): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"setBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def clearBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"clearBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def storeBit(byteOff: Code[Long], bitOff: Code[Long], b: Code[Boolean]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Boolean, Unit](\"setBit\", byteOff, bitOff, b)\n",
      "  }\n",
      "\n",
      "  def appendInt(i: Code[Int]): Code[Long] = {\n",
      "    region.invoke[Int, Long](\"appendInt\", i)\n",
      "  }\n",
      "\n",
      "  def appendLong(l: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"appendLong\", l)\n",
      "  }\n",
      "\n",
      "  def appendFloat(f: Code[Float]): Code[Long] = {\n",
      "    region.invoke[Float, Long](\"appendFloat\", f)\n",
      "  }\n",
      "\n",
      "  def appendDouble(d: Code[Double]): Code[Long] = {\n",
      "    region.invoke[Double, Long](\"appendDouble\", d)\n",
      "  }\n",
      "\n",
      "  def appendByte(b: Code[Byte]): Code[Long] = {\n",
      "    region.invoke[Byte, Long](\"appendByte\", b)\n",
      "  }\n",
      "\n",
      "  def appendBytes(bytes: Code[Array[Byte]]): Code[Long] = {\n",
      "    region.invoke[Array[Byte], Long](\"appendBytes\", bytes)\n",
      "  }\n",
      "\n",
      "  def appendBytes(bytes: Code[Array[Byte]], bytesOff: Code[Long], n: Code[Int]): Code[Long] = {\n",
      "    region.invoke[Array[Byte],Long, Int, Long](\"appendBytes\", bytes, bytesOff, n)\n",
      "  }\n",
      "\n",
      "  def appendString(string: Code[String]): Code[Long] = {\n",
      "    region.invoke[String, Long](\"appendString\", string)\n",
      "  }\n",
      "\n",
      "  def clear(): Code[Unit] = {\n",
      "    region.invoke[Unit](\"clear\")\n",
      "  }\n",
      "}\n",
      "\n",
      "Average Line Length: 29.6056338028169\n",
      "\n",
      "Example 9:\n",
      "<reponame>JackBuggins/spark\n",
      "/*\n",
      " * Licensed to the Apache Software Foundation (ASF) under one or more\n",
      " * contributor license agreements.  See the NOTICE file distributed with\n",
      " * this work for additional information regarding copyright ownership.\n",
      " * The ASF licenses this file to You under the Apache License, Version 2.0\n",
      " * (the \"License\"); you may not use this file except in compliance with\n",
      " * the License.  You may obtain a copy of the License at\n",
      " *\n",
      " *    http://www.apache.org/licenses/LICENSE-2.0\n",
      " *\n",
      " * Unless required by applicable law or agreed to in writing, software\n",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      " * See the License for the specific language governing permissions and\n",
      " * limitations under the License.\n",
      " */\n",
      "\n",
      "package org.apache.spark.sql.catalyst.plans.logical\n",
      "\n",
      "import scala.collection.mutable\n",
      "import scala.reflect.runtime.universe.TypeTag\n",
      "\n",
      "import org.apache.spark.sql.catalyst.dsl.expressions._\n",
      "import org.apache.spark.sql.catalyst.dsl.plans._\n",
      "import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n",
      "import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, ExpressionSet, UnspecifiedFrame}\n",
      "import org.apache.spark.sql.catalyst.plans._\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "\n",
      "class DistinctKeyVisitorSuite extends PlanTest {\n",
      "\n",
      "  private val a = AttributeReference(\"a\", IntegerType)()\n",
      "  private val b = AttributeReference(\"b\", IntegerType)()\n",
      "  private val c = AttributeReference(\"c\", IntegerType)()\n",
      "  private val d = a.as(\"aliased_a\")\n",
      "  private val e = b.as(\"aliased_b\")\n",
      "  private val f = Alias(a + 1, (a + 1).toString)()\n",
      "  private val x = AttributeReference(\"x\", IntegerType)()\n",
      "  private val y = AttributeReference(\"y\", IntegerType)()\n",
      "  private val z = AttributeReference(\"z\", IntegerType)()\n",
      "\n",
      "\n",
      "  private val t1 = LocalRelation(a, b, c).as(\"t1\")\n",
      "  private val t2 = LocalRelation(x, y, z).as(\"t2\")\n",
      "\n",
      "  private def checkDistinctAttributes(plan: LogicalPlan, distinctKeys: Set[ExpressionSet]) = {\n",
      "    assert(plan.analyze.distinctKeys === distinctKeys)\n",
      "  }\n",
      "\n",
      "  implicit private def productEncoder[T <: Product : TypeTag] = ExpressionEncoder[T]()\n",
      "\n",
      "  test(\"Aggregate's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\", 1), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\")($\"a\"), Set(ExpressionSet(Seq(a))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\"), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\", 1)($\"a\", $\"b\"), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\", 1), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\", 1)($\"a\", $\"b\", 1), Set(ExpressionSet(Seq(a, b))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"a\"), Set.empty)\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\"), Set.empty)\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\")($\"a\", max($\"b\")), Set(ExpressionSet(Seq(a))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\", d, e),\n",
      "      Set(ExpressionSet(Seq(a, b)), ExpressionSet(Seq(d.toAttribute, e.toAttribute))))\n",
      "    checkDistinctAttributes(t1.groupBy()(sum($\"c\")), Set.empty)\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\")($\"a\", $\"a\" % 10, d, sum($\"b\")),\n",
      "      Set(ExpressionSet(Seq(a)), ExpressionSet(Seq(d.toAttribute))))\n",
      "    checkDistinctAttributes(t1.groupBy(f.child, $\"b\")(f, $\"b\", sum($\"c\")),\n",
      "      Set(ExpressionSet(Seq(f.toAttribute, b))))\n",
      "  }\n",
      "\n",
      "  test(\"Distinct's distinct attributes\") {\n",
      "    checkDistinctAttributes(Distinct(t1), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(Distinct(t1.select($\"a\", $\"c\")), Set(ExpressionSet(Seq(a, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Except's distinct attributes\") {\n",
      "    checkDistinctAttributes(Except(t1, t2, false), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(Except(t1, t2, true), Set.empty)\n",
      "  }\n",
      "\n",
      "  test(\"Filter's distinct attributes\") {\n",
      "    checkDistinctAttributes(Filter($\"a\" > 1, t1), Set.empty)\n",
      "    checkDistinctAttributes(Filter($\"a\" > 1, Distinct(t1)), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Limit's distinct attributes\") {\n",
      "    checkDistinctAttributes(Distinct(t1).limit(10), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(LocalLimit(10, Distinct(t1)), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(t1.limit(1), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Intersect's distinct attributes\") {\n",
      "    checkDistinctAttributes(Intersect(t1, t2, false), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(Intersect(t1, t2, true), Set.empty)\n",
      "  }\n",
      "\n",
      "  test(\"Join's distinct attributes\") {\n",
      "    Seq(LeftSemi, LeftAnti).foreach { joinType =>\n",
      "      checkDistinctAttributes(\n",
      "        Distinct(t1).join(t2, joinType, Some($\"a\" === $\"x\")), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    }\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1).join(Distinct(t2), Inner, Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" === $\"z\")),\n",
      "      Set(ExpressionSet(Seq(a, b, c)), ExpressionSet(Seq(x, y, z))))\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1)\n",
      "        .join(Distinct(t2), LeftOuter, Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" === $\"z\")),\n",
      "      Set(ExpressionSet(Seq(a, b, c))))\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1)\n",
      "        .join(Distinct(t2), RightOuter, Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" === $\"z\")),\n",
      "      Set(ExpressionSet(Seq(x, y, z))))\n",
      "\n",
      "    Seq(Inner, Cross, LeftOuter, RightOuter).foreach { joinType =>\n",
      "      checkDistinctAttributes(t1.join(t2, joinType, Some($\"a\" === $\"x\")),\n",
      "        Set.empty)\n",
      "      checkDistinctAttributes(\n",
      "        Distinct(t1).join(Distinct(t2), joinType, Some($\"a\" === $\"x\" && $\"b\" === $\"y\")),\n",
      "        Set.empty)\n",
      "      checkDistinctAttributes(\n",
      "        Distinct(t1).join(Distinct(t2), joinType,\n",
      "          Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" % 5 === $\"z\" % 5)),\n",
      "        Set.empty)\n",
      "    }\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1).join(Distinct(t2), Cross, Some($\"a\" === $\"x\" && $\"b\" === $\"y\" && $\"c\" === $\"z\")),\n",
      "      Set.empty)\n",
      "  }\n",
      "\n",
      "  test(\"Project's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.select($\"a\", $\"b\"), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).select($\"a\"), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).select($\"a\", $\"b\", d, e), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1)\n",
      "      .select($\"a\", $\"b\", $\"c\", 1), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(Distinct(t1).select($\"a\", $\"b\", c, d),\n",
      "      Set(ExpressionSet(Seq(a, b, c)), ExpressionSet(Seq(b, c, d.toAttribute))))\n",
      "    checkDistinctAttributes(t1.groupBy($\"a\", $\"b\")($\"a\", $\"b\", d).select($\"a\", $\"b\", e),\n",
      "      Set(ExpressionSet(Seq(a, b)), ExpressionSet(Seq(a, e.toAttribute))))\n",
      "  }\n",
      "\n",
      "  test(\"Repartition's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.repartition(8), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).repartition(8), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(RepartitionByExpression(Seq(a), Distinct(t1), None),\n",
      "      Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Sample's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.sample(0, 0.2, false, 1), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).sample(0, 0.2, false, 1), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Window's distinct attributes\") {\n",
      "    val winExpr = windowExpr(count($\"b\"),\n",
      "      windowSpec($\"a\" :: Nil, $\"b\".asc :: Nil, UnspecifiedFrame))\n",
      "\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1)\n",
      "        .select($\"a\", $\"b\", $\"c\", winExpr.as(Symbol(\"window\"))), Set(ExpressionSet(Seq(a, b, c))))\n",
      "    checkDistinctAttributes(\n",
      "      Distinct(t1).select($\"a\", $\"b\", winExpr.as(Symbol(\"window\"))), Set())\n",
      "  }\n",
      "\n",
      "  test(\"Tail's distinct attributes\") {\n",
      "    checkDistinctAttributes(Tail(10, Distinct(t1)), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"Sort's distinct attributes\") {\n",
      "    checkDistinctAttributes(t1.sortBy($\"a\".asc), Set.empty)\n",
      "    checkDistinctAttributes(Distinct(t1).sortBy($\"a\".asc), Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"RebalancePartitions's distinct attributes\") {\n",
      "    checkDistinctAttributes(RebalancePartitions(Seq(a), Distinct(t1)),\n",
      "      Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "\n",
      "  test(\"WithCTE's distinct attributes\") {\n",
      "    checkDistinctAttributes(WithCTE(Distinct(t1), mutable.ArrayBuffer.empty[CTERelationDef].toSeq),\n",
      "      Set(ExpressionSet(Seq(a, b, c))))\n",
      "  }\n",
      "}\n",
      "\n",
      "Average Line Length: 43.32275132275132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def calculate_avg_line_length(example):\n",
    "    lines = example['content'].split('\\n')\n",
    "    avg_length = sum(len(line) for line in lines) / len(lines)\n",
    "    example['avg_line_length'] = avg_length\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(calculate_avg_line_length)\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    if i < 10:\n",
    "        print(f\"Example {i}:\")\n",
    "        print(example['content'])\n",
    "        print(f\"Average Line Length: {example['avg_line_length']}\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for avg_line_length:\n",
      "count    1.355788e+06\n",
      "mean     3.192329e+01\n",
      "std      8.714524e+00\n",
      "min      1.800000e+00\n",
      "25%      2.648315e+01\n",
      "50%      3.167347e+01\n",
      "75%      3.679892e+01\n",
      "max      9.904051e+01\n",
      "Name: avg_line_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "avg_line_length_stats = df['avg_line_length'].describe()\n",
    "print(\"Statistics for avg_line_length:\")\n",
    "print(avg_line_length_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=df['avg_line_length'])\n",
    "plt.title('Boxplot of Average Line Length')\n",
    "plt.xlabel('Average Line Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['avg_line_length'].quantile(0.25)\n",
    "Q3 = df['avg_line_length'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 32882\n"
     ]
    }
   ],
   "source": [
    "outliers = df[(df['avg_line_length'] < (Q1 - 1.5 * IQR)) | (df['avg_line_length'] > (Q3 + 1.5 * IQR))]\n",
    "print(f\"Number of outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    32882.000000\n",
       "mean        49.570592\n",
       "std         21.817582\n",
       "min          1.800000\n",
       "25%         52.675676\n",
       "50%         55.500000\n",
       "75%         60.736746\n",
       "max         99.040512\n",
       "Name: avg_line_length, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers['avg_line_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(outliers.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.322906e+06\n",
       "mean     3.148466e+01\n",
       "std      7.620113e+00\n",
       "min      1.102042e+01\n",
       "25%      2.645455e+01\n",
       "50%      3.153846e+01\n",
       "75%      3.647417e+01\n",
       "max      5.227132e+01\n",
       "Name: avg_line_length, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['avg_line_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.74545454545454"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.percentile(df['avg_line_length'], 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.322906e+06\n",
       "mean     9.489140e+01\n",
       "std      2.054064e+02\n",
       "min      1.000000e+00\n",
       "25%      2.600000e+01\n",
       "50%      5.100000e+01\n",
       "75%      1.010000e+02\n",
       "max      2.813500e+04\n",
       "Name: line_count, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['line_count'] = df['content'].apply(lambda x: len(x.split('\\n')))\n",
    "df['line_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_range_dict = {\n",
    "    \"0-100\": 0,\n",
    "    \"101-200\": 0,\n",
    "    \"201-300\": 0,\n",
    "    \"301-400\": 0,\n",
    "    \"401-500\": 0,\n",
    "    \"501-600\": 0,\n",
    "    \"601-700\": 0,\n",
    "    \"701-800\": 0,\n",
    "    \"801-900\": 0,\n",
    "    \"901-1000\": 0,\n",
    "    \"1000+\": 0\n",
    "}\n",
    "\n",
    "for count in df['line_count']:\n",
    "    if count <= 100:\n",
    "        count_range_dict[\"0-100\"] += 1\n",
    "    elif count <= 200:\n",
    "        count_range_dict[\"101-200\"] += 1\n",
    "    elif count <= 300:\n",
    "        count_range_dict[\"201-300\"] += 1\n",
    "    elif count <= 400:\n",
    "        count_range_dict[\"301-400\"] += 1\n",
    "    elif count <= 500:\n",
    "        count_range_dict[\"401-500\"] += 1\n",
    "    elif count <= 600:\n",
    "        count_range_dict[\"501-600\"] += 1\n",
    "    elif count <= 700:\n",
    "        count_range_dict[\"601-700\"] += 1\n",
    "    elif count <= 800:\n",
    "        count_range_dict[\"701-800\"] += 1\n",
    "    elif count <= 900:\n",
    "        count_range_dict[\"801-900\"] += 1\n",
    "    elif count <= 1000:\n",
    "        count_range_dict[\"901-1000\"] += 1\n",
    "    else:\n",
    "        count_range_dict[\"1000+\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples with line count in range 0-100: 990882\n",
      "Number of examples with line count in range 101-200: 202179\n",
      "Number of examples with line count in range 201-300: 62702\n",
      "Number of examples with line count in range 301-400: 27040\n",
      "Number of examples with line count in range 401-500: 13834\n",
      "Number of examples with line count in range 501-600: 8010\n",
      "Number of examples with line count in range 601-700: 4892\n",
      "Number of examples with line count in range 701-800: 3186\n",
      "Number of examples with line count in range 801-900: 2246\n",
      "Number of examples with line count in range 901-1000: 1616\n",
      "Number of examples with line count in range 1000+: 6319\n"
     ]
    }
   ],
   "source": [
    "for key, value in count_range_dict.items():\n",
    "    print(f\"Number of examples with line count in range {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(count_range_dict.keys()), y=list(count_range_dict.values()))\n",
    "plt.title(\"Distribution of Scala code lengths\")\n",
    "plt.xlabel(\"Length of Scala code\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Count</th>\n",
       "      <th>Cumulative Count</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>Cumulative Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-100</td>\n",
       "      <td>990882</td>\n",
       "      <td>990882</td>\n",
       "      <td>74.901920</td>\n",
       "      <td>74.901920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101-200</td>\n",
       "      <td>202179</td>\n",
       "      <td>1193061</td>\n",
       "      <td>15.282945</td>\n",
       "      <td>90.184866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201-300</td>\n",
       "      <td>62702</td>\n",
       "      <td>1255763</td>\n",
       "      <td>4.739717</td>\n",
       "      <td>94.924583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>301-400</td>\n",
       "      <td>27040</td>\n",
       "      <td>1282803</td>\n",
       "      <td>2.043985</td>\n",
       "      <td>96.968568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401-500</td>\n",
       "      <td>13834</td>\n",
       "      <td>1296637</td>\n",
       "      <td>1.045728</td>\n",
       "      <td>98.014296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>501-600</td>\n",
       "      <td>8010</td>\n",
       "      <td>1304647</td>\n",
       "      <td>0.605485</td>\n",
       "      <td>98.619781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>601-700</td>\n",
       "      <td>4892</td>\n",
       "      <td>1309539</td>\n",
       "      <td>0.369792</td>\n",
       "      <td>98.989573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>701-800</td>\n",
       "      <td>3186</td>\n",
       "      <td>1312725</td>\n",
       "      <td>0.240833</td>\n",
       "      <td>99.230406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>801-900</td>\n",
       "      <td>2246</td>\n",
       "      <td>1314971</td>\n",
       "      <td>0.169778</td>\n",
       "      <td>99.400184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901-1000</td>\n",
       "      <td>1616</td>\n",
       "      <td>1316587</td>\n",
       "      <td>0.122155</td>\n",
       "      <td>99.522339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1000+</td>\n",
       "      <td>6319</td>\n",
       "      <td>1322906</td>\n",
       "      <td>0.477661</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length   Count  Cumulative Count  Percentage  Cumulative Percentage\n",
       "0      0-100  990882            990882   74.901920              74.901920\n",
       "1    101-200  202179           1193061   15.282945              90.184866\n",
       "2    201-300   62702           1255763    4.739717              94.924583\n",
       "3    301-400   27040           1282803    2.043985              96.968568\n",
       "4    401-500   13834           1296637    1.045728              98.014296\n",
       "5    501-600    8010           1304647    0.605485              98.619781\n",
       "6    601-700    4892           1309539    0.369792              98.989573\n",
       "7    701-800    3186           1312725    0.240833              99.230406\n",
       "8    801-900    2246           1314971    0.169778              99.400184\n",
       "9   901-1000    1616           1316587    0.122155              99.522339\n",
       "10     1000+    6319           1322906    0.477661             100.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_line = pd.DataFrame(list(count_range_dict.items()), columns=[\"Length\", \"Count\"])\n",
    "df_line[\"Cumulative Count\"] = df_line[\"Count\"].cumsum()\n",
    "df_line[\"Percentage\"] = df_line[\"Count\"] / df_line[\"Count\"].sum() * 100\n",
    "df_line[\"Cumulative Percentage\"] = df_line[\"Percentage\"].cumsum()\n",
    "df_line.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id',\n",
       "       'content', 'avg_line_length', 'line_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>line_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>package com.wavesplatform\\n\\nimport java.io.Fi...</td>\n",
       "      <td>36.009662</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>package helpers\\n\\nimport org.specs2.mutable._...</td>\n",
       "      <td>31.782828</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/*********************************************...</td>\n",
       "      <td>35.455357</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>package cqrs.queries\\n\\nimport java.time.Insta...</td>\n",
       "      <td>28.333333</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/*\\nCopyright 2012 Twitter, Inc.\\n\\nLicensed u...</td>\n",
       "      <td>31.710526</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  avg_line_length  \\\n",
       "0  package com.wavesplatform\\n\\nimport java.io.Fi...        36.009662   \n",
       "1  package helpers\\n\\nimport org.specs2.mutable._...        31.782828   \n",
       "2  /*********************************************...        35.455357   \n",
       "3  package cqrs.queries\\n\\nimport java.time.Insta...        28.333333   \n",
       "4  /*\\nCopyright 2012 Twitter, Inc.\\n\\nLicensed u...        31.710526   \n",
       "\n",
       "   line_count  \n",
       "0         207  \n",
       "1         198  \n",
       "2         224  \n",
       "3          30  \n",
       "4          38  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df.drop(columns=['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id'], axis=1)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1322906, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193061, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_filtered[df_filtered['line_count'] <= 200]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(945655, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_filtered[df_filtered['avg_line_length'] <= 37]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>line_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>package helpers\\n\\nimport org.specs2.mutable._...</td>\n",
       "      <td>31.782828</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>package cqrs.queries\\n\\nimport java.time.Insta...</td>\n",
       "      <td>28.333333</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/*\\nCopyright 2012 Twitter, Inc.\\n\\nLicensed u...</td>\n",
       "      <td>31.710526</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>package is.hail.utils.richUtils\\n\\nimport is.h...</td>\n",
       "      <td>29.605634</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;filename&gt;scala-koans/src/test/scala/org/funct...</td>\n",
       "      <td>31.607143</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  avg_line_length  \\\n",
       "1   package helpers\\n\\nimport org.specs2.mutable._...        31.782828   \n",
       "3   package cqrs.queries\\n\\nimport java.time.Insta...        28.333333   \n",
       "4   /*\\nCopyright 2012 Twitter, Inc.\\n\\nLicensed u...        31.710526   \n",
       "8   package is.hail.utils.richUtils\\n\\nimport is.h...        29.605634   \n",
       "10  <filename>scala-koans/src/test/scala/org/funct...        31.607143   \n",
       "\n",
       "    line_count  \n",
       "1          198  \n",
       "3           30  \n",
       "4           38  \n",
       "8          142  \n",
       "10          56  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       avg_line_length    line_count\n",
      "count     1.322906e+06  1.322906e+06\n",
      "mean      3.148466e+01  9.489140e+01\n",
      "std       7.620113e+00  2.054064e+02\n",
      "min       1.102042e+01  1.000000e+00\n",
      "25%       2.645455e+01  2.600000e+01\n",
      "50%       3.153846e+01  5.100000e+01\n",
      "75%       3.647417e+01  1.010000e+02\n",
      "max       5.227132e+01  2.813500e+04\n",
      "\n",
      "       avg_line_length     line_count\n",
      "count    945655.000000  945655.000000\n",
      "mean         28.240522      53.911314\n",
      "std           5.699576      42.669056\n",
      "min          11.025000       1.000000\n",
      "25%          24.551724      22.000000\n",
      "50%          29.102941      41.000000\n",
      "75%          32.760870      73.000000\n",
      "max          37.000000     200.000000\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"avg_line_length\", \"line_count\"]].describe())\n",
    "print()\n",
    "print(df_filtered.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_scala = {}\n",
    "\n",
    "for i, example in enumerate(df_filtered[\"content\"]):\n",
    "    data_dict_scala[f\"scala_{i}\"] = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_dict_scala) == len(df_filtered), \"Data dictionary length does not match the number of examples in the dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example scala_0:\n",
      "package helpers\n",
      "\n",
      "import org.specs2.mutable._\n",
      "import com.ruimo.recoeng.RecoEngApi\n",
      "import com.ruimo.recoeng.json.{JsonResponseHeader, OnSalesJsonResponse, SalesItem, TransactionMode, TransactionSalesMode, SortOrder, JsonRequestPaging, Desc, Asc, ScoredItem}\n",
      "import com.ruimo.recoeng.json.RecommendByItemJsonResponse\n",
      "import play.api.libs.json.{JsSuccess, JsResult}\n",
      "import models.LoginSession\n",
      "import org.mockito.Mockito.mock\n",
      "import models.PersistedTransaction\n",
      "import models.TransactionLogItem\n",
      "import models.TransactionLogCoupon\n",
      "import models.TransactionLogHeader\n",
      "import models.TransactionType\n",
      "import models.Address\n",
      "import models.ItemName\n",
      "import helpers.Helper._\n",
      "import com.ruimo.scoins.Scoping._\n",
      "\n",
      "class RecommendEngineSpec extends Specification {\n",
      "  \"Recommend engine\" should {\n",
      "    \"Can send transaction\" in {\n",
      "      val api: RecoEngApi = new RecoEngApi {\n",
      "        def onSales(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          transactionMode: TransactionMode,\n",
      "          transactionTime: Long,\n",
      "          userCode: String,\n",
      "          salesItems: Seq[SalesItem]\n",
      "        ): JsResult[OnSalesJsonResponse] = {\n",
      "          transactionMode === TransactionSalesMode\n",
      "          transactionTime === 23456L\n",
      "          userCode === \"12345\"\n",
      "          salesItems.size === 3\n",
      "          val set = salesItems.toSet\n",
      "          set.contains(SalesItem(\"555\", \"8192\", 3)) must beTrue\n",
      "          set.contains(SalesItem(\"555\", \"8193\", 5)) must beTrue\n",
      "          set.contains(SalesItem(\"666\", \"8194\", 1)) must beTrue\n",
      "\n",
      "          JsSuccess(\n",
      "            OnSalesJsonResponse(\n",
      "              JsonResponseHeader(sequenceNumber = \"1234\", statusCode = \"OK\", message = \"msg\")\n",
      "            )\n",
      "          )\n",
      "        }\n",
      "\n",
      "        def recommendByItem(\n",
      "          requestTime: Long = System.currentTimeMillis,\n",
      "          sequenceNumber: Long,\n",
      "          salesItems: Seq[SalesItem],\n",
      "          sort: SortOrder = Desc(\"score\"),\n",
      "          paging: JsonRequestPaging\n",
      "        ): JsResult[RecommendByItemJsonResponse] = null\n",
      "      }\n",
      "\n",
      "      val login = mock(classOf[LoginSession])\n",
      "      val tran: PersistedTransaction = PersistedTransaction(\n",
      "        header = TransactionLogHeader(\n",
      "          id = None,\n",
      "          userId = 12345L,\n",
      "          transactionTime = 23456L,\n",
      "          currencyId = 111L,\n",
      "          totalAmount = BigDecimal(1234),\n",
      "          taxAmount = BigDecimal(20),\n",
      "          transactionType = TransactionType.NORMAL\n",
      "        ),\n",
      "        tranSiteLog = Map(),\n",
      "        siteTable = Seq(),\n",
      "        shippingTable = Map(),\n",
      "        taxTable = Map(),\n",
      "        itemTable = Map(\n",
      "          555L -> Seq(\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 888L,\n",
      "              itemId = 8192L,\n",
      "              itemPriceHistoryId = 444L,\n",
      "              quantity = 3,\n",
      "              amount = BigDecimal(123),\n",
      "              costPrice = BigDecimal(555555),\n",
      "              taxId = 1232L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]])),\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 889L,\n",
      "              itemId = 8193L,\n",
      "              itemPriceHistoryId = 445L,\n",
      "              quantity = 5,\n",
      "              amount = BigDecimal(124),\n",
      "              costPrice = BigDecimal(555556),\n",
      "              taxId = 1234L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]]))\n",
      "          ),\n",
      "          666L -> Seq(\n",
      "            (mock(classOf[ItemName]), TransactionLogItem(\n",
      "              id = None,\n",
      "              transactionSiteId = 890L,\n",
      "              itemId = 8194L,\n",
      "              itemPriceHistoryId = 446L,\n",
      "              quantity = 1,\n",
      "              amount = BigDecimal(125),\n",
      "              costPrice = BigDecimal(555557),\n",
      "              taxId = 1235L\n",
      "            ), mock(classOf[Option[TransactionLogCoupon]]))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      val addr = mock(classOf[Address])\n",
      "      val resp: JsResult[OnSalesJsonResponse] = RecommendEngine.sendOnSales(login, tran, Some(addr), api)\n",
      "      doWith(resp.get.header) { header =>\n",
      "        header.sequenceNumber === \"1234\"\n",
      "        header.statusCode === \"OK\"\n",
      "        header.message === \"msg\"\n",
      "      }\n",
      "    }\n",
      "\n",
      "    \"Can get recommendByItem\" in {\n",
      "      val api: RecoEngApi = new RecoEngApi {\n",
      "        def onSales(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          transactionMode: TransactionMode,\n",
      "          transactionTime: Long,\n",
      "          userCode: String,\n",
      "          itemTable: Seq[SalesItem]\n",
      "        ): JsResult[OnSalesJsonResponse] = null\n",
      "\n",
      "        def recommendByItem(\n",
      "          requestTime: Long,\n",
      "          sequenceNumber: Long,\n",
      "          salesItems: Seq[SalesItem],\n",
      "          sort: SortOrder,\n",
      "          paging: JsonRequestPaging\n",
      "        ): JsResult[RecommendByItemJsonResponse] = {\n",
      "          salesItems.size === 1\n",
      "          doWith(salesItems(0)) { item =>\n",
      "            item.storeCode === \"11111\"\n",
      "            item.itemCode === \"22222\"\n",
      "          }\n",
      "          sort === Desc(\"score\")\n",
      "          paging.offset === 0\n",
      "          paging.limit === 5\n",
      "\n",
      "          JsSuccess(\n",
      "            RecommendByItemJsonResponse(\n",
      "              JsonResponseHeader(sequenceNumber = \"1234\", statusCode = \"OK\", message = \"msg\"),\n",
      "              salesItems = Seq(\n",
      "                ScoredItem(\n",
      "                  storeCode = \"1212\",\n",
      "                  itemCode = \"2323\",\n",
      "                  score = 12\n",
      "                ),\n",
      "                ScoredItem(\n",
      "                  storeCode = \"3434\",\n",
      "                  itemCode = \"4545\",\n",
      "                  score = 11\n",
      "                )\n",
      "              ),\n",
      "              \"desc(\\\"col\\\")\",\n",
      "              JsonRequestPaging(\n",
      "                offset = 2,\n",
      "                limit = 20\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        }\n",
      "      }\n",
      "\n",
      "      val result: JsResult[RecommendByItemJsonResponse] =\n",
      "        RecommendEngine.sendRecommendByItem(\n",
      "          Seq(SalesItem(storeCode = \"11111\", itemCode = \"22222\", quantity = 1)),\n",
      "          api\n",
      "        )\n",
      "      doWith(result.get) { resp =>\n",
      "        doWith(resp.header) { header =>\n",
      "          header.sequenceNumber === \"1234\"\n",
      "          header.statusCode === \"OK\"\n",
      "          header.message === \"msg\"\n",
      "        }\n",
      "        doWith(resp.salesItems) { salesItems =>\n",
      "          salesItems.size === 2\n",
      "          doWith(salesItems(0)) { item =>\n",
      "            item.storeCode === \"1212\"\n",
      "            item.itemCode === \"2323\"\n",
      "            item.score === 12f\n",
      "          }\n",
      "          doWith(salesItems(1)) { item =>\n",
      "            item.storeCode === \"3434\"\n",
      "            item.itemCode === \"4545\"\n",
      "            item.score === 11f\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example scala_1:\n",
      "package cqrs.queries\n",
      "\n",
      "import java.time.Instant\n",
      "import java.util.UUID\n",
      "\n",
      "import io.circe.{Decoder, Encoder}\n",
      "import io.circe.generic.semiauto.{deriveDecoder, deriveEncoder}\n",
      "import io.circe.java8.time._\n",
      "\n",
      "import scala.collection.immutable.SortedMap\n",
      "\n",
      "/**\n",
      "  * This is the model used for querying.\n",
      "  */\n",
      "// TODO Add useful stats\n",
      "case class Meter(id: UUID, label: String, timeSeries: SortedMap[Instant, BigDecimal])\n",
      "\n",
      "object Meter {\n",
      "\n",
      "  implicit def decodeSortedMap[A : Decoder : Ordering, B : Decoder]: Decoder[SortedMap[A, B]] =\n",
      "    Decoder[Seq[(A, B)]].map(entries => (SortedMap.newBuilder[A, B] ++= entries).result())\n",
      "\n",
      "  implicit def encodeSortedMap[A : Encoder, B : Encoder]: Encoder[SortedMap[A, B]] =\n",
      "    Encoder.encodeList[(A, B)].contramap[SortedMap[A, B]](_.to[List])\n",
      "\n",
      "  implicit val decoder: Decoder[Meter] = deriveDecoder\n",
      "  implicit val encoder: Encoder[Meter] = deriveEncoder\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example scala_2:\n",
      "/*\n",
      "Copyright 2012 Twitter, Inc.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "*/\n",
      "\n",
      "package com.twitter.algebird\n",
      "\n",
      "class VectorSpaceProperties extends CheckProperties {\n",
      "  import com.twitter.algebird.BaseVectorSpaceProperties._\n",
      "\n",
      "  // TODO: we won't need this when we have an Equatable trait\n",
      "  def mapEqFn(a: Map[Int, Double], b: Map[Int, Double]) = {\n",
      "    (a.keySet ++ b.keySet).forall { key =>\n",
      "      (a.get(key), b.get(key)) match {\n",
      "        case (Some(aVal), Some(bVal)) => beCloseTo(aVal, bVal)\n",
      "        case (Some(aVal), None) => beCloseTo(aVal, 0.0)\n",
      "        case (None, Some(bVal)) => beCloseTo(bVal, 0.0)\n",
      "        case _ => true\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  property(\"map int double scaling\") {\n",
      "    vectorSpaceLaws[Double, ({ type x[a] = Map[Int, a] })#x](mapEqFn(_, _))\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example scala_3:\n",
      "package is.hail.utils.richUtils\n",
      "\n",
      "import is.hail.expr._\n",
      "import is.hail.annotations.Region\n",
      "import is.hail.asm4s.Code\n",
      "import is.hail.expr.types._\n",
      "\n",
      "class RichCodeRegion(val region: Code[Region]) extends AnyVal {\n",
      "  def size: Code[Long] = region.invoke[Long](\"size\")\n",
      "\n",
      "  def copyFrom(other: Code[Region], readStart: Code[Long], writeStart: Code[Long], n: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Region, Long, Long, Long, Unit](\"copyFrom\", other, readStart, writeStart, n)\n",
      "  }\n",
      "\n",
      "  def storeInt(off: Code[Long], v: Code[Int]): Code[Unit] = {\n",
      "    region.invoke[Long,Int,Unit](\"storeInt\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeLong(off: Code[Long], v: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long,Long,Unit](\"storeLong\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeFloat(off: Code[Long], v: Code[Float]): Code[Unit] = {\n",
      "    region.invoke[Long,Float,Unit](\"storeFloat\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeDouble(off: Code[Long], v: Code[Double]): Code[Unit] = {\n",
      "    region.invoke[Long,Double,Unit](\"storeDouble\", off, v)\n",
      "  }\n",
      "\n",
      "  def storeAddress(off: Code[Long], a: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long,Long,Unit](\"storeAddress\", off, a)\n",
      "  }\n",
      "\n",
      "  def storeByte(off: Code[Long], b: Code[Byte]): Code[Unit] = {\n",
      "    region.invoke[Long, Byte, Unit](\"storeByte\", off, b)\n",
      "  }\n",
      "\n",
      "  def storeBytes(off: Code[Long], bytes: Code[Array[Byte]]): Code[Unit] = {\n",
      "    region.invoke[Long, Array[Byte], Unit](\"storeBytes\", off, bytes)\n",
      "  }\n",
      "\n",
      "  def allocate(alignment: Code[Long], n: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long, Long](\"allocate\", alignment, n)\n",
      "  }\n",
      "\n",
      "  def loadBoolean(off: Code[Long]): Code[Boolean] = {\n",
      "    region.invoke[Long, Boolean](\"loadBoolean\", off)\n",
      "  }\n",
      "\n",
      "  def loadByte(off: Code[Long]): Code[Byte] = {\n",
      "    region.invoke[Long, Byte](\"loadByte\", off)\n",
      "  }\n",
      "\n",
      "  def loadInt(off: Code[Long]): Code[Int] = {\n",
      "    region.invoke[Long, Int](\"loadInt\", off)\n",
      "  }\n",
      "\n",
      "  def loadLong(off: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"loadLong\", off)\n",
      "  }\n",
      "\n",
      "  def loadFloat(off: Code[Long]): Code[Float] = {\n",
      "    region.invoke[Long, Float](\"loadFloat\", off)\n",
      "  }\n",
      "\n",
      "  def loadDouble(off: Code[Long]): Code[Double] = {\n",
      "    region.invoke[Long, Double](\"loadDouble\", off)\n",
      "  }\n",
      "\n",
      "  def loadAddress(off: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"loadAddress\", off)\n",
      "  }\n",
      "\n",
      "  def loadBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Boolean] = {\n",
      "    region.invoke[Long, Long, Boolean](\"loadBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def loadIRIntermediate(typ: Type): Code[Long] => Code[_] = typ.fundamentalType match {\n",
      "    case _: TBoolean => loadBoolean\n",
      "    case _: TInt32 => loadInt\n",
      "    case _: TInt64 => loadLong\n",
      "    case _: TFloat32 => loadFloat\n",
      "    case _: TFloat64 => loadDouble\n",
      "    case _: TArray => loadAddress\n",
      "    case _: TBinary => loadAddress\n",
      "    case _: TBaseStruct => off => off\n",
      "  }\n",
      "\n",
      "  def setBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"setBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def setBit(byteOff: Code[Long], bitOff: Long): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"setBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def clearBit(byteOff: Code[Long], bitOff: Code[Long]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Unit](\"clearBit\", byteOff, bitOff)\n",
      "  }\n",
      "\n",
      "  def storeBit(byteOff: Code[Long], bitOff: Code[Long], b: Code[Boolean]): Code[Unit] = {\n",
      "    region.invoke[Long, Long, Boolean, Unit](\"setBit\", byteOff, bitOff, b)\n",
      "  }\n",
      "\n",
      "  def appendInt(i: Code[Int]): Code[Long] = {\n",
      "    region.invoke[Int, Long](\"appendInt\", i)\n",
      "  }\n",
      "\n",
      "  def appendLong(l: Code[Long]): Code[Long] = {\n",
      "    region.invoke[Long, Long](\"appendLong\", l)\n",
      "  }\n",
      "\n",
      "  def appendFloat(f: Code[Float]): Code[Long] = {\n",
      "    region.invoke[Float, Long](\"appendFloat\", f)\n",
      "  }\n",
      "\n",
      "  def appendDouble(d: Code[Double]): Code[Long] = {\n",
      "    region.invoke[Double, Long](\"appendDouble\", d)\n",
      "  }\n",
      "\n",
      "  def appendByte(b: Code[Byte]): Code[Long] = {\n",
      "    region.invoke[Byte, Long](\"appendByte\", b)\n",
      "  }\n",
      "\n",
      "  def appendBytes(bytes: Code[Array[Byte]]): Code[Long] = {\n",
      "    region.invoke[Array[Byte], Long](\"appendBytes\", bytes)\n",
      "  }\n",
      "\n",
      "  def appendBytes(bytes: Code[Array[Byte]], bytesOff: Code[Long], n: Code[Int]): Code[Long] = {\n",
      "    region.invoke[Array[Byte],Long, Int, Long](\"appendBytes\", bytes, bytesOff, n)\n",
      "  }\n",
      "\n",
      "  def appendString(string: Code[String]): Code[Long] = {\n",
      "    region.invoke[String, Long](\"appendString\", string)\n",
      "  }\n",
      "\n",
      "  def clear(): Code[Unit] = {\n",
      "    region.invoke[Unit](\"clear\")\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example scala_4:\n",
      "<filename>scala-koans/src/test/scala/org/functionalkoans/forscala/AboutMutableSets.scala\n",
      "package org.functionalkoans.forscala\n",
      "\n",
      "import support.KoanSuite\n",
      "import scala.collection.mutable\n",
      "\n",
      "class AboutMutableSets extends KoanSuite {\n",
      "  koan(\"Mutable sets can be created easily\") {\n",
      "    val mySet = mutable.Set(\"Michigan\", \"Ohio\", \"Wisconsin\", \"Iowa\")\n",
      "    mySet.size should be(4)\n",
      "    mySet += \"Oregon\"\n",
      "    mySet contains \"Oregon\" should be(true)\n",
      "  }\n",
      "\n",
      "  koan(\"Mutable sets can have elements removed\") {\n",
      "    val mySet = mutable.Set(\"Michigan\", \"Ohio\", \"Wisconsin\", \"Iowa\")\n",
      "    mySet -= \"Ohio\"\n",
      "    mySet contains \"Ohio\" should be(false)\n",
      "  }\n",
      "\n",
      "  koan(\"Mutable sets can have tuples of elements removed\") {\n",
      "    val mySet = mutable.Set(\"Michigan\", \"Ohio\", \"Wisconsin\", \"Iowa\")\n",
      "    mySet -= (\"Iowa\", \"Ohio\")\n",
      "    mySet contains \"Ohio\" should be(false)\n",
      "    mySet.size should be(2)\n",
      "  }\n",
      "\n",
      "  koan(\"Mutable sets can have tuples of elements added\") {\n",
      "    val mySet = mutable.Set(\"Michigan\", \"Wisconsin\")\n",
      "    mySet += (\"Iowa\", \"Ohio\")\n",
      "    mySet contains \"Ohio\" should be(true)\n",
      "    mySet.size should be(4)\n",
      "  }\n",
      "\n",
      "  koan(\"Mutable sets can have Lists of elements added\") {\n",
      "    val mySet = mutable.Set(\"Michigan\", \"Wisconsin\")\n",
      "    mySet ++= List(\"Iowa\", \"Ohio\")\n",
      "    mySet contains \"Ohio\" should be(true)\n",
      "    mySet.size should be(4)\n",
      "  }\n",
      "\n",
      "  koan(\"Mutable sets can have Lists of elements removed\") {\n",
      "    val mySet = mutable.Set(\"Michigan\", \"Ohio\", \"Wisconsin\", \"Iowa\")\n",
      "    mySet --= List(\"Iowa\", \"Ohio\")\n",
      "    mySet contains \"Ohio\" should be(false)\n",
      "    mySet.size should be(2)\n",
      "  }\n",
      "\n",
      "  koan(\"Mutable sets can be cleared\") {\n",
      "    val mySet = mutable.Set(\"Michigan\", \"Ohio\", \"Wisconsin\", \"Iowa\")\n",
      "    mySet.clear() // Convention is to use parens if possible when method called changes state\n",
      "    mySet contains \"Ohio\" should be(false)\n",
      "    mySet.size should be(0)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (k,v) in enumerate(data_dict_scala.items()):\n",
    "    if i < 5:\n",
    "        print(f\"Example {k}:\")\n",
    "        print(v)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to 'scala_train.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('scala_train.json', 'w') as f:\n",
    "    json.dump(data_dict_scala, f)\n",
    "    print(\"Saved to 'scala_train.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malcodeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
