{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "# First, download the dataset with caching\n",
    "def download_starcoderdata_python(save_directory, split=\"train\", download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS):\n",
    "    try:\n",
    "        ds = load_dataset(\n",
    "            \"bigcode/starcoderdata\",\n",
    "            data_dir=\"python\",\n",
    "            split=split,\n",
    "            cache_dir=save_directory,\n",
    "            download_mode=download_mode,\n",
    "        )\n",
    "\n",
    "        # Save the dataset properly for later reloading\n",
    "        output_path = f\"{save_directory}/python_{split}_dataset\"\n",
    "        ds.save_to_disk(output_path)\n",
    "\n",
    "        print(f\"Dataset 'bigcode/starcoderdata' (PYTHON, {split}) successfully downloaded and saved to '{output_path}'.\")\n",
    "        return ds\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset 'bigcode/starcoderdata' (PYTHON, {split}): {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5631a1279fb14b36919f2df39da9fbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfec7ebfacf844359caf4142644d9a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda8e485dbba4fa1af3d02ae0a01b576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/124 shards):   0%|          | 0/12866649 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'bigcode/starcoderdata' (PYTHON, train) successfully downloaded and saved to './/python_train_dataset'.\n"
     ]
    }
   ],
   "source": [
    "ds = download_starcoderdata_python(save_directory=\"./\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866d310ef94445e49c58f8a39054576c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Later, load the dataset from the saved location\n",
    "dataset = load_from_disk(\"./python_train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id', 'content'],\n",
       "    num_rows: 12866649\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_stars_repo_path': 'public_data/serializers.py', 'max_stars_repo_name': 'MTES-MCT/sparte', 'max_stars_count': 0, 'id': '0', 'content': '<reponame>MTES-MCT/sparte\\nfrom rest_framework_gis import serializers\\nfrom rest_framework import serializers as s\\n\\nfrom .models import (\\n    Artificialisee2015to2018,\\n    Artificielle2018,\\n    CommunesSybarval,\\n    CouvertureSol,\\n    EnveloppeUrbaine2018,\\n    Ocsge,\\n    Renaturee2018to2015,\\n    Sybarval,\\n    Voirie2018,\\n    ZonesBaties2018,\\n    UsageSol,\\n)\\n\\n\\ndef get_label(code=\"\", label=\"\"):\\n    if code is None:\\n        code = \"-\"\\n    if label is None:\\n        label = \"inconnu\"\\n    return f\"{code} {label[:30]}\"\\n\\n\\nclass Artificialisee2015to2018Serializer(serializers.GeoFeatureModelSerializer):\\n    usage_2015 = s.SerializerMethodField()\\n    usage_2018 = s.SerializerMethodField()\\n    couverture_2015 = s.SerializerMethodField()\\n    couverture_2018 = s.SerializerMethodField()\\n\\n    def get_usage_2015(self, obj):\\n        return get_label(code=obj.us_2015, label=obj.us_2015_label)\\n\\n    def get_usage_2018(self, obj):\\n        return get_label(code=obj.us_2018, label=obj.us_2018_label)\\n\\n    def get_couverture_2015(self, obj):\\n        return get_label(code=obj.cs_2015, label=obj.cs_2015_label)\\n\\n    def get_couverture_2018(self, obj):\\n        return get_label(code=obj.cs_2018, label=obj.cs_2018_label)\\n\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"surface\",\\n            \"usage_2015\",\\n            \"usage_2018\",\\n            \"couverture_2015\",\\n            \"couverture_2018\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = Artificialisee2015to2018\\n\\n\\nclass Artificielle2018Serializer(serializers.GeoFeatureModelSerializer):\\n    couverture = s.SerializerMethodField()\\n\\n    def get_couverture(self, obj):\\n        return get_label(code=obj.couverture, label=obj.couverture_label)\\n\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"surface\",\\n            \"couverture\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = Artificielle2018\\n\\n\\nclass CommunesSybarvalSerializer(serializers.GeoFeatureModelSerializer):\\n    \"\"\"Marker GeoJSON serializer.\"\"\"\\n\\n    class Meta:\\n        \"\"\"Marker serializer meta class.\"\"\"\\n\\n        fields = (\\n            \"nom\",\\n            \"code_insee\",\\n            \"surface\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = CommunesSybarval\\n\\n\\nclass EnveloppeUrbaine2018Serializer(serializers.GeoFeatureModelSerializer):\\n    couverture = s.SerializerMethodField()\\n\\n    def get_couverture(self, obj):\\n        return get_label(code=obj.couverture, label=obj.couverture_label)\\n\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"couverture\",\\n            \"surface\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = EnveloppeUrbaine2018\\n\\n\\nclass OcsgeSerializer(serializers.GeoFeatureModelSerializer):\\n    couverture = s.SerializerMethodField()\\n    usage = s.SerializerMethodField()\\n\\n    def get_couverture(self, obj):\\n        return get_label(code=obj.couverture, label=obj.couverture_label)\\n\\n    def get_usage(self, obj):\\n        return get_label(code=obj.usage, label=obj.usage_label)\\n\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"couverture\",\\n            \"usage\",\\n            \"millesime\",\\n            \"map_color\",\\n            \"year\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = Ocsge\\n\\n\\nclass Renaturee2018to2015Serializer(serializers.GeoFeatureModelSerializer):\\n    usage_2015 = s.SerializerMethodField()\\n    usage_2018 = s.SerializerMethodField()\\n    couverture_2015 = s.SerializerMethodField()\\n    couverture_2018 = s.SerializerMethodField()\\n\\n    def get_usage_2015(self, obj):\\n        return get_label(code=obj.us_2015, label=obj.us_2015_label)\\n\\n    def get_usage_2018(self, obj):\\n        return get_label(code=obj.us_2018, label=obj.us_2018_label)\\n\\n    def get_couverture_2015(self, obj):\\n        return get_label(code=obj.cs_2015, label=obj.cs_2015_label)\\n\\n    def get_couverture_2018(self, obj):\\n        return get_label(code=obj.cs_2018, label=obj.cs_2018_label)\\n\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"surface\",\\n            \"usage_2015\",\\n            \"usage_2018\",\\n            \"couverture_2015\",\\n            \"couverture_2018\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = Renaturee2018to2015\\n\\n\\nclass SybarvalSerializer(serializers.GeoFeatureModelSerializer):\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"surface\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = Sybarval\\n\\n\\nclass Voirie2018Serializer(serializers.GeoFeatureModelSerializer):\\n    couverture = s.SerializerMethodField()\\n    usage = s.SerializerMethodField()\\n\\n    def get_couverture(self, obj):\\n        return get_label(code=obj.couverture, label=obj.couverture_label)\\n\\n    def get_usage(self, obj):\\n        return get_label(code=obj.usage, label=obj.usage_label)\\n\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"surface\",\\n            \"couverture\",\\n            \"usage\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = Voirie2018\\n\\n\\nclass ZonesBaties2018Serializer(serializers.GeoFeatureModelSerializer):\\n    couverture = s.SerializerMethodField()\\n    usage = s.SerializerMethodField()\\n\\n    def get_couverture(self, obj):\\n        return get_label(code=obj.couverture, label=obj.couverture_label)\\n\\n    def get_usage(self, obj):\\n        return get_label(code=obj.usage, label=obj.usage_label)\\n\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"couverture\",\\n            \"usage\",\\n            \"surface\",\\n        )\\n        geo_field = \"mpoly\"\\n        model = ZonesBaties2018\\n\\n\\nclass CouvertureSolSerializer(serializers.ModelSerializer):\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"parent\",\\n            \"code\",\\n            \"label\",\\n            \"is_artificial\",\\n        )\\n        model = CouvertureSol\\n\\n\\nclass UsageSolSerializer(serializers.ModelSerializer):\\n    class Meta:\\n        fields = (\\n            \"id\",\\n            \"parent\",\\n            \"code\",\\n            \"label\",\\n        )\\n        model = UsageSol\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sort(column_names=[\"max_stars_count\", \"max_stars_repo_name\"], reverse=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "from argparse import ArgumentParser, _HelpAction\n",
      "from pkgutil import get_data\n",
      "from sys import exit\n",
      "\n",
      "# flie basename no extension\n",
      "LICENSES = [\n",
      "    \"agpl-3.0\",\n",
      "    \"apache-2.0\",\n",
      "    \"bsd-2-clause\",\n",
      "    \"bsd-3-clause\",\n",
      "    \"epl-2.0\",\n",
      "    \"gpl-2.0\",\n",
      "    \"gpl-3.0\",\n",
      "    \"lgpl-2.1\",\n",
      "    \"lgpl-3.0\",\n",
      "    \"mit\",\n",
      "    \"mpl-2.0\",\n",
      "    \"unlicenses\",\n",
      "    \"996icu-0.1\",\n",
      "]\n",
      "\n",
      "\n",
      "def getparser():\n",
      "    parser = ArgumentParser(\n",
      "        prog=\"gen-license\",\n",
      "        description=\"tools to create license file, support GitHub LICENSE code.\",\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"code\", help=\"LICENSE Code, --list to see\", choices=LICENSES,\n",
      "        nargs=\"?\", const=None\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--list\", dest=\"list\", help=\"Show supported LICENSE Codes\", required=False,\n",
      "        action=\"store_true\"\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--996icu\", dest=\"icu\", help=\"Expand LICENSE with 996ICU LICENSE, Choose a language vesion or default zh-cn\",\n",
      "        required=False, nargs=\"?\", const=\"zh-cn\", default=None,\n",
      "        choices=[\"en-us\", \"zh-cn\"]\n",
      "    )\n",
      "\n",
      "    return parser\n",
      "\n",
      "\n",
      "def select_template(language_code):\n",
      "    \"\"\"choose a 996icu LICENSE template according to *language_code*\n",
      "    \"\"\"\n",
      "    map_ = {\n",
      "        \"zh\": \"zh-cn\",\n",
      "        \"zh-cn\": \"zh-cn\",\n",
      "        \"zh-hans\": \"zh-cn\",\n",
      "        \"en\": \"en-us\",\n",
      "        \"en-us\": \"en-us\",\n",
      "    }\n",
      "\n",
      "    template = get_data(\n",
      "        __package__,\n",
      "        \"licenses/996.icu.template.{}.txt\".format(\n",
      "            map_.get(language_code, \"zh-cn\")\n",
      "        )\n",
      "    ).decode(\"utf-8\")\n",
      "\n",
      "    return template\n",
      "\n",
      "\n",
      "def main():\n",
      "    parser = getparser()\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    if args.list:\n",
      "        for license in LICENSES:\n",
      "            print(license)\n",
      "\n",
      "        exit(0)\n",
      "    else:  # main\n",
      "\n",
      "        # if no args input, show help and exit\n",
      "        if args.code is None:\n",
      "            parser.print_help()\n",
      "            parser.exit()\n",
      "\n",
      "        resource = get_data(\n",
      "            __package__,\n",
      "            \"licenses/{code}.txt\".format(code=args.code)\n",
      "        ).decode(\"utf-8\")\n",
      "\n",
      "        if args.icu is not None:  # --996icu option enabled\n",
      "            template = select_template(args.icu)\n",
      "\n",
      "            output = template.format(\n",
      "                other=args.code,\n",
      "                content=resource\n",
      "            ).encode(\"utf-8\")\n",
      "\n",
      "        else:  # common license\n",
      "            output = resource.encode(\"utf-8\")\n",
      "\n",
      "        with open(\"LICENSE\", \"wb\") as file:\n",
      "            file.write(output)\n",
      "\n",
      "        exit(0)\n",
      "\n",
      "\n",
      "\n",
      "Example 1:\n",
      "<reponame>vegYY/react\n",
      "{\n",
      "  \"targets\": [\n",
      "    {\n",
      "      \"target_name\": \"perfcounters\",\n",
      "      \"sources\": [\n",
      "        \"src/hardware-counter.cpp\",\n",
      "        \"src/perf-counters.cpp\",\n",
      "        \"src/thread-local.cpp\",\n",
      "      ],\n",
      "      \"cflags\": [\n",
      "        \"-Wno-sign-compare\",\n",
      "      ],\n",
      "    },\n",
      "  ],\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 2:\n",
      "<reponame>EricRemmerswaal/tensorflow<filename>tensorflow/python/debug/lib/debug_events_writer_test.py\n",
      "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests for the debug events writer Python class.\"\"\"\n",
      "\n",
      "import glob\n",
      "import json as json_lib\n",
      "import os\n",
      "import re\n",
      "import threading\n",
      "import time\n",
      "\n",
      "from absl.testing import parameterized\n",
      "\n",
      "from tensorflow.core.protobuf import debug_event_pb2\n",
      "from tensorflow.python.debug.lib import debug_events_reader\n",
      "from tensorflow.python.debug.lib import debug_events_writer\n",
      "from tensorflow.python.debug.lib import dumping_callback_test_lib\n",
      "from tensorflow.python.framework import ops\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.framework import versions\n",
      "from tensorflow.python.platform import googletest\n",
      "\n",
      "\n",
      "class DebugEventsWriterTest(dumping_callback_test_lib.DumpingCallbackTestBase,\n",
      "                            parameterized.TestCase):\n",
      "\n",
      "  def testMultiThreadedConstructorCallWorks(self):\n",
      "    def init_writer():\n",
      "      debug_events_writer.DebugEventsWriter(self.dump_root, self.tfdbg_run_id)\n",
      "\n",
      "    num_threads = 4\n",
      "    threads = []\n",
      "    for _ in range(num_threads):\n",
      "      thread = threading.Thread(target=init_writer)\n",
      "      thread.start()\n",
      "      threads.append(thread)\n",
      "    for thread in threads:\n",
      "      thread.join()\n",
      "\n",
      "    # Verify that there is only one debug event file of each type.\n",
      "    metadata_paths = glob.glob(os.path.join(self.dump_root, \"*.metadata\"))\n",
      "    self.assertLen(metadata_paths, 1)\n",
      "    source_files_paths = glob.glob(\n",
      "        os.path.join(self.dump_root, \"*.source_files\"))\n",
      "    self.assertLen(source_files_paths, 1)\n",
      "    stack_frames_paths = glob.glob(\n",
      "        os.path.join(self.dump_root, \"*.stack_frames\"))\n",
      "    self.assertLen(stack_frames_paths, 1)\n",
      "    graphs_paths = glob.glob(os.path.join(self.dump_root, \"*.graphs\"))\n",
      "    self.assertLen(graphs_paths, 1)\n",
      "    self._readAndCheckMetadataFile()\n",
      "\n",
      "  def testWriteSourceFilesAndStackFrames(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    num_protos = 10\n",
      "    for i in range(num_protos):\n",
      "      source_file = debug_event_pb2.SourceFile()\n",
      "      source_file.file_path = \"/home/tf2user/main.py\"\n",
      "      source_file.host_name = \"machine.cluster\"\n",
      "      source_file.lines.append(\"print(%d)\" % i)\n",
      "      writer.WriteSourceFile(source_file)\n",
      "\n",
      "      stack_frame = debug_event_pb2.StackFrameWithId()\n",
      "      stack_frame.id = \"stack_%d\" % i\n",
      "      stack_frame.file_line_col.file_index = i * 10\n",
      "      writer.WriteStackFrameWithId(stack_frame)\n",
      "\n",
      "    writer.FlushNonExecutionFiles()\n",
      "\n",
      "    with debug_events_reader.DebugEventsReader(self.dump_root) as reader:\n",
      "      actuals = list(item.debug_event.source_file\n",
      "                     for item in reader.source_files_iterator())\n",
      "      self.assertLen(actuals, num_protos)\n",
      "      for i in range(num_protos):\n",
      "        self.assertEqual(actuals[i].file_path, \"/home/tf2user/main.py\")\n",
      "        self.assertEqual(actuals[i].host_name, \"machine.cluster\")\n",
      "        self.assertEqual(actuals[i].lines, [\"print(%d)\" % i])\n",
      "\n",
      "      actuals = list(item.debug_event.stack_frame_with_id\n",
      "                     for item in reader.stack_frames_iterator())\n",
      "      self.assertLen(actuals, num_protos)\n",
      "      for i in range(num_protos):\n",
      "        self.assertEqual(actuals[i].id, \"stack_%d\" % i)\n",
      "        self.assertEqual(actuals[i].file_line_col.file_index, i * 10)\n",
      "\n",
      "  def testWriteGraphOpCreationAndDebuggedGraphs(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    num_op_creations = 10\n",
      "    for i in range(num_op_creations):\n",
      "      graph_op_creation = debug_event_pb2.GraphOpCreation()\n",
      "      graph_op_creation.op_type = \"Conv2D\"\n",
      "      graph_op_creation.op_name = \"Conv2D_%d\" % i\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph()\n",
      "    debugged_graph.graph_id = \"deadbeaf\"\n",
      "    debugged_graph.graph_name = \"MyGraph1\"\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "\n",
      "    reader = debug_events_reader.DebugEventsReader(self.dump_root)\n",
      "    actuals = list(item.debug_event for item in reader.graphs_iterator())\n",
      "    self.assertLen(actuals, num_op_creations + 1)\n",
      "    for i in range(num_op_creations):\n",
      "      self.assertEqual(actuals[i].graph_op_creation.op_type, \"Conv2D\")\n",
      "      self.assertEqual(actuals[i].graph_op_creation.op_name, \"Conv2D_%d\" % i)\n",
      "    self.assertEqual(actuals[num_op_creations].debugged_graph.graph_id,\n",
      "                     \"deadbeaf\")\n",
      "\n",
      "  def testConcurrentWritesToNonExecutionFilesWorks(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "\n",
      "    source_file_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def writer_source_file():\n",
      "      source_file = debug_event_pb2.SourceFile()\n",
      "      with source_file_state[\"lock\"]:\n",
      "        source_file.file_path = \"/home/tf2user/file_%d.py\" % source_file_state[\n",
      "            \"counter\"]\n",
      "        source_file_state[\"counter\"] += 1\n",
      "      writer.WriteSourceFile(source_file)\n",
      "      # More-frequent-than-necessary concurrent flushing is not recommended,\n",
      "      # but tolerated.\n",
      "      writer.FlushNonExecutionFiles()\n",
      "\n",
      "    stack_frame_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def write_stack_frame():\n",
      "      stack_frame = debug_event_pb2.StackFrameWithId()\n",
      "      with stack_frame_state[\"lock\"]:\n",
      "        stack_frame.id = \"stack_frame_%d\" % stack_frame_state[\"counter\"]\n",
      "        stack_frame_state[\"counter\"] += 1\n",
      "      writer.WriteStackFrameWithId(stack_frame)\n",
      "      # More-frequent-than-necessary concurrent flushing is not recommended,\n",
      "      # but tolerated.\n",
      "      writer.FlushNonExecutionFiles()\n",
      "\n",
      "    graph_op_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def write_graph_op_creation():\n",
      "      graph_op_creation = debug_event_pb2.GraphOpCreation()\n",
      "      with graph_op_state[\"lock\"]:\n",
      "        graph_op_creation.op_name = \"Op%d\" % graph_op_state[\"counter\"]\n",
      "        graph_op_state[\"counter\"] += 1\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      # More-frequent-than-necessary concurrent flushing is not recommended,\n",
      "      # but tolerated.\n",
      "      writer.FlushNonExecutionFiles()\n",
      "\n",
      "    num_threads = 9\n",
      "    threads = []\n",
      "    for i in range(num_threads):\n",
      "      if i % 3 == 0:\n",
      "        target = writer_source_file\n",
      "      elif i % 3 == 1:\n",
      "        target = write_stack_frame\n",
      "      else:\n",
      "        target = write_graph_op_creation\n",
      "      thread = threading.Thread(target=target)\n",
      "      thread.start()\n",
      "      threads.append(thread)\n",
      "    for thread in threads:\n",
      "      thread.join()\n",
      "\n",
      "    # Verify the content of the .source_files file.\n",
      "    with debug_events_reader.DebugEventsReader(self.dump_root) as reader:\n",
      "      source_files_iter = reader.source_files_iterator()\n",
      "      actuals = list(item.debug_event.source_file for item in source_files_iter)\n",
      "      file_paths = sorted([actual.file_path for actual in actuals])\n",
      "      self.assertEqual(file_paths, [\n",
      "          \"/home/tf2user/file_0.py\", \"/home/tf2user/file_1.py\",\n",
      "          \"/home/tf2user/file_2.py\"\n",
      "      ])\n",
      "\n",
      "    # Verify the content of the .stack_frames file.\n",
      "    actuals = list(item.debug_event.stack_frame_with_id\n",
      "                   for item in reader.stack_frames_iterator())\n",
      "    stack_frame_ids = sorted([actual.id for actual in actuals])\n",
      "    self.assertEqual(stack_frame_ids,\n",
      "                     [\"stack_frame_0\", \"stack_frame_1\", \"stack_frame_2\"])\n",
      "\n",
      "    # Verify the content of the .graphs file.\n",
      "    actuals = list(item.debug_event.graph_op_creation\n",
      "                   for item in reader.graphs_iterator())\n",
      "    graph_op_names = sorted([actual.op_name for actual in actuals])\n",
      "    self.assertEqual(graph_op_names, [\"Op0\", \"Op1\", \"Op2\"])\n",
      "\n",
      "  def testWriteAndReadMetadata(self):\n",
      "    t0 = time.time()\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    writer.Close()\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      self.assertIsInstance(reader.starting_wall_time(), float)\n",
      "      self.assertGreaterEqual(reader.starting_wall_time(), t0)\n",
      "      self.assertEqual(reader.tensorflow_version(), versions.__version__)\n",
      "      self.assertTrue(reader.tfdbg_run_id())\n",
      "\n",
      "  def testWriteExecutionEventsWithCircularBuffer(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    num_execution_events = debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE * 2\n",
      "    for i in range(num_execution_events):\n",
      "      execution = debug_event_pb2.Execution()\n",
      "      execution.op_type = \"OpType%d\" % i\n",
      "      writer.WriteExecution(execution)\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      # Before FlushExecutionFiles() is called. No data should have been written\n",
      "      # to the file.\n",
      "      reader.update()\n",
      "      self.assertFalse(reader.executions())\n",
      "\n",
      "      writer.FlushExecutionFiles()\n",
      "      reader.update()\n",
      "      executions = reader.executions()\n",
      "      for i, execution in enumerate(executions):\n",
      "        self.assertEqual(\n",
      "            execution.op_type,\n",
      "            \"OpType%d\" % (i + debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE))\n",
      "\n",
      "  def testWriteExecutionEventsWithoutCircularBufferBehavior(self):\n",
      "    # A circular buffer size of 0 abolishes the circular buffer behavior.\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id, 0)\n",
      "    num_execution_events = debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE * 2\n",
      "    for i in range(num_execution_events):\n",
      "      execution = debug_event_pb2.Execution()\n",
      "      execution.op_type = \"OpType%d\" % i\n",
      "      writer.WriteExecution(execution)\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      reader.update()\n",
      "      executions = reader.executions()\n",
      "      self.assertLen(executions, num_execution_events)\n",
      "      for i, execution in enumerate(executions):\n",
      "        self.assertEqual(execution.op_type, \"OpType%d\" % i)\n",
      "\n",
      "  def testWriteGraphExecutionTraceEventsWithCircularBuffer(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    num_execution_events = debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE * 2\n",
      "    for i in range(num_execution_events):\n",
      "      trace = debug_event_pb2.GraphExecutionTrace()\n",
      "      trace.op_name = \"Op%d\" % i\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "\n",
      "    with debug_events_reader.DebugEventsReader(self.dump_root) as reader:\n",
      "      actuals = list(reader.graph_execution_traces_iterators()[0])\n",
      "      # Before FlushExecutionFiles() is called. No data should have been written\n",
      "      # to the file.\n",
      "      self.assertEmpty(actuals)\n",
      "\n",
      "      writer.FlushExecutionFiles()\n",
      "      actuals = list(item.debug_event.graph_execution_trace\n",
      "                     for item in reader.graph_execution_traces_iterators()[0])\n",
      "      self.assertLen(actuals, debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE)\n",
      "      for i in range(debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE):\n",
      "        self.assertEqual(\n",
      "            actuals[i].op_name,\n",
      "            \"Op%d\" % (i + debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE))\n",
      "\n",
      "  def testWriteGraphExecutionTraceEventsWithoutCircularBufferBehavior(self):\n",
      "    # A circular buffer size of 0 abolishes the circular buffer behavior.\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id, 0)\n",
      "    num_execution_events = debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE * 2\n",
      "    for i in range(num_execution_events):\n",
      "      trace = debug_event_pb2.GraphExecutionTrace()\n",
      "      trace.op_name = \"Op%d\" % i\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    with debug_events_reader.DebugEventsReader(self.dump_root) as reader:\n",
      "      actuals = list(item.debug_event.graph_execution_trace\n",
      "                     for item in reader.graph_execution_traces_iterators()[0])\n",
      "    self.assertLen(actuals, num_execution_events)\n",
      "    for i in range(num_execution_events):\n",
      "      self.assertEqual(actuals[i].op_name, \"Op%d\" % i)\n",
      "\n",
      "  def testConcurrentWritesToExecutionFiles(self):\n",
      "    circular_buffer_size = 5\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph(graph_id=\"graph1\",\n",
      "                                                   graph_name=\"graph1\")\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "\n",
      "    execution_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def write_execution():\n",
      "      execution = debug_event_pb2.Execution()\n",
      "      with execution_state[\"lock\"]:\n",
      "        execution.op_type = \"OpType%d\" % execution_state[\"counter\"]\n",
      "        execution_state[\"counter\"] += 1\n",
      "      writer.WriteExecution(execution)\n",
      "\n",
      "    graph_execution_trace_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def write_graph_execution_trace():\n",
      "      with graph_execution_trace_state[\"lock\"]:\n",
      "        op_name = \"Op%d\" % graph_execution_trace_state[\"counter\"]\n",
      "        graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "            op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "        trace = debug_event_pb2.GraphExecutionTrace(\n",
      "            op_name=op_name, tfdbg_context_id=\"graph1\")\n",
      "        graph_execution_trace_state[\"counter\"] += 1\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "\n",
      "    threads = []\n",
      "    for i in range(circular_buffer_size * 4):\n",
      "      if i % 2 == 0:\n",
      "        target = write_execution\n",
      "      else:\n",
      "        target = write_graph_execution_trace\n",
      "      thread = threading.Thread(target=target)\n",
      "      thread.start()\n",
      "      threads.append(thread)\n",
      "    for thread in threads:\n",
      "      thread.join()\n",
      "    writer.FlushNonExecutionFiles()\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      reader.update()\n",
      "      # Verify the content of the .execution file.\n",
      "      executions = reader.executions()\n",
      "      executed_op_types = [execution.op_type for execution in executions]\n",
      "      self.assertLen(executed_op_types, circular_buffer_size)\n",
      "      self.assertLen(executed_op_types, len(set(executed_op_types)))\n",
      "\n",
      "      # Verify the content of the .graph_execution_traces file.\n",
      "      op_names = [trace.op_name for trace in reader.graph_execution_traces()]\n",
      "      self.assertLen(op_names, circular_buffer_size)\n",
      "      self.assertLen(op_names, len(set(op_names)))\n",
      "\n",
      "  def testConcurrentSourceFileRandomReads(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "\n",
      "    for i in range(100):\n",
      "      source_file = debug_event_pb2.SourceFile(\n",
      "          host_name=\"localhost\", file_path=\"/tmp/file_%d.py\" % i)\n",
      "      source_file.lines.append(\"# File %d\" % i)\n",
      "      writer.WriteSourceFile(source_file)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "\n",
      "    reader = debug_events_reader.DebugDataReader(self.dump_root)\n",
      "    reader.update()\n",
      "    lines = [None] * 100\n",
      "    def read_job_1():\n",
      "      # Read in the reverse order to enhance randomness of the read access.\n",
      "      for i in range(49, -1, -1):\n",
      "        lines[i] = reader.source_lines(\"localhost\", \"/tmp/file_%d.py\" % i)\n",
      "    def read_job_2():\n",
      "      for i in range(99, 49, -1):\n",
      "        lines[i] = reader.source_lines(\"localhost\", \"/tmp/file_%d.py\" % i)\n",
      "    thread_1 = threading.Thread(target=read_job_1)\n",
      "    thread_2 = threading.Thread(target=read_job_2)\n",
      "    thread_1.start()\n",
      "    thread_2.start()\n",
      "    thread_1.join()\n",
      "    thread_2.join()\n",
      "    for i in range(100):\n",
      "      self.assertEqual(lines[i], [\"# File %d\" % i])\n",
      "\n",
      "  def testConcurrentExecutionUpdateAndRandomRead(self):\n",
      "    circular_buffer_size = -1\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "\n",
      "    writer_state = {\"counter\": 0, \"done\": False}\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      def write_and_update_job():\n",
      "        while True:\n",
      "          if writer_state[\"done\"]:\n",
      "            break\n",
      "          execution = debug_event_pb2.Execution()\n",
      "          execution.op_type = \"OpType%d\" % writer_state[\"counter\"]\n",
      "          writer_state[\"counter\"] += 1\n",
      "          writer.WriteExecution(execution)\n",
      "          writer.FlushExecutionFiles()\n",
      "          reader.update()\n",
      "      # On the sub-thread, keep writing and reading new Execution protos.\n",
      "      write_and_update_thread = threading.Thread(target=write_and_update_job)\n",
      "      write_and_update_thread.start()\n",
      "      # On the main thread, do concurrent random read.\n",
      "      while True:\n",
      "        exec_digests = reader.executions(digest=True)\n",
      "        if exec_digests:\n",
      "          exec_0 = reader.read_execution(exec_digests[0])\n",
      "          self.assertEqual(exec_0.op_type, \"OpType0\")\n",
      "          writer_state[\"done\"] = True\n",
      "          break\n",
      "        else:\n",
      "          time.sleep(0.1)\n",
      "          continue\n",
      "      write_and_update_thread.join()\n",
      "\n",
      "  def testConcurrentExecutionRandomReads(self):\n",
      "    circular_buffer_size = -1\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "\n",
      "    for i in range(100):\n",
      "      execution = debug_event_pb2.Execution()\n",
      "      execution.op_type = \"OpType%d\" % i\n",
      "      writer.WriteExecution(execution)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    reader = debug_events_reader.DebugDataReader(self.dump_root)\n",
      "    reader.update()\n",
      "    executions = [None] * 100\n",
      "    def read_job_1():\n",
      "      execution_digests = reader.executions(digest=True)\n",
      "      # Read in the reverse order to enhance randomness of the read access.\n",
      "      for i in range(49, -1, -1):\n",
      "        execution = reader.read_execution(execution_digests[i])\n",
      "        executions[i] = execution\n",
      "    def read_job_2():\n",
      "      execution_digests = reader.executions(digest=True)\n",
      "      for i in range(99, 49, -1):\n",
      "        execution = reader.read_execution(execution_digests[i])\n",
      "        executions[i] = execution\n",
      "    thread_1 = threading.Thread(target=read_job_1)\n",
      "    thread_2 = threading.Thread(target=read_job_2)\n",
      "    thread_1.start()\n",
      "    thread_2.start()\n",
      "    thread_1.join()\n",
      "    thread_2.join()\n",
      "    for i in range(100):\n",
      "      self.assertEqual(executions[i].op_type, \"OpType%d\" % i)\n",
      "\n",
      "  def testConcurrentGraphExecutionTraceUpdateAndRandomRead(self):\n",
      "    circular_buffer_size = -1\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph(graph_id=\"graph1\",\n",
      "                                                   graph_name=\"graph1\")\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "\n",
      "    writer_state = {\"counter\": 0, \"done\": False}\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      def write_and_update_job():\n",
      "        while True:\n",
      "          if writer_state[\"done\"]:\n",
      "            break\n",
      "          op_name = \"Op%d\" % writer_state[\"counter\"]\n",
      "          graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "              op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "          writer.WriteGraphOpCreation(graph_op_creation)\n",
      "          trace = debug_event_pb2.GraphExecutionTrace(\n",
      "              op_name=op_name, tfdbg_context_id=\"graph1\")\n",
      "          writer.WriteGraphExecutionTrace(trace)\n",
      "          writer_state[\"counter\"] += 1\n",
      "          writer.FlushNonExecutionFiles()\n",
      "          writer.FlushExecutionFiles()\n",
      "          reader.update()\n",
      "      # On the sub-thread, keep writing and reading new GraphExecutionTraces.\n",
      "      write_and_update_thread = threading.Thread(target=write_and_update_job)\n",
      "      write_and_update_thread.start()\n",
      "      # On the main thread, do concurrent random read.\n",
      "      while True:\n",
      "        digests = reader.graph_execution_traces(digest=True)\n",
      "        if digests:\n",
      "          trace_0 = reader.read_graph_execution_trace(digests[0])\n",
      "          self.assertEqual(trace_0.op_name, \"Op0\")\n",
      "          writer_state[\"done\"] = True\n",
      "          break\n",
      "        else:\n",
      "          time.sleep(0.1)\n",
      "          continue\n",
      "      write_and_update_thread.join()\n",
      "\n",
      "  def testConcurrentGraphExecutionTraceRandomReads(self):\n",
      "    circular_buffer_size = -1\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph(graph_id=\"graph1\",\n",
      "                                                   graph_name=\"graph1\")\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "\n",
      "    for i in range(100):\n",
      "      op_name = \"Op%d\" % i\n",
      "      graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "          op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      trace = debug_event_pb2.GraphExecutionTrace(\n",
      "          op_name=op_name, tfdbg_context_id=\"graph1\")\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    reader = debug_events_reader.DebugDataReader(self.dump_root)\n",
      "    reader.update()\n",
      "    traces = [None] * 100\n",
      "    def read_job_1():\n",
      "      digests = reader.graph_execution_traces(digest=True)\n",
      "      for i in range(49, -1, -1):\n",
      "        traces[i] = reader.read_graph_execution_trace(digests[i])\n",
      "    def read_job_2():\n",
      "      digests = reader.graph_execution_traces(digest=True)\n",
      "      for i in range(99, 49, -1):\n",
      "        traces[i] = reader.read_graph_execution_trace(digests[i])\n",
      "    thread_1 = threading.Thread(target=read_job_1)\n",
      "    thread_2 = threading.Thread(target=read_job_2)\n",
      "    thread_1.start()\n",
      "    thread_2.start()\n",
      "    thread_1.join()\n",
      "    thread_2.join()\n",
      "    for i in range(100):\n",
      "      self.assertEqual(traces[i].op_name, \"Op%d\" % i)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      (\"Begin1End3\", 1, 3, 1, 3),\n",
      "      (\"Begin0End3\", 0, 3, 0, 3),\n",
      "      (\"Begin0EndNeg1\", 0, -1, 0, 4),\n",
      "      (\"BeginNoneEnd3\", None, 3, 0, 3),\n",
      "      (\"Begin2EndNone\", 2, None, 2, 5),\n",
      "      (\"BeginNoneEndNone\", None, None, 0, 5),\n",
      "  )\n",
      "  def testRangeReadingExecutions(self, begin, end, expected_begin,\n",
      "                                 expected_end):\n",
      "    writer = debug_events_writer.DebugEventsWriter(\n",
      "        self.dump_root, self.tfdbg_run_id, circular_buffer_size=-1)\n",
      "    for i in range(5):\n",
      "      execution = debug_event_pb2.Execution(op_type=\"OpType%d\" % i)\n",
      "      writer.WriteExecution(execution)\n",
      "    writer.FlushExecutionFiles()\n",
      "    writer.Close()\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      reader.update()\n",
      "      executions = reader.executions(begin=begin, end=end)\n",
      "    self.assertLen(executions, expected_end - expected_begin)\n",
      "    self.assertEqual(executions[0].op_type, \"OpType%d\" % expected_begin)\n",
      "    self.assertEqual(executions[-1].op_type, \"OpType%d\" % (expected_end - 1))\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      (\"Begin1End3\", 1, 3, 1, 3),\n",
      "      (\"Begin0End3\", 0, 3, 0, 3),\n",
      "      (\"Begin0EndNeg1\", 0, -1, 0, 4),\n",
      "      (\"BeginNoneEnd3\", None, 3, 0, 3),\n",
      "      (\"Begin2EndNone\", 2, None, 2, 5),\n",
      "      (\"BeginNoneEndNone\", None, None, 0, 5),\n",
      "  )\n",
      "  def testRangeReadingGraphExecutionTraces(self, begin, end, expected_begin,\n",
      "                                           expected_end):\n",
      "    writer = debug_events_writer.DebugEventsWriter(\n",
      "        self.dump_root, self.tfdbg_run_id, circular_buffer_size=-1)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph(\n",
      "        graph_id=\"graph1\", graph_name=\"graph1\")\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "    for i in range(5):\n",
      "      op_name = \"Op_%d\" % i\n",
      "      graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "          op_name=op_name, graph_id=\"graph1\")\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      trace = debug_event_pb2.GraphExecutionTrace(\n",
      "          op_name=op_name, tfdbg_context_id=\"graph1\")\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "    writer.FlushExecutionFiles()\n",
      "    writer.Close()\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      reader.update()\n",
      "      traces = reader.graph_execution_traces(begin=begin, end=end)\n",
      "    self.assertLen(traces, expected_end - expected_begin)\n",
      "    self.assertEqual(traces[0].op_name, \"Op_%d\" % expected_begin)\n",
      "    self.assertEqual(traces[-1].op_name, \"Op_%d\" % (expected_end - 1))\n",
      "\n",
      "\n",
      "class MultiSetReaderTest(dumping_callback_test_lib.DumpingCallbackTestBase):\n",
      "  \"\"\"Test for DebugDataReader for multiple file sets under a dump root.\"\"\"\n",
      "\n",
      "  def testReadingTwoFileSetsWithTheSameDumpRootSucceeds(self):\n",
      "    # To simulate a multi-host data dump, we first generate file sets in two\n",
      "    # different directories, with the same tfdbg_run_id, and then combine them.\n",
      "    tfdbg_run_id = \"foo\"\n",
      "    for i in range(2):\n",
      "      writer = debug_events_writer.DebugEventsWriter(\n",
      "          os.path.join(self.dump_root, str(i)),\n",
      "          tfdbg_run_id,\n",
      "          circular_buffer_size=-1)\n",
      "      if i == 0:\n",
      "        debugged_graph = debug_event_pb2.DebuggedGraph(\n",
      "            graph_id=\"graph1\", graph_name=\"graph1\")\n",
      "        writer.WriteDebuggedGraph(debugged_graph)\n",
      "        op_name = \"Op_0\"\n",
      "        graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "            op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "        writer.WriteGraphOpCreation(graph_op_creation)\n",
      "        op_name = \"Op_1\"\n",
      "        graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "            op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "        writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      for _ in range(10):\n",
      "        trace = debug_event_pb2.GraphExecutionTrace(\n",
      "            op_name=\"Op_%d\" % i, tfdbg_context_id=\"graph1\")\n",
      "        writer.WriteGraphExecutionTrace(trace)\n",
      "        writer.FlushNonExecutionFiles()\n",
      "        writer.FlushExecutionFiles()\n",
      "\n",
      "    # Move all files from the subdirectory /1 to subdirectory /0.\n",
      "    dump_root_0 = os.path.join(self.dump_root, \"0\")\n",
      "    src_paths = glob.glob(os.path.join(self.dump_root, \"1\", \"*\"))\n",
      "    for src_path in src_paths:\n",
      "      dst_path = os.path.join(\n",
      "          dump_root_0,\n",
      "          # Rename the file set to avoid file name collision.\n",
      "          re.sub(r\"(tfdbg_events\\.\\d+)\", r\"\\g<1>1\", os.path.basename(src_path)))\n",
      "      os.rename(src_path, dst_path)\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(dump_root_0) as reader:\n",
      "      reader.update()\n",
      "      # Verify the content of the .graph_execution_traces file.\n",
      "      trace_digests = reader.graph_execution_traces(digest=True)\n",
      "      self.assertLen(trace_digests, 20)\n",
      "      for _ in range(10):\n",
      "        trace = reader.read_graph_execution_trace(trace_digests[i])\n",
      "        self.assertEqual(trace.op_name, \"Op_0\")\n",
      "      for _ in range(10):\n",
      "        trace = reader.read_graph_execution_trace(trace_digests[i + 10])\n",
      "        self.assertEqual(trace.op_name, \"Op_1\")\n",
      "\n",
      "  def testReadingTwoFileSetsWithTheDifferentRootsLeadsToError(self):\n",
      "    # To simulate a multi-host data dump, we first generate file sets in two\n",
      "    # different directories, with different tfdbg_run_ids, and then combine\n",
      "    # them.\n",
      "    for i in range(2):\n",
      "      writer = debug_events_writer.DebugEventsWriter(\n",
      "          os.path.join(self.dump_root, str(i)),\n",
      "          \"run_id_%d\" % i,\n",
      "          circular_buffer_size=-1)\n",
      "      writer.FlushNonExecutionFiles()\n",
      "      writer.FlushExecutionFiles()\n",
      "\n",
      "    # Move all files from the subdirectory /1 to subdirectory /0.\n",
      "    dump_root_0 = os.path.join(self.dump_root, \"0\")\n",
      "    src_paths = glob.glob(os.path.join(self.dump_root, \"1\", \"*\"))\n",
      "    for src_path in src_paths:\n",
      "      dst_path = os.path.join(\n",
      "          dump_root_0,\n",
      "          # Rename the file set to avoid file name collision.\n",
      "          re.sub(r\"(tfdbg_events\\.\\d+)\", r\"\\g<1>1\", os.path.basename(src_path)))\n",
      "      os.rename(src_path, dst_path)\n",
      "\n",
      "    with self.assertRaisesRegex(ValueError,\n",
      "                                r\"Found multiple \\(2\\) tfdbg2 runs\"):\n",
      "      debug_events_reader.DebugDataReader(dump_root_0)\n",
      "\n",
      "\n",
      "class DataObjectsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n",
      "\n",
      "  def jsonRoundTripCheck(self, obj):\n",
      "    self.assertEqual(\n",
      "        json_lib.dumps(json_lib.loads(json_lib.dumps(obj)), sort_keys=True),\n",
      "        json_lib.dumps(obj, sort_keys=True))\n",
      "\n",
      "  def testExecutionDigestWithNoOutputToJson(self):\n",
      "    execution_digest = debug_events_reader.ExecutionDigest(\n",
      "        1234, 5678, \"FooOp\", output_tensor_device_ids=None)\n",
      "    json = execution_digest.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"output_tensor_device_ids\"], None)\n",
      "\n",
      "  def testExecutionDigestWithTwoOutputsToJson(self):\n",
      "    execution_digest = debug_events_reader.ExecutionDigest(\n",
      "        1234, 5678, \"FooOp\", output_tensor_device_ids=[1357, 2468])\n",
      "    json = execution_digest.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"output_tensor_device_ids\"], (1357, 2468))\n",
      "\n",
      "  def testExecutionNoGraphNoInputToJson(self):\n",
      "    execution_digest = debug_events_reader.ExecutionDigest(\n",
      "        1234, 5678, \"FooOp\", output_tensor_device_ids=[1357])\n",
      "    execution = debug_events_reader.Execution(\n",
      "        execution_digest,\n",
      "        \"localhost\",\n",
      "        (\"a1\", \"b2\"),\n",
      "        debug_event_pb2.TensorDebugMode.CURT_HEALTH,\n",
      "        graph_id=None,\n",
      "        input_tensor_ids=None,\n",
      "        output_tensor_ids=[2468],\n",
      "        debug_tensor_values=([1, 0],))\n",
      "    json = execution.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"output_tensor_device_ids\"], (1357,))\n",
      "    self.assertEqual(json[\"host_name\"], \"localhost\")\n",
      "    self.assertEqual(json[\"stack_frame_ids\"], (\"a1\", \"b2\"))\n",
      "    self.assertEqual(json[\"tensor_debug_mode\"],\n",
      "                     debug_event_pb2.TensorDebugMode.CURT_HEALTH)\n",
      "    self.assertIsNone(json[\"graph_id\"])\n",
      "    self.assertIsNone(json[\"input_tensor_ids\"])\n",
      "    self.assertEqual(json[\"output_tensor_ids\"], (2468,))\n",
      "    self.assertEqual(json[\"debug_tensor_values\"], ([1, 0],))\n",
      "\n",
      "  def testExecutionNoGraphNoInputButWithOutputToJson(self):\n",
      "    execution_digest = debug_events_reader.ExecutionDigest(\n",
      "        1234, 5678, \"FooOp\", output_tensor_device_ids=[1357])\n",
      "    execution = debug_events_reader.Execution(\n",
      "        execution_digest,\n",
      "        \"localhost\",\n",
      "        (\"a1\", \"b2\"),\n",
      "        debug_event_pb2.TensorDebugMode.FULL_HEALTH,\n",
      "        graph_id=\"abcd\",\n",
      "        input_tensor_ids=[13, 37],\n",
      "        output_tensor_ids=None,\n",
      "        debug_tensor_values=None)\n",
      "    json = execution.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"output_tensor_device_ids\"], (1357,))\n",
      "    self.assertEqual(json[\"host_name\"], \"localhost\")\n",
      "    self.assertEqual(json[\"stack_frame_ids\"], (\"a1\", \"b2\"))\n",
      "    self.assertEqual(json[\"tensor_debug_mode\"],\n",
      "                     debug_event_pb2.TensorDebugMode.FULL_HEALTH)\n",
      "    self.assertEqual(json[\"graph_id\"], \"abcd\")\n",
      "    self.assertEqual(json[\"input_tensor_ids\"], (13, 37))\n",
      "    self.assertIsNone(json[\"output_tensor_ids\"])\n",
      "    self.assertIsNone(json[\"debug_tensor_values\"])\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      (\"EmptyList\", []),\n",
      "      (\"None\", None),\n",
      "  )\n",
      "  def testExecutionWithNoOutputTensorsReturnsZeroForNumOutputs(\n",
      "      self, output_tensor_ids):\n",
      "    execution = debug_events_reader.Execution(\n",
      "        debug_events_reader.ExecutionDigest(1234, 5678, \"FooOp\"),\n",
      "        \"localhost\", (\"a1\", \"b2\"),\n",
      "        debug_event_pb2.TensorDebugMode.FULL_HEALTH,\n",
      "        graph_id=\"abcd\",\n",
      "        input_tensor_ids=[13, 37],\n",
      "        output_tensor_ids=output_tensor_ids,\n",
      "        debug_tensor_values=None)\n",
      "    self.assertEqual(execution.num_outputs, 0)\n",
      "\n",
      "  def testDebuggedDeviceToJons(self):\n",
      "    debugged_device = debug_events_reader.DebuggedDevice(\"/TPU:3\", 4)\n",
      "    self.assertEqual(debugged_device.to_json(), {\n",
      "        \"device_name\": \"/TPU:3\",\n",
      "        \"device_id\": 4,\n",
      "    })\n",
      "\n",
      "  def testDebuggedGraphToJonsWitouthNameInnerOuterGraphIds(self):\n",
      "    debugged_graph = debug_events_reader.DebuggedGraph(\n",
      "        None,\n",
      "        \"b1c2\",\n",
      "        outer_graph_id=None,\n",
      "    )\n",
      "    self.assertEqual(\n",
      "        debugged_graph.to_json(), {\n",
      "            \"name\": None,\n",
      "            \"graph_id\": \"b1c2\",\n",
      "            \"outer_graph_id\": None,\n",
      "            \"inner_graph_ids\": [],\n",
      "        })\n",
      "\n",
      "  def testDebuggedGraphToJonsWithNameAndInnerOuterGraphIds(self):\n",
      "    debugged_graph = debug_events_reader.DebuggedGraph(\n",
      "        \"loss_function\",\n",
      "        \"b1c2\",\n",
      "        outer_graph_id=\"a0b1\",\n",
      "    )\n",
      "    debugged_graph.add_inner_graph_id(\"c2d3\")\n",
      "    debugged_graph.add_inner_graph_id(\"c2d3e4\")\n",
      "    self.assertEqual(\n",
      "        debugged_graph.to_json(), {\n",
      "            \"name\": \"loss_function\",\n",
      "            \"graph_id\": \"b1c2\",\n",
      "            \"outer_graph_id\": \"a0b1\",\n",
      "            \"inner_graph_ids\": [\"c2d3\", \"c2d3e4\"],\n",
      "        })\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      (\"EmptyList\", []),\n",
      "      (\"None\", None),\n",
      "  )\n",
      "  def testGraphOpDigestWithNoOutpusReturnsNumOutputsZero(\n",
      "      self, output_tensor_ids):\n",
      "    op_creation_digest = debug_events_reader.GraphOpCreationDigest(\n",
      "        1234,\n",
      "        5678,\n",
      "        \"deadbeef\",\n",
      "        \"FooOp\",\n",
      "        \"Model_1/Foo_2\",\n",
      "        output_tensor_ids,\n",
      "        \"machine.cluster\", (\"a1\", \"a2\"),\n",
      "        input_names=None,\n",
      "        device_name=None)\n",
      "    self.assertEqual(op_creation_digest.num_outputs, 0)\n",
      "\n",
      "  def testGraphOpCreationDigestNoInputNoDeviceNameToJson(self):\n",
      "    op_creation_digest = debug_events_reader.GraphOpCreationDigest(\n",
      "        1234,\n",
      "        5678,\n",
      "        \"deadbeef\",\n",
      "        \"FooOp\",\n",
      "        \"Model_1/Foo_2\", [135],\n",
      "        \"machine.cluster\", (\"a1\", \"a2\"),\n",
      "        input_names=None,\n",
      "        device_name=None)\n",
      "    json = op_creation_digest.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_tensor_ids\"], (135,))\n",
      "    self.assertEqual(json[\"host_name\"], \"machine.cluster\")\n",
      "    self.assertEqual(json[\"stack_frame_ids\"], (\"a1\", \"a2\"))\n",
      "    self.assertIsNone(json[\"input_names\"])\n",
      "    self.assertIsNone(json[\"device_name\"])\n",
      "\n",
      "  def testGraphOpCreationDigestWithInputsAndDeviceNameToJson(self):\n",
      "    op_creation_digest = debug_events_reader.GraphOpCreationDigest(\n",
      "        1234,\n",
      "        5678,\n",
      "        \"deadbeef\",\n",
      "        \"FooOp\",\n",
      "        \"Model_1/Foo_2\", [135],\n",
      "        \"machine.cluster\", (\"a1\", \"a2\"),\n",
      "        input_names=[\"Bar_1\", \"Qux_2\"],\n",
      "        device_name=\"/device:GPU:0\")\n",
      "    json = op_creation_digest.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_tensor_ids\"], (135,))\n",
      "    self.assertEqual(json[\"host_name\"], \"machine.cluster\")\n",
      "    self.assertEqual(json[\"stack_frame_ids\"], (\"a1\", \"a2\"))\n",
      "    self.assertEqual(json[\"input_names\"], (\"Bar_1\", \"Qux_2\"))\n",
      "    self.assertEqual(json[\"device_name\"], \"/device:GPU:0\")\n",
      "\n",
      "  def testGraphExecutionTraceDigestToJson(self):\n",
      "    trace_digest = debug_events_reader.GraphExecutionTraceDigest(\n",
      "        1234, 5678, \"FooOp\", \"Model_1/Foo_2\", 1, \"deadbeef\")\n",
      "    json = trace_digest.to_json()\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_slot\"], 1)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "\n",
      "  def testGraphExecutionTraceWithTensorDebugValueAndDeviceNameToJson(self):\n",
      "    trace_digest = debug_events_reader.GraphExecutionTraceDigest(\n",
      "        1234, 5678, \"FooOp\", \"Model_1/Foo_2\", 1, \"deadbeef\")\n",
      "    trace = debug_events_reader.GraphExecutionTrace(\n",
      "        trace_digest, [\"g1\", \"g2\", \"deadbeef\"],\n",
      "        debug_event_pb2.TensorDebugMode.CURT_HEALTH,\n",
      "        debug_tensor_value=[3, 1], device_name=\"/device:GPU:0\")\n",
      "    json = trace.to_json()\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_slot\"], 1)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "    self.assertEqual(json[\"graph_ids\"], (\"g1\", \"g2\", \"deadbeef\"))\n",
      "    self.assertEqual(json[\"tensor_debug_mode\"],\n",
      "                     debug_event_pb2.TensorDebugMode.CURT_HEALTH)\n",
      "    self.assertEqual(json[\"debug_tensor_value\"], (3, 1))\n",
      "    self.assertEqual(json[\"device_name\"], \"/device:GPU:0\")\n",
      "\n",
      "  def testGraphExecutionTraceNoTensorDebugValueNoDeviceNameToJson(self):\n",
      "    trace_digest = debug_events_reader.GraphExecutionTraceDigest(\n",
      "        1234, 5678, \"FooOp\", \"Model_1/Foo_2\", 1, \"deadbeef\")\n",
      "    trace = debug_events_reader.GraphExecutionTrace(\n",
      "        trace_digest, [\"g1\", \"g2\", \"deadbeef\"],\n",
      "        debug_event_pb2.TensorDebugMode.NO_TENSOR,\n",
      "        debug_tensor_value=None, device_name=None)\n",
      "    json = trace.to_json()\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_slot\"], 1)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "    self.assertEqual(json[\"graph_ids\"], (\"g1\", \"g2\", \"deadbeef\"))\n",
      "    self.assertEqual(json[\"tensor_debug_mode\"],\n",
      "                     debug_event_pb2.TensorDebugMode.NO_TENSOR)\n",
      "    self.assertIsNone(json[\"debug_tensor_value\"])\n",
      "    self.assertIsNone(json[\"device_name\"])\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  ops.enable_eager_execution()\n",
      "  googletest.main()\n",
      "\n",
      "\n",
      "\n",
      "Example 3:\n",
      "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"This is a Python API fuzzer for tf.raw_ops.DataFormatVecPermute.\"\"\"\n",
      "import atheris\n",
      "with atheris.instrument_imports():\n",
      "  import sys\n",
      "  from python_fuzzing import FuzzingHelper\n",
      "  import tensorflow as tf\n",
      "\n",
      "\n",
      "@atheris.instrument_func\n",
      "def TestOneInput(input_bytes):\n",
      "  \"\"\"Test randomized integer fuzzing input for tf.raw_ops.DataFormatVecPermute.\"\"\"\n",
      "  fh = FuzzingHelper(input_bytes)\n",
      "\n",
      "  dtype = fh.get_tf_dtype()\n",
      "  # Max shape can be 8 in length and randomized from 0-8 without running into\n",
      "  # a OOM error.\n",
      "  shape = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\n",
      "  seed = fh.get_int()\n",
      "  try:\n",
      "    x = tf.random.uniform(shape=shape, dtype=dtype, seed=seed)\n",
      "    src_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\n",
      "    dest_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\n",
      "    _ = tf.raw_ops.DataFormatVecPermute(\n",
      "        x,\n",
      "        src_format=src_format_digits,\n",
      "        dst_format=dest_format_digits,\n",
      "        name=fh.get_string())\n",
      "  except (tf.errors.InvalidArgumentError, ValueError, TypeError):\n",
      "    pass\n",
      "\n",
      "\n",
      "def main():\n",
      "  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n",
      "  atheris.Fuzz()\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  main()\n",
      "\n",
      "\n",
      "\n",
      "Example 4:\n",
      "<reponame>EricRemmerswaal/tensorflow<gh_stars>1000+\n",
      "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"FuncGraphs for V2 control flow.\"\"\"\n",
      "\n",
      "from tensorflow.python.framework import func_graph\n",
      "from tensorflow.python.framework import ops\n",
      "\n",
      "\n",
      "class ControlFlowFuncGraph(func_graph.FuncGraph):\n",
      "  \"\"\"Contains control flow-specific FuncGraph logic.\"\"\"\n",
      "\n",
      "  def __init__(self, *args, **kwargs):\n",
      "    super(ControlFlowFuncGraph, self).__init__(*args, **kwargs)\n",
      "    outer_graph = self.outer_graph\n",
      "    # Unlike tf.function, control flow FuncGraphs are generally created one per\n",
      "    # op. This means hard-coding any outer device scopes in the body (rather\n",
      "    # than inspecting the call-time placement of the control flow op) makes\n",
      "    # sense.\n",
      "    self._device_function_stack = outer_graph._device_function_stack.copy()  # pylint: disable=protected-access\n",
      "    self.is_control_flow_graph = True\n",
      "    if ops.executing_eagerly_outside_functions():\n",
      "      func_graph.override_func_graph_name_scope(\n",
      "          self, self.outer_graph.get_name_scope())\n",
      "\n",
      "\n",
      "class CondBranchFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for branches of tf.cond().\n",
      "\n",
      "  This is used to distinguish cond branches from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "class WhileCondFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for the condition of tf.while_loop().\n",
      "\n",
      "  This is used to distinguish while conditions from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "class WhileBodyFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for the body of tf.while_loop().\n",
      "\n",
      "  This is used to distinguish while bodies from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "\n",
      "Example 5:\n",
      "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"This is a Python API fuzzer for tf.raw_ops.RaggedCountSparseOutput.\"\"\"\n",
      "import atheris\n",
      "with atheris.instrument_imports():\n",
      "  import sys\n",
      "  from python_fuzzing import FuzzingHelper\n",
      "  import tensorflow as tf\n",
      "\n",
      "\n",
      "@atheris.instrument_func\n",
      "def TestOneInput(input_bytes):\n",
      "  \"\"\"Test randomized integer/float fuzzing input for tf.raw_ops.RaggedCountSparseOutput.\"\"\"\n",
      "  fh = FuzzingHelper(input_bytes)\n",
      "\n",
      "  splits = fh.get_int_list()\n",
      "  values = fh.get_int_or_float_list()\n",
      "  weights = fh.get_int_list()\n",
      "  try:\n",
      "    _, _, _, = tf.raw_ops.RaggedCountSparseOutput(\n",
      "        splits=splits, values=values, weights=weights, binary_output=False)\n",
      "  except tf.errors.InvalidArgumentError:\n",
      "    pass\n",
      "\n",
      "\n",
      "def main():\n",
      "  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n",
      "  atheris.Fuzz()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  main()\n",
      "\n",
      "\n",
      "\n",
      "Example 6:\n",
      "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests for tensorflow.ops.nn_ops.Cross.\"\"\"\n",
      "\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.ops import array_ops\n",
      "from tensorflow.python.ops import gradient_checker\n",
      "from tensorflow.python.ops import math_ops\n",
      "from tensorflow.python.platform import test\n",
      "\n",
      "\n",
      "class CrossOpTest(test.TestCase):\n",
      "\n",
      "  @test_util.run_deprecated_v1\n",
      "  def testGradientRandomValues(self):\n",
      "    with self.cached_session():\n",
      "      us = [2, 3]\n",
      "      u = array_ops.reshape(\n",
      "          [0.854, -0.616, 0.767, 0.725, -0.927, 0.159], shape=us)\n",
      "      v = array_ops.reshape(\n",
      "          [-0.522, 0.755, 0.407, -0.652, 0.241, 0.247], shape=us)\n",
      "      s = math_ops.cross(u, v)\n",
      "      jacob_u, jacob_v = gradient_checker.compute_gradient([u, v], [us, us], s,\n",
      "                                                           us)\n",
      "\n",
      "    self.assertAllClose(jacob_u[0], jacob_u[1], rtol=1e-3, atol=1e-3)\n",
      "    self.assertAllClose(jacob_v[0], jacob_v[1], rtol=1e-3, atol=1e-3)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  test.main()\n",
      "\n",
      "\n",
      "\n",
      "Example 7:\n",
      "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests for tensorflow.kernels.logging_ops.\"\"\"\n",
      "\n",
      "\n",
      "from tensorflow.python.eager import context\n",
      "from tensorflow.python.framework import constant_op\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.ops import array_ops\n",
      "from tensorflow.python.ops import math_ops\n",
      "from tensorflow.python.ops import string_ops\n",
      "from tensorflow.python.ops import variables\n",
      "from tensorflow.python.platform import test\n",
      "from tensorflow.python.util import compat\n",
      "\n",
      "\n",
      "class StringFormatOpTest(test.TestCase):\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDim(self):\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(10)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 ... 7 8 9]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(10)\n",
      "      format_output = string_ops.string_format(\"{}\", [tensor])\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 ... 7 8 9]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneVariableScalar(self):\n",
      "    with self.cached_session():\n",
      "      var = variables.Variable(3.34)\n",
      "      format_output = string_ops.string_format(\"{}\", [var])\n",
      "      if not context.executing_eagerly():\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"3.34\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneVariableOneDim(self):\n",
      "    with self.cached_session():\n",
      "      var = variables.Variable(math_ops.range(10))\n",
      "      format_output = string_ops.string_format(\"{}\", [var])\n",
      "      if not context.executing_eagerly():\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 ... 7 8 9]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatTwoVariablesWithAssignAdd(self):\n",
      "    with self.cached_session():\n",
      "      var_one = variables.Variable(2.14)\n",
      "      plus_one = var_one.assign_add(1.0)\n",
      "      var_two = variables.Variable(math_ops.range(10))\n",
      "      format_output = string_ops.string_format(\"{}, {}\", [var_one, var_two])\n",
      "      if not context.executing_eagerly():\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "      self.evaluate(plus_one)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"3.14, [0 1 2 ... 7 8 9]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDimFloat(self):\n",
      "    with self.cached_session():\n",
      "      tensor = constant_op.constant([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 0.1 0.2 ... 0.5 0.6 0.7]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDimMatchesSummarize(self):\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=3)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 3 4 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDimVarySummarize(self):\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=-1)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 3 4 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=1)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 ... 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=2)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 ... 4 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=10)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 3 4 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDimAlmostSummarize(self):\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(5)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=3)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 3 4]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTwoDimLessThanSummarize(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(4), [2, 2])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=3)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[0 1]\\n\"\n",
      "                  \" [2 3]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTwoDim(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTwoDimSummarizeTwo(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=2)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[0 1 ... 8 9]\\n\"\n",
      "                  \" [10 11 ... 18 19]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [80 81 ... 88 89]\\n\"\n",
      "                  \" [90 91 ... 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorThreeDim(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(1000), [10, 10, 10])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \"  [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \"  [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \"  [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \"  [90 91 92 ... 97 98 99]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[100 101 102 ... 107 108 109]\\n\"\n",
      "                  \"  [110 111 112 ... 117 118 119]\\n\"\n",
      "                  \"  [120 121 122 ... 127 128 129]\\n\"\n",
      "                  \"  ...\\n  [170 171 172 ... 177 178 179]\\n\"\n",
      "                  \"  [180 181 182 ... 187 188 189]\\n\"\n",
      "                  \"  [190 191 192 ... 197 198 199]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[200 201 202 ... 207 208 209]\\n\"\n",
      "                  \"  [210 211 212 ... 217 218 219]\\n\"\n",
      "                  \"  [220 221 222 ... 227 228 229]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [270 271 272 ... 277 278 279]\\n\"\n",
      "                  \"  [280 281 282 ... 287 288 289]\\n\"\n",
      "                  \"  [290 291 292 ... 297 298 299]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[700 701 702 ... 707 708 709]\\n\"\n",
      "                  \"  [710 711 712 ... 717 718 719]\\n\"\n",
      "                  \"  [720 721 722 ... 727 728 729]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [770 771 772 ... 777 778 779]\\n\"\n",
      "                  \"  [780 781 782 ... 787 788 789]\\n\"\n",
      "                  \"  [790 791 792 ... 797 798 799]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[800 801 802 ... 807 808 809]\\n\"\n",
      "                  \"  [810 811 812 ... 817 818 819]\\n\"\n",
      "                  \"  [820 821 822 ... 827 828 829]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [870 871 872 ... 877 878 879]\\n\"\n",
      "                  \"  [880 881 882 ... 887 888 889]\\n\"\n",
      "                  \"  [890 891 892 ... 897 898 899]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[900 901 902 ... 907 908 909]\\n\"\n",
      "                  \"  [910 911 912 ... 917 918 919]\\n\"\n",
      "                  \"  [920 921 922 ... 927 928 929]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [970 971 972 ... 977 978 979]\\n\"\n",
      "                  \"  [980 981 982 ... 987 988 989]\\n\"\n",
      "                  \"  [990 991 992 ... 997 998 999]]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTemplatePrefix(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: {}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTemplatePrefixAndSuffix(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: {}, suffix\",\n",
      "                                               tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]], suffix\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTemplateSuffix(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"{}, suffix\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]], suffix\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatNoTensor(self):\n",
      "    with self.cached_session():\n",
      "      format_output = string_ops.string_format(\"No tensor.\", ())\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"No tensor.\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatMultiTensor(self):\n",
      "    with self.cached_session():\n",
      "      tensor_one = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      tensor_two = tensor_one * 10\n",
      "      format_output = string_ops.string_format(\"One: {},\\nTwo: {}\",\n",
      "                                               (tensor_one, tensor_two))\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"One: [[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]],\\n\"\n",
      "                  \"Two: [[0 10 20 ... 70 80 90]\\n\"\n",
      "                  \" [100 110 120 ... 170 180 190]\\n\"\n",
      "                  \" [200 210 220 ... 270 280 290]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [700 710 720 ... 770 780 790]\\n\"\n",
      "                  \" [800 810 820 ... 870 880 890]\\n\"\n",
      "                  \" [900 910 920 ... 970 980 990]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatSummarizeOne(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: {}\", tensor,\n",
      "                                               summarize=1)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 ... 9]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [90 ... 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatSummarizeTwo(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: {}\", tensor,\n",
      "                                               summarize=2)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 1 ... 8 9]\\n\"\n",
      "                  \" [10 11 ... 18 19]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [80 81 ... 88 89]\\n\"\n",
      "                  \" [90 91 ... 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatPlaceholder(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: %t%\", tensor,\n",
      "                                               placeholder=\"%t%\")\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testTensorCountMustMatchPlaceholderCount(self):\n",
      "    with self.cached_session():\n",
      "      with self.assertRaisesRegex(\n",
      "          ValueError, r\"The template expects 2 tensors, but the inputs only has\"\n",
      "          r\" 1\\.\\s.*\"):\n",
      "        tensor = math_ops.range(10)\n",
      "        format_output = string_ops.string_format(\"{} {}\", tensor)\n",
      "        self.evaluate(format_output)\n",
      "    with self.cached_session():\n",
      "      with self.assertRaisesRegex(\n",
      "          ValueError, r\"The template expects 2 tensors, but the inputs only has\"\n",
      "          r\" 1\\.\\s.*\"):\n",
      "        tensor = math_ops.range(10)\n",
      "        format_output = string_ops.string_format(\"{} {}\", [tensor])\n",
      "        self.evaluate(format_output)\n",
      "    with self.cached_session():\n",
      "      with self.assertRaisesRegex(\n",
      "          ValueError, r\"The template expects 1 tensors, but the inputs only has\"\n",
      "          r\" 2\\.\\s.*\"):\n",
      "        tensor = math_ops.range(10)\n",
      "        format_output = string_ops.string_format(\"{}\", (tensor, tensor))\n",
      "        self.evaluate(format_output)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testTensorAndFormatUnicode(self):\n",
      "    with self.cached_session():\n",
      "      tensor = constant_op.constant(\"😊\")\n",
      "      format_output = string_ops.string_format(\"😊:{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = '😊:\"😊\"'\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  test.main()\n",
      "\n",
      "\n",
      "\n",
      "Example 8:\n",
      "#!/usr/bin/python\n",
      "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "#\n",
      "# Test that checks if we have any issues with case insensitive filesystems.\n",
      "\n",
      "import os\n",
      "\n",
      "BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
      "ERROR_MESSAGE = \"\"\"\n",
      "Files with same name but different case detected in directory: {}\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def main():\n",
      "  # Make sure BASE_DIR ends with tensorflow.  If it doesn't, we probably\n",
      "  # computed the wrong directory.\n",
      "  if os.path.split(BASE_DIR)[-1] != 'tensorflow':\n",
      "    raise AssertionError(\n",
      "        \"BASE_DIR = '%s' doesn't end with tensorflow\" % BASE_DIR)\n",
      "\n",
      "  for dirpath, dirnames, filenames in os.walk(BASE_DIR, followlinks=True):\n",
      "    lowercase_directories = [x.lower() for x in dirnames]\n",
      "    lowercase_files = [x.lower() for x in filenames]\n",
      "\n",
      "    lowercase_dir_contents = lowercase_directories + lowercase_files\n",
      "    if len(lowercase_dir_contents) != len(set(lowercase_dir_contents)):\n",
      "      raise AssertionError(ERROR_MESSAGE.format(dirpath))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  main()\n",
      "\n",
      "\n",
      "\n",
      "Example 9:\n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests for kernelized_utils.py.\"\"\"\n",
      "\n",
      "import functools\n",
      "\n",
      "from absl.testing import parameterized\n",
      "\n",
      "from tensorflow.python.framework import constant_op\n",
      "from tensorflow.python.keras.utils import kernelized_utils\n",
      "from tensorflow.python.platform import test\n",
      "\n",
      "\n",
      "def _exact_gaussian(stddev):\n",
      "  return functools.partial(\n",
      "      kernelized_utils.exact_gaussian_kernel, stddev=stddev)\n",
      "\n",
      "\n",
      "def _exact_laplacian(stddev):\n",
      "  return functools.partial(\n",
      "      kernelized_utils.exact_laplacian_kernel, stddev=stddev)\n",
      "\n",
      "\n",
      "class KernelizedUtilsTest(test.TestCase, parameterized.TestCase):\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=10.0), [[1.0]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=50.0), [[1.0]]))\n",
      "  def test_equal_vectors(self, exact_kernel_fn, expected_values):\n",
      "    \"\"\"Identical vectors give exactly the identity kernel value.\"\"\"\n",
      "    x = constant_op.constant([0.5, -0.5, -0.5, 0.5])\n",
      "    y = constant_op.constant([0.5, -0.5, -0.5, 0.5])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    # x and y are identical and therefore K(x, y) will be precisely equal to\n",
      "    # the identity value of the kernel.\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-6)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=10.0), [[1.0]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=50.0), [[1.0]]))\n",
      "  def test_almost_identical_vectors(self, exact_kernel_fn, expected_values):\n",
      "    \"\"\"Almost identical vectors give the identity kernel value.\"\"\"\n",
      "    x = constant_op.constant([1.0, 0.4, -2.1, -1.1])\n",
      "    y = constant_op.constant([1.01, 0.39, -2.099, -1.101])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    # x and y are almost identical and therefore K(x, y) will be almost equal to\n",
      "    # the identity value of the kernel.\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-3)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=1.0), [[0.99], [0.977]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=5.0), [[0.96], [0.94]]))\n",
      "  def test_similar_matrices(self, exact_kernel_fn, expected_values):\n",
      "    \"\"\"Pairwise \"close\" vectors give high kernel values (similarity scores).\"\"\"\n",
      "    x = constant_op.constant([1.0, 3.4, -2.1, 0.9, 3.3, -2.0], shape=[2, 3])\n",
      "    y = constant_op.constant([1.1, 3.35, -2.05])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    # The 2 rows of x are close to y. The pairwise kernel values (similarity\n",
      "    # scores) are somewhat close to the identity value of the kernel.\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-2)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=2.0), [[.997, .279], [.251, 1.],\n",
      "                                                 [.164, 0.019]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=2.0), [[.904, .128], [.116, 1.],\n",
      "                                                   [.07, 0.027]]))\n",
      "  def test_matrices_varying_similarity(self, exact_kernel_fn, expected_values):\n",
      "    \"\"\"Test matrices with row vectors of varying pairwise similarity.\"\"\"\n",
      "    x = constant_op.constant([1.0, 2., -2., 0.9, 3.3, -1.0], shape=[3, 2])\n",
      "    y = constant_op.constant([1.1, 2.1, -2., 0.9], shape=[2, 2])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-2)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=1.0), [[0.0]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=1.0), [[0.0]]))\n",
      "  def test_completely_dissimilar_vectors(self, exact_kernel_fn,\n",
      "                                         expected_values):\n",
      "    \"\"\"Very dissimilar vectors give very low similarity scores.\"\"\"\n",
      "    x = constant_op.constant([1.0, 3.4, -2.1, -5.1])\n",
      "    y = constant_op.constant([0.5, 2.1, 1.0, 3.0])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    # x and y are very \"far\" from each other and so the corresponding kernel\n",
      "    # value will be very low.\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-2)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  test.main()\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 examples\n",
    "for i, example in enumerate(dataset):\n",
    "    if i < 10:\n",
    "        print(f\"Example {i}:\")\n",
    "        print(example['content'])\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "from argparse import ArgumentParser, _HelpAction\n",
      "from pkgutil import get_data\n",
      "from sys import exit\n",
      "\n",
      "# flie basename no extension\n",
      "LICENSES = [\n",
      "    \"agpl-3.0\",\n",
      "    \"apache-2.0\",\n",
      "    \"bsd-2-clause\",\n",
      "    \"bsd-3-clause\",\n",
      "    \"epl-2.0\",\n",
      "    \"gpl-2.0\",\n",
      "    \"gpl-3.0\",\n",
      "    \"lgpl-2.1\",\n",
      "    \"lgpl-3.0\",\n",
      "    \"mit\",\n",
      "    \"mpl-2.0\",\n",
      "    \"unlicenses\",\n",
      "    \"996icu-0.1\",\n",
      "]\n",
      "\n",
      "\n",
      "def getparser():\n",
      "    parser = ArgumentParser(\n",
      "        prog=\"gen-license\",\n",
      "        description=\"tools to create license file, support GitHub LICENSE code.\",\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"code\", help=\"LICENSE Code, --list to see\", choices=LICENSES,\n",
      "        nargs=\"?\", const=None\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--list\", dest=\"list\", help=\"Show supported LICENSE Codes\", required=False,\n",
      "        action=\"store_true\"\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--996icu\", dest=\"icu\", help=\"Expand LICENSE with 996ICU LICENSE, Choose a language vesion or default zh-cn\",\n",
      "        required=False, nargs=\"?\", const=\"zh-cn\", default=None,\n",
      "        choices=[\"en-us\", \"zh-cn\"]\n",
      "    )\n",
      "\n",
      "    return parser\n",
      "\n",
      "\n",
      "def select_template(language_code):\n",
      "    \"\"\"choose a 996icu LICENSE template according to *language_code*\n",
      "    \"\"\"\n",
      "    map_ = {\n",
      "        \"zh\": \"zh-cn\",\n",
      "        \"zh-cn\": \"zh-cn\",\n",
      "        \"zh-hans\": \"zh-cn\",\n",
      "        \"en\": \"en-us\",\n",
      "        \"en-us\": \"en-us\",\n",
      "    }\n",
      "\n",
      "    template = get_data(\n",
      "        __package__,\n",
      "        \"licenses/996.icu.template.{}.txt\".format(\n",
      "            map_.get(language_code, \"zh-cn\")\n",
      "        )\n",
      "    ).decode(\"utf-8\")\n",
      "\n",
      "    return template\n",
      "\n",
      "\n",
      "def main():\n",
      "    parser = getparser()\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    if args.list:\n",
      "        for license in LICENSES:\n",
      "            print(license)\n",
      "\n",
      "        exit(0)\n",
      "    else:  # main\n",
      "\n",
      "        # if no args input, show help and exit\n",
      "        if args.code is None:\n",
      "            parser.print_help()\n",
      "            parser.exit()\n",
      "\n",
      "        resource = get_data(\n",
      "            __package__,\n",
      "            \"licenses/{code}.txt\".format(code=args.code)\n",
      "        ).decode(\"utf-8\")\n",
      "\n",
      "        if args.icu is not None:  # --996icu option enabled\n",
      "            template = select_template(args.icu)\n",
      "\n",
      "            output = template.format(\n",
      "                other=args.code,\n",
      "                content=resource\n",
      "            ).encode(\"utf-8\")\n",
      "\n",
      "        else:  # common license\n",
      "            output = resource.encode(\"utf-8\")\n",
      "\n",
      "        with open(\"LICENSE\", \"wb\") as file:\n",
      "            file.write(output)\n",
      "\n",
      "        exit(0)\n",
      "\n",
      "Average Line Length: 21.82857142857143\n",
      "\n",
      "Example 1:\n",
      "<reponame>vegYY/react\n",
      "{\n",
      "  \"targets\": [\n",
      "    {\n",
      "      \"target_name\": \"perfcounters\",\n",
      "      \"sources\": [\n",
      "        \"src/hardware-counter.cpp\",\n",
      "        \"src/perf-counters.cpp\",\n",
      "        \"src/thread-local.cpp\",\n",
      "      ],\n",
      "      \"cflags\": [\n",
      "        \"-Wno-sign-compare\",\n",
      "      ],\n",
      "    },\n",
      "  ],\n",
      "}\n",
      "\n",
      "Average Line Length: 15.588235294117647\n",
      "\n",
      "Example 2:\n",
      "<reponame>EricRemmerswaal/tensorflow<filename>tensorflow/python/debug/lib/debug_events_writer_test.py\n",
      "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests for the debug events writer Python class.\"\"\"\n",
      "\n",
      "import glob\n",
      "import json as json_lib\n",
      "import os\n",
      "import re\n",
      "import threading\n",
      "import time\n",
      "\n",
      "from absl.testing import parameterized\n",
      "\n",
      "from tensorflow.core.protobuf import debug_event_pb2\n",
      "from tensorflow.python.debug.lib import debug_events_reader\n",
      "from tensorflow.python.debug.lib import debug_events_writer\n",
      "from tensorflow.python.debug.lib import dumping_callback_test_lib\n",
      "from tensorflow.python.framework import ops\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.framework import versions\n",
      "from tensorflow.python.platform import googletest\n",
      "\n",
      "\n",
      "class DebugEventsWriterTest(dumping_callback_test_lib.DumpingCallbackTestBase,\n",
      "                            parameterized.TestCase):\n",
      "\n",
      "  def testMultiThreadedConstructorCallWorks(self):\n",
      "    def init_writer():\n",
      "      debug_events_writer.DebugEventsWriter(self.dump_root, self.tfdbg_run_id)\n",
      "\n",
      "    num_threads = 4\n",
      "    threads = []\n",
      "    for _ in range(num_threads):\n",
      "      thread = threading.Thread(target=init_writer)\n",
      "      thread.start()\n",
      "      threads.append(thread)\n",
      "    for thread in threads:\n",
      "      thread.join()\n",
      "\n",
      "    # Verify that there is only one debug event file of each type.\n",
      "    metadata_paths = glob.glob(os.path.join(self.dump_root, \"*.metadata\"))\n",
      "    self.assertLen(metadata_paths, 1)\n",
      "    source_files_paths = glob.glob(\n",
      "        os.path.join(self.dump_root, \"*.source_files\"))\n",
      "    self.assertLen(source_files_paths, 1)\n",
      "    stack_frames_paths = glob.glob(\n",
      "        os.path.join(self.dump_root, \"*.stack_frames\"))\n",
      "    self.assertLen(stack_frames_paths, 1)\n",
      "    graphs_paths = glob.glob(os.path.join(self.dump_root, \"*.graphs\"))\n",
      "    self.assertLen(graphs_paths, 1)\n",
      "    self._readAndCheckMetadataFile()\n",
      "\n",
      "  def testWriteSourceFilesAndStackFrames(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    num_protos = 10\n",
      "    for i in range(num_protos):\n",
      "      source_file = debug_event_pb2.SourceFile()\n",
      "      source_file.file_path = \"/home/tf2user/main.py\"\n",
      "      source_file.host_name = \"machine.cluster\"\n",
      "      source_file.lines.append(\"print(%d)\" % i)\n",
      "      writer.WriteSourceFile(source_file)\n",
      "\n",
      "      stack_frame = debug_event_pb2.StackFrameWithId()\n",
      "      stack_frame.id = \"stack_%d\" % i\n",
      "      stack_frame.file_line_col.file_index = i * 10\n",
      "      writer.WriteStackFrameWithId(stack_frame)\n",
      "\n",
      "    writer.FlushNonExecutionFiles()\n",
      "\n",
      "    with debug_events_reader.DebugEventsReader(self.dump_root) as reader:\n",
      "      actuals = list(item.debug_event.source_file\n",
      "                     for item in reader.source_files_iterator())\n",
      "      self.assertLen(actuals, num_protos)\n",
      "      for i in range(num_protos):\n",
      "        self.assertEqual(actuals[i].file_path, \"/home/tf2user/main.py\")\n",
      "        self.assertEqual(actuals[i].host_name, \"machine.cluster\")\n",
      "        self.assertEqual(actuals[i].lines, [\"print(%d)\" % i])\n",
      "\n",
      "      actuals = list(item.debug_event.stack_frame_with_id\n",
      "                     for item in reader.stack_frames_iterator())\n",
      "      self.assertLen(actuals, num_protos)\n",
      "      for i in range(num_protos):\n",
      "        self.assertEqual(actuals[i].id, \"stack_%d\" % i)\n",
      "        self.assertEqual(actuals[i].file_line_col.file_index, i * 10)\n",
      "\n",
      "  def testWriteGraphOpCreationAndDebuggedGraphs(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    num_op_creations = 10\n",
      "    for i in range(num_op_creations):\n",
      "      graph_op_creation = debug_event_pb2.GraphOpCreation()\n",
      "      graph_op_creation.op_type = \"Conv2D\"\n",
      "      graph_op_creation.op_name = \"Conv2D_%d\" % i\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph()\n",
      "    debugged_graph.graph_id = \"deadbeaf\"\n",
      "    debugged_graph.graph_name = \"MyGraph1\"\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "\n",
      "    reader = debug_events_reader.DebugEventsReader(self.dump_root)\n",
      "    actuals = list(item.debug_event for item in reader.graphs_iterator())\n",
      "    self.assertLen(actuals, num_op_creations + 1)\n",
      "    for i in range(num_op_creations):\n",
      "      self.assertEqual(actuals[i].graph_op_creation.op_type, \"Conv2D\")\n",
      "      self.assertEqual(actuals[i].graph_op_creation.op_name, \"Conv2D_%d\" % i)\n",
      "    self.assertEqual(actuals[num_op_creations].debugged_graph.graph_id,\n",
      "                     \"deadbeaf\")\n",
      "\n",
      "  def testConcurrentWritesToNonExecutionFilesWorks(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "\n",
      "    source_file_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def writer_source_file():\n",
      "      source_file = debug_event_pb2.SourceFile()\n",
      "      with source_file_state[\"lock\"]:\n",
      "        source_file.file_path = \"/home/tf2user/file_%d.py\" % source_file_state[\n",
      "            \"counter\"]\n",
      "        source_file_state[\"counter\"] += 1\n",
      "      writer.WriteSourceFile(source_file)\n",
      "      # More-frequent-than-necessary concurrent flushing is not recommended,\n",
      "      # but tolerated.\n",
      "      writer.FlushNonExecutionFiles()\n",
      "\n",
      "    stack_frame_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def write_stack_frame():\n",
      "      stack_frame = debug_event_pb2.StackFrameWithId()\n",
      "      with stack_frame_state[\"lock\"]:\n",
      "        stack_frame.id = \"stack_frame_%d\" % stack_frame_state[\"counter\"]\n",
      "        stack_frame_state[\"counter\"] += 1\n",
      "      writer.WriteStackFrameWithId(stack_frame)\n",
      "      # More-frequent-than-necessary concurrent flushing is not recommended,\n",
      "      # but tolerated.\n",
      "      writer.FlushNonExecutionFiles()\n",
      "\n",
      "    graph_op_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def write_graph_op_creation():\n",
      "      graph_op_creation = debug_event_pb2.GraphOpCreation()\n",
      "      with graph_op_state[\"lock\"]:\n",
      "        graph_op_creation.op_name = \"Op%d\" % graph_op_state[\"counter\"]\n",
      "        graph_op_state[\"counter\"] += 1\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      # More-frequent-than-necessary concurrent flushing is not recommended,\n",
      "      # but tolerated.\n",
      "      writer.FlushNonExecutionFiles()\n",
      "\n",
      "    num_threads = 9\n",
      "    threads = []\n",
      "    for i in range(num_threads):\n",
      "      if i % 3 == 0:\n",
      "        target = writer_source_file\n",
      "      elif i % 3 == 1:\n",
      "        target = write_stack_frame\n",
      "      else:\n",
      "        target = write_graph_op_creation\n",
      "      thread = threading.Thread(target=target)\n",
      "      thread.start()\n",
      "      threads.append(thread)\n",
      "    for thread in threads:\n",
      "      thread.join()\n",
      "\n",
      "    # Verify the content of the .source_files file.\n",
      "    with debug_events_reader.DebugEventsReader(self.dump_root) as reader:\n",
      "      source_files_iter = reader.source_files_iterator()\n",
      "      actuals = list(item.debug_event.source_file for item in source_files_iter)\n",
      "      file_paths = sorted([actual.file_path for actual in actuals])\n",
      "      self.assertEqual(file_paths, [\n",
      "          \"/home/tf2user/file_0.py\", \"/home/tf2user/file_1.py\",\n",
      "          \"/home/tf2user/file_2.py\"\n",
      "      ])\n",
      "\n",
      "    # Verify the content of the .stack_frames file.\n",
      "    actuals = list(item.debug_event.stack_frame_with_id\n",
      "                   for item in reader.stack_frames_iterator())\n",
      "    stack_frame_ids = sorted([actual.id for actual in actuals])\n",
      "    self.assertEqual(stack_frame_ids,\n",
      "                     [\"stack_frame_0\", \"stack_frame_1\", \"stack_frame_2\"])\n",
      "\n",
      "    # Verify the content of the .graphs file.\n",
      "    actuals = list(item.debug_event.graph_op_creation\n",
      "                   for item in reader.graphs_iterator())\n",
      "    graph_op_names = sorted([actual.op_name for actual in actuals])\n",
      "    self.assertEqual(graph_op_names, [\"Op0\", \"Op1\", \"Op2\"])\n",
      "\n",
      "  def testWriteAndReadMetadata(self):\n",
      "    t0 = time.time()\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    writer.Close()\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      self.assertIsInstance(reader.starting_wall_time(), float)\n",
      "      self.assertGreaterEqual(reader.starting_wall_time(), t0)\n",
      "      self.assertEqual(reader.tensorflow_version(), versions.__version__)\n",
      "      self.assertTrue(reader.tfdbg_run_id())\n",
      "\n",
      "  def testWriteExecutionEventsWithCircularBuffer(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    num_execution_events = debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE * 2\n",
      "    for i in range(num_execution_events):\n",
      "      execution = debug_event_pb2.Execution()\n",
      "      execution.op_type = \"OpType%d\" % i\n",
      "      writer.WriteExecution(execution)\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      # Before FlushExecutionFiles() is called. No data should have been written\n",
      "      # to the file.\n",
      "      reader.update()\n",
      "      self.assertFalse(reader.executions())\n",
      "\n",
      "      writer.FlushExecutionFiles()\n",
      "      reader.update()\n",
      "      executions = reader.executions()\n",
      "      for i, execution in enumerate(executions):\n",
      "        self.assertEqual(\n",
      "            execution.op_type,\n",
      "            \"OpType%d\" % (i + debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE))\n",
      "\n",
      "  def testWriteExecutionEventsWithoutCircularBufferBehavior(self):\n",
      "    # A circular buffer size of 0 abolishes the circular buffer behavior.\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id, 0)\n",
      "    num_execution_events = debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE * 2\n",
      "    for i in range(num_execution_events):\n",
      "      execution = debug_event_pb2.Execution()\n",
      "      execution.op_type = \"OpType%d\" % i\n",
      "      writer.WriteExecution(execution)\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      reader.update()\n",
      "      executions = reader.executions()\n",
      "      self.assertLen(executions, num_execution_events)\n",
      "      for i, execution in enumerate(executions):\n",
      "        self.assertEqual(execution.op_type, \"OpType%d\" % i)\n",
      "\n",
      "  def testWriteGraphExecutionTraceEventsWithCircularBuffer(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "    num_execution_events = debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE * 2\n",
      "    for i in range(num_execution_events):\n",
      "      trace = debug_event_pb2.GraphExecutionTrace()\n",
      "      trace.op_name = \"Op%d\" % i\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "\n",
      "    with debug_events_reader.DebugEventsReader(self.dump_root) as reader:\n",
      "      actuals = list(reader.graph_execution_traces_iterators()[0])\n",
      "      # Before FlushExecutionFiles() is called. No data should have been written\n",
      "      # to the file.\n",
      "      self.assertEmpty(actuals)\n",
      "\n",
      "      writer.FlushExecutionFiles()\n",
      "      actuals = list(item.debug_event.graph_execution_trace\n",
      "                     for item in reader.graph_execution_traces_iterators()[0])\n",
      "      self.assertLen(actuals, debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE)\n",
      "      for i in range(debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE):\n",
      "        self.assertEqual(\n",
      "            actuals[i].op_name,\n",
      "            \"Op%d\" % (i + debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE))\n",
      "\n",
      "  def testWriteGraphExecutionTraceEventsWithoutCircularBufferBehavior(self):\n",
      "    # A circular buffer size of 0 abolishes the circular buffer behavior.\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id, 0)\n",
      "    num_execution_events = debug_events_writer.DEFAULT_CIRCULAR_BUFFER_SIZE * 2\n",
      "    for i in range(num_execution_events):\n",
      "      trace = debug_event_pb2.GraphExecutionTrace()\n",
      "      trace.op_name = \"Op%d\" % i\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    with debug_events_reader.DebugEventsReader(self.dump_root) as reader:\n",
      "      actuals = list(item.debug_event.graph_execution_trace\n",
      "                     for item in reader.graph_execution_traces_iterators()[0])\n",
      "    self.assertLen(actuals, num_execution_events)\n",
      "    for i in range(num_execution_events):\n",
      "      self.assertEqual(actuals[i].op_name, \"Op%d\" % i)\n",
      "\n",
      "  def testConcurrentWritesToExecutionFiles(self):\n",
      "    circular_buffer_size = 5\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph(graph_id=\"graph1\",\n",
      "                                                   graph_name=\"graph1\")\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "\n",
      "    execution_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def write_execution():\n",
      "      execution = debug_event_pb2.Execution()\n",
      "      with execution_state[\"lock\"]:\n",
      "        execution.op_type = \"OpType%d\" % execution_state[\"counter\"]\n",
      "        execution_state[\"counter\"] += 1\n",
      "      writer.WriteExecution(execution)\n",
      "\n",
      "    graph_execution_trace_state = {\"counter\": 0, \"lock\": threading.Lock()}\n",
      "\n",
      "    def write_graph_execution_trace():\n",
      "      with graph_execution_trace_state[\"lock\"]:\n",
      "        op_name = \"Op%d\" % graph_execution_trace_state[\"counter\"]\n",
      "        graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "            op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "        trace = debug_event_pb2.GraphExecutionTrace(\n",
      "            op_name=op_name, tfdbg_context_id=\"graph1\")\n",
      "        graph_execution_trace_state[\"counter\"] += 1\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "\n",
      "    threads = []\n",
      "    for i in range(circular_buffer_size * 4):\n",
      "      if i % 2 == 0:\n",
      "        target = write_execution\n",
      "      else:\n",
      "        target = write_graph_execution_trace\n",
      "      thread = threading.Thread(target=target)\n",
      "      thread.start()\n",
      "      threads.append(thread)\n",
      "    for thread in threads:\n",
      "      thread.join()\n",
      "    writer.FlushNonExecutionFiles()\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      reader.update()\n",
      "      # Verify the content of the .execution file.\n",
      "      executions = reader.executions()\n",
      "      executed_op_types = [execution.op_type for execution in executions]\n",
      "      self.assertLen(executed_op_types, circular_buffer_size)\n",
      "      self.assertLen(executed_op_types, len(set(executed_op_types)))\n",
      "\n",
      "      # Verify the content of the .graph_execution_traces file.\n",
      "      op_names = [trace.op_name for trace in reader.graph_execution_traces()]\n",
      "      self.assertLen(op_names, circular_buffer_size)\n",
      "      self.assertLen(op_names, len(set(op_names)))\n",
      "\n",
      "  def testConcurrentSourceFileRandomReads(self):\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id)\n",
      "\n",
      "    for i in range(100):\n",
      "      source_file = debug_event_pb2.SourceFile(\n",
      "          host_name=\"localhost\", file_path=\"/tmp/file_%d.py\" % i)\n",
      "      source_file.lines.append(\"# File %d\" % i)\n",
      "      writer.WriteSourceFile(source_file)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "\n",
      "    reader = debug_events_reader.DebugDataReader(self.dump_root)\n",
      "    reader.update()\n",
      "    lines = [None] * 100\n",
      "    def read_job_1():\n",
      "      # Read in the reverse order to enhance randomness of the read access.\n",
      "      for i in range(49, -1, -1):\n",
      "        lines[i] = reader.source_lines(\"localhost\", \"/tmp/file_%d.py\" % i)\n",
      "    def read_job_2():\n",
      "      for i in range(99, 49, -1):\n",
      "        lines[i] = reader.source_lines(\"localhost\", \"/tmp/file_%d.py\" % i)\n",
      "    thread_1 = threading.Thread(target=read_job_1)\n",
      "    thread_2 = threading.Thread(target=read_job_2)\n",
      "    thread_1.start()\n",
      "    thread_2.start()\n",
      "    thread_1.join()\n",
      "    thread_2.join()\n",
      "    for i in range(100):\n",
      "      self.assertEqual(lines[i], [\"# File %d\" % i])\n",
      "\n",
      "  def testConcurrentExecutionUpdateAndRandomRead(self):\n",
      "    circular_buffer_size = -1\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "\n",
      "    writer_state = {\"counter\": 0, \"done\": False}\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      def write_and_update_job():\n",
      "        while True:\n",
      "          if writer_state[\"done\"]:\n",
      "            break\n",
      "          execution = debug_event_pb2.Execution()\n",
      "          execution.op_type = \"OpType%d\" % writer_state[\"counter\"]\n",
      "          writer_state[\"counter\"] += 1\n",
      "          writer.WriteExecution(execution)\n",
      "          writer.FlushExecutionFiles()\n",
      "          reader.update()\n",
      "      # On the sub-thread, keep writing and reading new Execution protos.\n",
      "      write_and_update_thread = threading.Thread(target=write_and_update_job)\n",
      "      write_and_update_thread.start()\n",
      "      # On the main thread, do concurrent random read.\n",
      "      while True:\n",
      "        exec_digests = reader.executions(digest=True)\n",
      "        if exec_digests:\n",
      "          exec_0 = reader.read_execution(exec_digests[0])\n",
      "          self.assertEqual(exec_0.op_type, \"OpType0\")\n",
      "          writer_state[\"done\"] = True\n",
      "          break\n",
      "        else:\n",
      "          time.sleep(0.1)\n",
      "          continue\n",
      "      write_and_update_thread.join()\n",
      "\n",
      "  def testConcurrentExecutionRandomReads(self):\n",
      "    circular_buffer_size = -1\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "\n",
      "    for i in range(100):\n",
      "      execution = debug_event_pb2.Execution()\n",
      "      execution.op_type = \"OpType%d\" % i\n",
      "      writer.WriteExecution(execution)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    reader = debug_events_reader.DebugDataReader(self.dump_root)\n",
      "    reader.update()\n",
      "    executions = [None] * 100\n",
      "    def read_job_1():\n",
      "      execution_digests = reader.executions(digest=True)\n",
      "      # Read in the reverse order to enhance randomness of the read access.\n",
      "      for i in range(49, -1, -1):\n",
      "        execution = reader.read_execution(execution_digests[i])\n",
      "        executions[i] = execution\n",
      "    def read_job_2():\n",
      "      execution_digests = reader.executions(digest=True)\n",
      "      for i in range(99, 49, -1):\n",
      "        execution = reader.read_execution(execution_digests[i])\n",
      "        executions[i] = execution\n",
      "    thread_1 = threading.Thread(target=read_job_1)\n",
      "    thread_2 = threading.Thread(target=read_job_2)\n",
      "    thread_1.start()\n",
      "    thread_2.start()\n",
      "    thread_1.join()\n",
      "    thread_2.join()\n",
      "    for i in range(100):\n",
      "      self.assertEqual(executions[i].op_type, \"OpType%d\" % i)\n",
      "\n",
      "  def testConcurrentGraphExecutionTraceUpdateAndRandomRead(self):\n",
      "    circular_buffer_size = -1\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph(graph_id=\"graph1\",\n",
      "                                                   graph_name=\"graph1\")\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "\n",
      "    writer_state = {\"counter\": 0, \"done\": False}\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      def write_and_update_job():\n",
      "        while True:\n",
      "          if writer_state[\"done\"]:\n",
      "            break\n",
      "          op_name = \"Op%d\" % writer_state[\"counter\"]\n",
      "          graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "              op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "          writer.WriteGraphOpCreation(graph_op_creation)\n",
      "          trace = debug_event_pb2.GraphExecutionTrace(\n",
      "              op_name=op_name, tfdbg_context_id=\"graph1\")\n",
      "          writer.WriteGraphExecutionTrace(trace)\n",
      "          writer_state[\"counter\"] += 1\n",
      "          writer.FlushNonExecutionFiles()\n",
      "          writer.FlushExecutionFiles()\n",
      "          reader.update()\n",
      "      # On the sub-thread, keep writing and reading new GraphExecutionTraces.\n",
      "      write_and_update_thread = threading.Thread(target=write_and_update_job)\n",
      "      write_and_update_thread.start()\n",
      "      # On the main thread, do concurrent random read.\n",
      "      while True:\n",
      "        digests = reader.graph_execution_traces(digest=True)\n",
      "        if digests:\n",
      "          trace_0 = reader.read_graph_execution_trace(digests[0])\n",
      "          self.assertEqual(trace_0.op_name, \"Op0\")\n",
      "          writer_state[\"done\"] = True\n",
      "          break\n",
      "        else:\n",
      "          time.sleep(0.1)\n",
      "          continue\n",
      "      write_and_update_thread.join()\n",
      "\n",
      "  def testConcurrentGraphExecutionTraceRandomReads(self):\n",
      "    circular_buffer_size = -1\n",
      "    writer = debug_events_writer.DebugEventsWriter(self.dump_root,\n",
      "                                                   self.tfdbg_run_id,\n",
      "                                                   circular_buffer_size)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph(graph_id=\"graph1\",\n",
      "                                                   graph_name=\"graph1\")\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "\n",
      "    for i in range(100):\n",
      "      op_name = \"Op%d\" % i\n",
      "      graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "          op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      trace = debug_event_pb2.GraphExecutionTrace(\n",
      "          op_name=op_name, tfdbg_context_id=\"graph1\")\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "    writer.FlushExecutionFiles()\n",
      "\n",
      "    reader = debug_events_reader.DebugDataReader(self.dump_root)\n",
      "    reader.update()\n",
      "    traces = [None] * 100\n",
      "    def read_job_1():\n",
      "      digests = reader.graph_execution_traces(digest=True)\n",
      "      for i in range(49, -1, -1):\n",
      "        traces[i] = reader.read_graph_execution_trace(digests[i])\n",
      "    def read_job_2():\n",
      "      digests = reader.graph_execution_traces(digest=True)\n",
      "      for i in range(99, 49, -1):\n",
      "        traces[i] = reader.read_graph_execution_trace(digests[i])\n",
      "    thread_1 = threading.Thread(target=read_job_1)\n",
      "    thread_2 = threading.Thread(target=read_job_2)\n",
      "    thread_1.start()\n",
      "    thread_2.start()\n",
      "    thread_1.join()\n",
      "    thread_2.join()\n",
      "    for i in range(100):\n",
      "      self.assertEqual(traces[i].op_name, \"Op%d\" % i)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      (\"Begin1End3\", 1, 3, 1, 3),\n",
      "      (\"Begin0End3\", 0, 3, 0, 3),\n",
      "      (\"Begin0EndNeg1\", 0, -1, 0, 4),\n",
      "      (\"BeginNoneEnd3\", None, 3, 0, 3),\n",
      "      (\"Begin2EndNone\", 2, None, 2, 5),\n",
      "      (\"BeginNoneEndNone\", None, None, 0, 5),\n",
      "  )\n",
      "  def testRangeReadingExecutions(self, begin, end, expected_begin,\n",
      "                                 expected_end):\n",
      "    writer = debug_events_writer.DebugEventsWriter(\n",
      "        self.dump_root, self.tfdbg_run_id, circular_buffer_size=-1)\n",
      "    for i in range(5):\n",
      "      execution = debug_event_pb2.Execution(op_type=\"OpType%d\" % i)\n",
      "      writer.WriteExecution(execution)\n",
      "    writer.FlushExecutionFiles()\n",
      "    writer.Close()\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      reader.update()\n",
      "      executions = reader.executions(begin=begin, end=end)\n",
      "    self.assertLen(executions, expected_end - expected_begin)\n",
      "    self.assertEqual(executions[0].op_type, \"OpType%d\" % expected_begin)\n",
      "    self.assertEqual(executions[-1].op_type, \"OpType%d\" % (expected_end - 1))\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      (\"Begin1End3\", 1, 3, 1, 3),\n",
      "      (\"Begin0End3\", 0, 3, 0, 3),\n",
      "      (\"Begin0EndNeg1\", 0, -1, 0, 4),\n",
      "      (\"BeginNoneEnd3\", None, 3, 0, 3),\n",
      "      (\"Begin2EndNone\", 2, None, 2, 5),\n",
      "      (\"BeginNoneEndNone\", None, None, 0, 5),\n",
      "  )\n",
      "  def testRangeReadingGraphExecutionTraces(self, begin, end, expected_begin,\n",
      "                                           expected_end):\n",
      "    writer = debug_events_writer.DebugEventsWriter(\n",
      "        self.dump_root, self.tfdbg_run_id, circular_buffer_size=-1)\n",
      "    debugged_graph = debug_event_pb2.DebuggedGraph(\n",
      "        graph_id=\"graph1\", graph_name=\"graph1\")\n",
      "    writer.WriteDebuggedGraph(debugged_graph)\n",
      "    for i in range(5):\n",
      "      op_name = \"Op_%d\" % i\n",
      "      graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "          op_name=op_name, graph_id=\"graph1\")\n",
      "      writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      trace = debug_event_pb2.GraphExecutionTrace(\n",
      "          op_name=op_name, tfdbg_context_id=\"graph1\")\n",
      "      writer.WriteGraphExecutionTrace(trace)\n",
      "    writer.FlushNonExecutionFiles()\n",
      "    writer.FlushExecutionFiles()\n",
      "    writer.Close()\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(self.dump_root) as reader:\n",
      "      reader.update()\n",
      "      traces = reader.graph_execution_traces(begin=begin, end=end)\n",
      "    self.assertLen(traces, expected_end - expected_begin)\n",
      "    self.assertEqual(traces[0].op_name, \"Op_%d\" % expected_begin)\n",
      "    self.assertEqual(traces[-1].op_name, \"Op_%d\" % (expected_end - 1))\n",
      "\n",
      "\n",
      "class MultiSetReaderTest(dumping_callback_test_lib.DumpingCallbackTestBase):\n",
      "  \"\"\"Test for DebugDataReader for multiple file sets under a dump root.\"\"\"\n",
      "\n",
      "  def testReadingTwoFileSetsWithTheSameDumpRootSucceeds(self):\n",
      "    # To simulate a multi-host data dump, we first generate file sets in two\n",
      "    # different directories, with the same tfdbg_run_id, and then combine them.\n",
      "    tfdbg_run_id = \"foo\"\n",
      "    for i in range(2):\n",
      "      writer = debug_events_writer.DebugEventsWriter(\n",
      "          os.path.join(self.dump_root, str(i)),\n",
      "          tfdbg_run_id,\n",
      "          circular_buffer_size=-1)\n",
      "      if i == 0:\n",
      "        debugged_graph = debug_event_pb2.DebuggedGraph(\n",
      "            graph_id=\"graph1\", graph_name=\"graph1\")\n",
      "        writer.WriteDebuggedGraph(debugged_graph)\n",
      "        op_name = \"Op_0\"\n",
      "        graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "            op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "        writer.WriteGraphOpCreation(graph_op_creation)\n",
      "        op_name = \"Op_1\"\n",
      "        graph_op_creation = debug_event_pb2.GraphOpCreation(\n",
      "            op_type=\"FooOp\", op_name=op_name, graph_id=\"graph1\")\n",
      "        writer.WriteGraphOpCreation(graph_op_creation)\n",
      "      for _ in range(10):\n",
      "        trace = debug_event_pb2.GraphExecutionTrace(\n",
      "            op_name=\"Op_%d\" % i, tfdbg_context_id=\"graph1\")\n",
      "        writer.WriteGraphExecutionTrace(trace)\n",
      "        writer.FlushNonExecutionFiles()\n",
      "        writer.FlushExecutionFiles()\n",
      "\n",
      "    # Move all files from the subdirectory /1 to subdirectory /0.\n",
      "    dump_root_0 = os.path.join(self.dump_root, \"0\")\n",
      "    src_paths = glob.glob(os.path.join(self.dump_root, \"1\", \"*\"))\n",
      "    for src_path in src_paths:\n",
      "      dst_path = os.path.join(\n",
      "          dump_root_0,\n",
      "          # Rename the file set to avoid file name collision.\n",
      "          re.sub(r\"(tfdbg_events\\.\\d+)\", r\"\\g<1>1\", os.path.basename(src_path)))\n",
      "      os.rename(src_path, dst_path)\n",
      "\n",
      "    with debug_events_reader.DebugDataReader(dump_root_0) as reader:\n",
      "      reader.update()\n",
      "      # Verify the content of the .graph_execution_traces file.\n",
      "      trace_digests = reader.graph_execution_traces(digest=True)\n",
      "      self.assertLen(trace_digests, 20)\n",
      "      for _ in range(10):\n",
      "        trace = reader.read_graph_execution_trace(trace_digests[i])\n",
      "        self.assertEqual(trace.op_name, \"Op_0\")\n",
      "      for _ in range(10):\n",
      "        trace = reader.read_graph_execution_trace(trace_digests[i + 10])\n",
      "        self.assertEqual(trace.op_name, \"Op_1\")\n",
      "\n",
      "  def testReadingTwoFileSetsWithTheDifferentRootsLeadsToError(self):\n",
      "    # To simulate a multi-host data dump, we first generate file sets in two\n",
      "    # different directories, with different tfdbg_run_ids, and then combine\n",
      "    # them.\n",
      "    for i in range(2):\n",
      "      writer = debug_events_writer.DebugEventsWriter(\n",
      "          os.path.join(self.dump_root, str(i)),\n",
      "          \"run_id_%d\" % i,\n",
      "          circular_buffer_size=-1)\n",
      "      writer.FlushNonExecutionFiles()\n",
      "      writer.FlushExecutionFiles()\n",
      "\n",
      "    # Move all files from the subdirectory /1 to subdirectory /0.\n",
      "    dump_root_0 = os.path.join(self.dump_root, \"0\")\n",
      "    src_paths = glob.glob(os.path.join(self.dump_root, \"1\", \"*\"))\n",
      "    for src_path in src_paths:\n",
      "      dst_path = os.path.join(\n",
      "          dump_root_0,\n",
      "          # Rename the file set to avoid file name collision.\n",
      "          re.sub(r\"(tfdbg_events\\.\\d+)\", r\"\\g<1>1\", os.path.basename(src_path)))\n",
      "      os.rename(src_path, dst_path)\n",
      "\n",
      "    with self.assertRaisesRegex(ValueError,\n",
      "                                r\"Found multiple \\(2\\) tfdbg2 runs\"):\n",
      "      debug_events_reader.DebugDataReader(dump_root_0)\n",
      "\n",
      "\n",
      "class DataObjectsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n",
      "\n",
      "  def jsonRoundTripCheck(self, obj):\n",
      "    self.assertEqual(\n",
      "        json_lib.dumps(json_lib.loads(json_lib.dumps(obj)), sort_keys=True),\n",
      "        json_lib.dumps(obj, sort_keys=True))\n",
      "\n",
      "  def testExecutionDigestWithNoOutputToJson(self):\n",
      "    execution_digest = debug_events_reader.ExecutionDigest(\n",
      "        1234, 5678, \"FooOp\", output_tensor_device_ids=None)\n",
      "    json = execution_digest.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"output_tensor_device_ids\"], None)\n",
      "\n",
      "  def testExecutionDigestWithTwoOutputsToJson(self):\n",
      "    execution_digest = debug_events_reader.ExecutionDigest(\n",
      "        1234, 5678, \"FooOp\", output_tensor_device_ids=[1357, 2468])\n",
      "    json = execution_digest.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"output_tensor_device_ids\"], (1357, 2468))\n",
      "\n",
      "  def testExecutionNoGraphNoInputToJson(self):\n",
      "    execution_digest = debug_events_reader.ExecutionDigest(\n",
      "        1234, 5678, \"FooOp\", output_tensor_device_ids=[1357])\n",
      "    execution = debug_events_reader.Execution(\n",
      "        execution_digest,\n",
      "        \"localhost\",\n",
      "        (\"a1\", \"b2\"),\n",
      "        debug_event_pb2.TensorDebugMode.CURT_HEALTH,\n",
      "        graph_id=None,\n",
      "        input_tensor_ids=None,\n",
      "        output_tensor_ids=[2468],\n",
      "        debug_tensor_values=([1, 0],))\n",
      "    json = execution.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"output_tensor_device_ids\"], (1357,))\n",
      "    self.assertEqual(json[\"host_name\"], \"localhost\")\n",
      "    self.assertEqual(json[\"stack_frame_ids\"], (\"a1\", \"b2\"))\n",
      "    self.assertEqual(json[\"tensor_debug_mode\"],\n",
      "                     debug_event_pb2.TensorDebugMode.CURT_HEALTH)\n",
      "    self.assertIsNone(json[\"graph_id\"])\n",
      "    self.assertIsNone(json[\"input_tensor_ids\"])\n",
      "    self.assertEqual(json[\"output_tensor_ids\"], (2468,))\n",
      "    self.assertEqual(json[\"debug_tensor_values\"], ([1, 0],))\n",
      "\n",
      "  def testExecutionNoGraphNoInputButWithOutputToJson(self):\n",
      "    execution_digest = debug_events_reader.ExecutionDigest(\n",
      "        1234, 5678, \"FooOp\", output_tensor_device_ids=[1357])\n",
      "    execution = debug_events_reader.Execution(\n",
      "        execution_digest,\n",
      "        \"localhost\",\n",
      "        (\"a1\", \"b2\"),\n",
      "        debug_event_pb2.TensorDebugMode.FULL_HEALTH,\n",
      "        graph_id=\"abcd\",\n",
      "        input_tensor_ids=[13, 37],\n",
      "        output_tensor_ids=None,\n",
      "        debug_tensor_values=None)\n",
      "    json = execution.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"output_tensor_device_ids\"], (1357,))\n",
      "    self.assertEqual(json[\"host_name\"], \"localhost\")\n",
      "    self.assertEqual(json[\"stack_frame_ids\"], (\"a1\", \"b2\"))\n",
      "    self.assertEqual(json[\"tensor_debug_mode\"],\n",
      "                     debug_event_pb2.TensorDebugMode.FULL_HEALTH)\n",
      "    self.assertEqual(json[\"graph_id\"], \"abcd\")\n",
      "    self.assertEqual(json[\"input_tensor_ids\"], (13, 37))\n",
      "    self.assertIsNone(json[\"output_tensor_ids\"])\n",
      "    self.assertIsNone(json[\"debug_tensor_values\"])\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      (\"EmptyList\", []),\n",
      "      (\"None\", None),\n",
      "  )\n",
      "  def testExecutionWithNoOutputTensorsReturnsZeroForNumOutputs(\n",
      "      self, output_tensor_ids):\n",
      "    execution = debug_events_reader.Execution(\n",
      "        debug_events_reader.ExecutionDigest(1234, 5678, \"FooOp\"),\n",
      "        \"localhost\", (\"a1\", \"b2\"),\n",
      "        debug_event_pb2.TensorDebugMode.FULL_HEALTH,\n",
      "        graph_id=\"abcd\",\n",
      "        input_tensor_ids=[13, 37],\n",
      "        output_tensor_ids=output_tensor_ids,\n",
      "        debug_tensor_values=None)\n",
      "    self.assertEqual(execution.num_outputs, 0)\n",
      "\n",
      "  def testDebuggedDeviceToJons(self):\n",
      "    debugged_device = debug_events_reader.DebuggedDevice(\"/TPU:3\", 4)\n",
      "    self.assertEqual(debugged_device.to_json(), {\n",
      "        \"device_name\": \"/TPU:3\",\n",
      "        \"device_id\": 4,\n",
      "    })\n",
      "\n",
      "  def testDebuggedGraphToJonsWitouthNameInnerOuterGraphIds(self):\n",
      "    debugged_graph = debug_events_reader.DebuggedGraph(\n",
      "        None,\n",
      "        \"b1c2\",\n",
      "        outer_graph_id=None,\n",
      "    )\n",
      "    self.assertEqual(\n",
      "        debugged_graph.to_json(), {\n",
      "            \"name\": None,\n",
      "            \"graph_id\": \"b1c2\",\n",
      "            \"outer_graph_id\": None,\n",
      "            \"inner_graph_ids\": [],\n",
      "        })\n",
      "\n",
      "  def testDebuggedGraphToJonsWithNameAndInnerOuterGraphIds(self):\n",
      "    debugged_graph = debug_events_reader.DebuggedGraph(\n",
      "        \"loss_function\",\n",
      "        \"b1c2\",\n",
      "        outer_graph_id=\"a0b1\",\n",
      "    )\n",
      "    debugged_graph.add_inner_graph_id(\"c2d3\")\n",
      "    debugged_graph.add_inner_graph_id(\"c2d3e4\")\n",
      "    self.assertEqual(\n",
      "        debugged_graph.to_json(), {\n",
      "            \"name\": \"loss_function\",\n",
      "            \"graph_id\": \"b1c2\",\n",
      "            \"outer_graph_id\": \"a0b1\",\n",
      "            \"inner_graph_ids\": [\"c2d3\", \"c2d3e4\"],\n",
      "        })\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      (\"EmptyList\", []),\n",
      "      (\"None\", None),\n",
      "  )\n",
      "  def testGraphOpDigestWithNoOutpusReturnsNumOutputsZero(\n",
      "      self, output_tensor_ids):\n",
      "    op_creation_digest = debug_events_reader.GraphOpCreationDigest(\n",
      "        1234,\n",
      "        5678,\n",
      "        \"deadbeef\",\n",
      "        \"FooOp\",\n",
      "        \"Model_1/Foo_2\",\n",
      "        output_tensor_ids,\n",
      "        \"machine.cluster\", (\"a1\", \"a2\"),\n",
      "        input_names=None,\n",
      "        device_name=None)\n",
      "    self.assertEqual(op_creation_digest.num_outputs, 0)\n",
      "\n",
      "  def testGraphOpCreationDigestNoInputNoDeviceNameToJson(self):\n",
      "    op_creation_digest = debug_events_reader.GraphOpCreationDigest(\n",
      "        1234,\n",
      "        5678,\n",
      "        \"deadbeef\",\n",
      "        \"FooOp\",\n",
      "        \"Model_1/Foo_2\", [135],\n",
      "        \"machine.cluster\", (\"a1\", \"a2\"),\n",
      "        input_names=None,\n",
      "        device_name=None)\n",
      "    json = op_creation_digest.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_tensor_ids\"], (135,))\n",
      "    self.assertEqual(json[\"host_name\"], \"machine.cluster\")\n",
      "    self.assertEqual(json[\"stack_frame_ids\"], (\"a1\", \"a2\"))\n",
      "    self.assertIsNone(json[\"input_names\"])\n",
      "    self.assertIsNone(json[\"device_name\"])\n",
      "\n",
      "  def testGraphOpCreationDigestWithInputsAndDeviceNameToJson(self):\n",
      "    op_creation_digest = debug_events_reader.GraphOpCreationDigest(\n",
      "        1234,\n",
      "        5678,\n",
      "        \"deadbeef\",\n",
      "        \"FooOp\",\n",
      "        \"Model_1/Foo_2\", [135],\n",
      "        \"machine.cluster\", (\"a1\", \"a2\"),\n",
      "        input_names=[\"Bar_1\", \"Qux_2\"],\n",
      "        device_name=\"/device:GPU:0\")\n",
      "    json = op_creation_digest.to_json()\n",
      "    self.jsonRoundTripCheck(json)\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_tensor_ids\"], (135,))\n",
      "    self.assertEqual(json[\"host_name\"], \"machine.cluster\")\n",
      "    self.assertEqual(json[\"stack_frame_ids\"], (\"a1\", \"a2\"))\n",
      "    self.assertEqual(json[\"input_names\"], (\"Bar_1\", \"Qux_2\"))\n",
      "    self.assertEqual(json[\"device_name\"], \"/device:GPU:0\")\n",
      "\n",
      "  def testGraphExecutionTraceDigestToJson(self):\n",
      "    trace_digest = debug_events_reader.GraphExecutionTraceDigest(\n",
      "        1234, 5678, \"FooOp\", \"Model_1/Foo_2\", 1, \"deadbeef\")\n",
      "    json = trace_digest.to_json()\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_slot\"], 1)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "\n",
      "  def testGraphExecutionTraceWithTensorDebugValueAndDeviceNameToJson(self):\n",
      "    trace_digest = debug_events_reader.GraphExecutionTraceDigest(\n",
      "        1234, 5678, \"FooOp\", \"Model_1/Foo_2\", 1, \"deadbeef\")\n",
      "    trace = debug_events_reader.GraphExecutionTrace(\n",
      "        trace_digest, [\"g1\", \"g2\", \"deadbeef\"],\n",
      "        debug_event_pb2.TensorDebugMode.CURT_HEALTH,\n",
      "        debug_tensor_value=[3, 1], device_name=\"/device:GPU:0\")\n",
      "    json = trace.to_json()\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_slot\"], 1)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "    self.assertEqual(json[\"graph_ids\"], (\"g1\", \"g2\", \"deadbeef\"))\n",
      "    self.assertEqual(json[\"tensor_debug_mode\"],\n",
      "                     debug_event_pb2.TensorDebugMode.CURT_HEALTH)\n",
      "    self.assertEqual(json[\"debug_tensor_value\"], (3, 1))\n",
      "    self.assertEqual(json[\"device_name\"], \"/device:GPU:0\")\n",
      "\n",
      "  def testGraphExecutionTraceNoTensorDebugValueNoDeviceNameToJson(self):\n",
      "    trace_digest = debug_events_reader.GraphExecutionTraceDigest(\n",
      "        1234, 5678, \"FooOp\", \"Model_1/Foo_2\", 1, \"deadbeef\")\n",
      "    trace = debug_events_reader.GraphExecutionTrace(\n",
      "        trace_digest, [\"g1\", \"g2\", \"deadbeef\"],\n",
      "        debug_event_pb2.TensorDebugMode.NO_TENSOR,\n",
      "        debug_tensor_value=None, device_name=None)\n",
      "    json = trace.to_json()\n",
      "    self.assertEqual(json[\"wall_time\"], 1234)\n",
      "    self.assertEqual(json[\"op_type\"], \"FooOp\")\n",
      "    self.assertEqual(json[\"op_name\"], \"Model_1/Foo_2\")\n",
      "    self.assertEqual(json[\"output_slot\"], 1)\n",
      "    self.assertEqual(json[\"graph_id\"], \"deadbeef\")\n",
      "    self.assertEqual(json[\"graph_ids\"], (\"g1\", \"g2\", \"deadbeef\"))\n",
      "    self.assertEqual(json[\"tensor_debug_mode\"],\n",
      "                     debug_event_pb2.TensorDebugMode.NO_TENSOR)\n",
      "    self.assertIsNone(json[\"debug_tensor_value\"])\n",
      "    self.assertIsNone(json[\"device_name\"])\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  ops.enable_eager_execution()\n",
      "  googletest.main()\n",
      "\n",
      "Average Line Length: 40.89815817984832\n",
      "\n",
      "Example 3:\n",
      "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"This is a Python API fuzzer for tf.raw_ops.DataFormatVecPermute.\"\"\"\n",
      "import atheris\n",
      "with atheris.instrument_imports():\n",
      "  import sys\n",
      "  from python_fuzzing import FuzzingHelper\n",
      "  import tensorflow as tf\n",
      "\n",
      "\n",
      "@atheris.instrument_func\n",
      "def TestOneInput(input_bytes):\n",
      "  \"\"\"Test randomized integer fuzzing input for tf.raw_ops.DataFormatVecPermute.\"\"\"\n",
      "  fh = FuzzingHelper(input_bytes)\n",
      "\n",
      "  dtype = fh.get_tf_dtype()\n",
      "  # Max shape can be 8 in length and randomized from 0-8 without running into\n",
      "  # a OOM error.\n",
      "  shape = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\n",
      "  seed = fh.get_int()\n",
      "  try:\n",
      "    x = tf.random.uniform(shape=shape, dtype=dtype, seed=seed)\n",
      "    src_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\n",
      "    dest_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\n",
      "    _ = tf.raw_ops.DataFormatVecPermute(\n",
      "        x,\n",
      "        src_format=src_format_digits,\n",
      "        dst_format=dest_format_digits,\n",
      "        name=fh.get_string())\n",
      "  except (tf.errors.InvalidArgumentError, ValueError, TypeError):\n",
      "    pass\n",
      "\n",
      "\n",
      "def main():\n",
      "  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n",
      "  atheris.Fuzz()\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  main()\n",
      "\n",
      "Average Line Length: 34.339622641509436\n",
      "\n",
      "Example 4:\n",
      "<reponame>EricRemmerswaal/tensorflow<gh_stars>1000+\n",
      "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"FuncGraphs for V2 control flow.\"\"\"\n",
      "\n",
      "from tensorflow.python.framework import func_graph\n",
      "from tensorflow.python.framework import ops\n",
      "\n",
      "\n",
      "class ControlFlowFuncGraph(func_graph.FuncGraph):\n",
      "  \"\"\"Contains control flow-specific FuncGraph logic.\"\"\"\n",
      "\n",
      "  def __init__(self, *args, **kwargs):\n",
      "    super(ControlFlowFuncGraph, self).__init__(*args, **kwargs)\n",
      "    outer_graph = self.outer_graph\n",
      "    # Unlike tf.function, control flow FuncGraphs are generally created one per\n",
      "    # op. This means hard-coding any outer device scopes in the body (rather\n",
      "    # than inspecting the call-time placement of the control flow op) makes\n",
      "    # sense.\n",
      "    self._device_function_stack = outer_graph._device_function_stack.copy()  # pylint: disable=protected-access\n",
      "    self.is_control_flow_graph = True\n",
      "    if ops.executing_eagerly_outside_functions():\n",
      "      func_graph.override_func_graph_name_scope(\n",
      "          self, self.outer_graph.get_name_scope())\n",
      "\n",
      "\n",
      "class CondBranchFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for branches of tf.cond().\n",
      "\n",
      "  This is used to distinguish cond branches from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "class WhileCondFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for the condition of tf.while_loop().\n",
      "\n",
      "  This is used to distinguish while conditions from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "class WhileBodyFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for the body of tf.while_loop().\n",
      "\n",
      "  This is used to distinguish while bodies from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "Average Line Length: 36.6551724137931\n",
      "\n",
      "Example 5:\n",
      "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"This is a Python API fuzzer for tf.raw_ops.RaggedCountSparseOutput.\"\"\"\n",
      "import atheris\n",
      "with atheris.instrument_imports():\n",
      "  import sys\n",
      "  from python_fuzzing import FuzzingHelper\n",
      "  import tensorflow as tf\n",
      "\n",
      "\n",
      "@atheris.instrument_func\n",
      "def TestOneInput(input_bytes):\n",
      "  \"\"\"Test randomized integer/float fuzzing input for tf.raw_ops.RaggedCountSparseOutput.\"\"\"\n",
      "  fh = FuzzingHelper(input_bytes)\n",
      "\n",
      "  splits = fh.get_int_list()\n",
      "  values = fh.get_int_or_float_list()\n",
      "  weights = fh.get_int_list()\n",
      "  try:\n",
      "    _, _, _, = tf.raw_ops.RaggedCountSparseOutput(\n",
      "        splits=splits, values=values, weights=weights, binary_output=False)\n",
      "  except tf.errors.InvalidArgumentError:\n",
      "    pass\n",
      "\n",
      "\n",
      "def main():\n",
      "  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n",
      "  atheris.Fuzz()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  main()\n",
      "\n",
      "Average Line Length: 32.333333333333336\n",
      "\n",
      "Example 6:\n",
      "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests for tensorflow.ops.nn_ops.Cross.\"\"\"\n",
      "\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.ops import array_ops\n",
      "from tensorflow.python.ops import gradient_checker\n",
      "from tensorflow.python.ops import math_ops\n",
      "from tensorflow.python.platform import test\n",
      "\n",
      "\n",
      "class CrossOpTest(test.TestCase):\n",
      "\n",
      "  @test_util.run_deprecated_v1\n",
      "  def testGradientRandomValues(self):\n",
      "    with self.cached_session():\n",
      "      us = [2, 3]\n",
      "      u = array_ops.reshape(\n",
      "          [0.854, -0.616, 0.767, 0.725, -0.927, 0.159], shape=us)\n",
      "      v = array_ops.reshape(\n",
      "          [-0.522, 0.755, 0.407, -0.652, 0.241, 0.247], shape=us)\n",
      "      s = math_ops.cross(u, v)\n",
      "      jacob_u, jacob_v = gradient_checker.compute_gradient([u, v], [us, us], s,\n",
      "                                                           us)\n",
      "\n",
      "    self.assertAllClose(jacob_u[0], jacob_u[1], rtol=1e-3, atol=1e-3)\n",
      "    self.assertAllClose(jacob_v[0], jacob_v[1], rtol=1e-3, atol=1e-3)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  test.main()\n",
      "\n",
      "Average Line Length: 37.0\n",
      "\n",
      "Example 7:\n",
      "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests for tensorflow.kernels.logging_ops.\"\"\"\n",
      "\n",
      "\n",
      "from tensorflow.python.eager import context\n",
      "from tensorflow.python.framework import constant_op\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.ops import array_ops\n",
      "from tensorflow.python.ops import math_ops\n",
      "from tensorflow.python.ops import string_ops\n",
      "from tensorflow.python.ops import variables\n",
      "from tensorflow.python.platform import test\n",
      "from tensorflow.python.util import compat\n",
      "\n",
      "\n",
      "class StringFormatOpTest(test.TestCase):\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDim(self):\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(10)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 ... 7 8 9]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(10)\n",
      "      format_output = string_ops.string_format(\"{}\", [tensor])\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 ... 7 8 9]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneVariableScalar(self):\n",
      "    with self.cached_session():\n",
      "      var = variables.Variable(3.34)\n",
      "      format_output = string_ops.string_format(\"{}\", [var])\n",
      "      if not context.executing_eagerly():\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"3.34\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneVariableOneDim(self):\n",
      "    with self.cached_session():\n",
      "      var = variables.Variable(math_ops.range(10))\n",
      "      format_output = string_ops.string_format(\"{}\", [var])\n",
      "      if not context.executing_eagerly():\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 ... 7 8 9]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatTwoVariablesWithAssignAdd(self):\n",
      "    with self.cached_session():\n",
      "      var_one = variables.Variable(2.14)\n",
      "      plus_one = var_one.assign_add(1.0)\n",
      "      var_two = variables.Variable(math_ops.range(10))\n",
      "      format_output = string_ops.string_format(\"{}, {}\", [var_one, var_two])\n",
      "      if not context.executing_eagerly():\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "      self.evaluate(plus_one)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"3.14, [0 1 2 ... 7 8 9]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDimFloat(self):\n",
      "    with self.cached_session():\n",
      "      tensor = constant_op.constant([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 0.1 0.2 ... 0.5 0.6 0.7]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDimMatchesSummarize(self):\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=3)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 3 4 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDimVarySummarize(self):\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=-1)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 3 4 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=1)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 ... 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=2)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 ... 4 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(6)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=10)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 3 4 5]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorOneDimAlmostSummarize(self):\n",
      "    with self.cached_session():\n",
      "      tensor = math_ops.range(5)\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=3)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"[0 1 2 3 4]\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTwoDimLessThanSummarize(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(4), [2, 2])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=3)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[0 1]\\n\"\n",
      "                  \" [2 3]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTwoDim(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTwoDimSummarizeTwo(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor, summarize=2)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[0 1 ... 8 9]\\n\"\n",
      "                  \" [10 11 ... 18 19]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [80 81 ... 88 89]\\n\"\n",
      "                  \" [90 91 ... 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorThreeDim(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(1000), [10, 10, 10])\n",
      "      format_output = string_ops.string_format(\"{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \"  [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \"  [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \"  [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \"  [90 91 92 ... 97 98 99]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[100 101 102 ... 107 108 109]\\n\"\n",
      "                  \"  [110 111 112 ... 117 118 119]\\n\"\n",
      "                  \"  [120 121 122 ... 127 128 129]\\n\"\n",
      "                  \"  ...\\n  [170 171 172 ... 177 178 179]\\n\"\n",
      "                  \"  [180 181 182 ... 187 188 189]\\n\"\n",
      "                  \"  [190 191 192 ... 197 198 199]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[200 201 202 ... 207 208 209]\\n\"\n",
      "                  \"  [210 211 212 ... 217 218 219]\\n\"\n",
      "                  \"  [220 221 222 ... 227 228 229]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [270 271 272 ... 277 278 279]\\n\"\n",
      "                  \"  [280 281 282 ... 287 288 289]\\n\"\n",
      "                  \"  [290 291 292 ... 297 298 299]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[700 701 702 ... 707 708 709]\\n\"\n",
      "                  \"  [710 711 712 ... 717 718 719]\\n\"\n",
      "                  \"  [720 721 722 ... 727 728 729]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [770 771 772 ... 777 778 779]\\n\"\n",
      "                  \"  [780 781 782 ... 787 788 789]\\n\"\n",
      "                  \"  [790 791 792 ... 797 798 799]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[800 801 802 ... 807 808 809]\\n\"\n",
      "                  \"  [810 811 812 ... 817 818 819]\\n\"\n",
      "                  \"  [820 821 822 ... 827 828 829]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [870 871 872 ... 877 878 879]\\n\"\n",
      "                  \"  [880 881 882 ... 887 888 889]\\n\"\n",
      "                  \"  [890 891 892 ... 897 898 899]]\\n\"\n",
      "                  \"\\n\"\n",
      "                  \" [[900 901 902 ... 907 908 909]\\n\"\n",
      "                  \"  [910 911 912 ... 917 918 919]\\n\"\n",
      "                  \"  [920 921 922 ... 927 928 929]\\n\"\n",
      "                  \"  ...\\n\"\n",
      "                  \"  [970 971 972 ... 977 978 979]\\n\"\n",
      "                  \"  [980 981 982 ... 987 988 989]\\n\"\n",
      "                  \"  [990 991 992 ... 997 998 999]]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTemplatePrefix(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: {}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTemplatePrefixAndSuffix(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: {}, suffix\",\n",
      "                                               tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]], suffix\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatOneTensorTemplateSuffix(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"{}, suffix\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"[[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]], suffix\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatNoTensor(self):\n",
      "    with self.cached_session():\n",
      "      format_output = string_ops.string_format(\"No tensor.\", ())\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = \"No tensor.\"\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatMultiTensor(self):\n",
      "    with self.cached_session():\n",
      "      tensor_one = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      tensor_two = tensor_one * 10\n",
      "      format_output = string_ops.string_format(\"One: {},\\nTwo: {}\",\n",
      "                                               (tensor_one, tensor_two))\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"One: [[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]],\\n\"\n",
      "                  \"Two: [[0 10 20 ... 70 80 90]\\n\"\n",
      "                  \" [100 110 120 ... 170 180 190]\\n\"\n",
      "                  \" [200 210 220 ... 270 280 290]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [700 710 720 ... 770 780 790]\\n\"\n",
      "                  \" [800 810 820 ... 870 880 890]\\n\"\n",
      "                  \" [900 910 920 ... 970 980 990]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatSummarizeOne(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: {}\", tensor,\n",
      "                                               summarize=1)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 ... 9]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [90 ... 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatSummarizeTwo(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: {}\", tensor,\n",
      "                                               summarize=2)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 1 ... 8 9]\\n\"\n",
      "                  \" [10 11 ... 18 19]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [80 81 ... 88 89]\\n\"\n",
      "                  \" [90 91 ... 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testFormatPlaceholder(self):\n",
      "    with self.cached_session():\n",
      "      tensor = array_ops.reshape(math_ops.range(100), [10, 10])\n",
      "      format_output = string_ops.string_format(\"tensor summary: %t%\", tensor,\n",
      "                                               placeholder=\"%t%\")\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = (\"tensor summary: [[0 1 2 ... 7 8 9]\\n\"\n",
      "                  \" [10 11 12 ... 17 18 19]\\n\"\n",
      "                  \" [20 21 22 ... 27 28 29]\\n\"\n",
      "                  \" ...\\n\"\n",
      "                  \" [70 71 72 ... 77 78 79]\\n\"\n",
      "                  \" [80 81 82 ... 87 88 89]\\n\"\n",
      "                  \" [90 91 92 ... 97 98 99]]\")\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testTensorCountMustMatchPlaceholderCount(self):\n",
      "    with self.cached_session():\n",
      "      with self.assertRaisesRegex(\n",
      "          ValueError, r\"The template expects 2 tensors, but the inputs only has\"\n",
      "          r\" 1\\.\\s.*\"):\n",
      "        tensor = math_ops.range(10)\n",
      "        format_output = string_ops.string_format(\"{} {}\", tensor)\n",
      "        self.evaluate(format_output)\n",
      "    with self.cached_session():\n",
      "      with self.assertRaisesRegex(\n",
      "          ValueError, r\"The template expects 2 tensors, but the inputs only has\"\n",
      "          r\" 1\\.\\s.*\"):\n",
      "        tensor = math_ops.range(10)\n",
      "        format_output = string_ops.string_format(\"{} {}\", [tensor])\n",
      "        self.evaluate(format_output)\n",
      "    with self.cached_session():\n",
      "      with self.assertRaisesRegex(\n",
      "          ValueError, r\"The template expects 1 tensors, but the inputs only has\"\n",
      "          r\" 2\\.\\s.*\"):\n",
      "        tensor = math_ops.range(10)\n",
      "        format_output = string_ops.string_format(\"{}\", (tensor, tensor))\n",
      "        self.evaluate(format_output)\n",
      "\n",
      "  @test_util.run_in_graph_and_eager_modes()\n",
      "  def testTensorAndFormatUnicode(self):\n",
      "    with self.cached_session():\n",
      "      tensor = constant_op.constant(\"😊\")\n",
      "      format_output = string_ops.string_format(\"😊:{}\", tensor)\n",
      "      out = self.evaluate(format_output)\n",
      "      expected = '😊:\"😊\"'\n",
      "      self.assertEqual(compat.as_text(out), expected)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  test.main()\n",
      "\n",
      "Average Line Length: 41.49487179487179\n",
      "\n",
      "Example 8:\n",
      "#!/usr/bin/python\n",
      "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "#\n",
      "# Test that checks if we have any issues with case insensitive filesystems.\n",
      "\n",
      "import os\n",
      "\n",
      "BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
      "ERROR_MESSAGE = \"\"\"\n",
      "Files with same name but different case detected in directory: {}\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def main():\n",
      "  # Make sure BASE_DIR ends with tensorflow.  If it doesn't, we probably\n",
      "  # computed the wrong directory.\n",
      "  if os.path.split(BASE_DIR)[-1] != 'tensorflow':\n",
      "    raise AssertionError(\n",
      "        \"BASE_DIR = '%s' doesn't end with tensorflow\" % BASE_DIR)\n",
      "\n",
      "  for dirpath, dirnames, filenames in os.walk(BASE_DIR, followlinks=True):\n",
      "    lowercase_directories = [x.lower() for x in dirnames]\n",
      "    lowercase_files = [x.lower() for x in filenames]\n",
      "\n",
      "    lowercase_dir_contents = lowercase_directories + lowercase_files\n",
      "    if len(lowercase_dir_contents) != len(set(lowercase_dir_contents)):\n",
      "      raise AssertionError(ERROR_MESSAGE.format(dirpath))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  main()\n",
      "\n",
      "Average Line Length: 35.733333333333334\n",
      "\n",
      "Example 9:\n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests for kernelized_utils.py.\"\"\"\n",
      "\n",
      "import functools\n",
      "\n",
      "from absl.testing import parameterized\n",
      "\n",
      "from tensorflow.python.framework import constant_op\n",
      "from tensorflow.python.keras.utils import kernelized_utils\n",
      "from tensorflow.python.platform import test\n",
      "\n",
      "\n",
      "def _exact_gaussian(stddev):\n",
      "  return functools.partial(\n",
      "      kernelized_utils.exact_gaussian_kernel, stddev=stddev)\n",
      "\n",
      "\n",
      "def _exact_laplacian(stddev):\n",
      "  return functools.partial(\n",
      "      kernelized_utils.exact_laplacian_kernel, stddev=stddev)\n",
      "\n",
      "\n",
      "class KernelizedUtilsTest(test.TestCase, parameterized.TestCase):\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=10.0), [[1.0]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=50.0), [[1.0]]))\n",
      "  def test_equal_vectors(self, exact_kernel_fn, expected_values):\n",
      "    \"\"\"Identical vectors give exactly the identity kernel value.\"\"\"\n",
      "    x = constant_op.constant([0.5, -0.5, -0.5, 0.5])\n",
      "    y = constant_op.constant([0.5, -0.5, -0.5, 0.5])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    # x and y are identical and therefore K(x, y) will be precisely equal to\n",
      "    # the identity value of the kernel.\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-6)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=10.0), [[1.0]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=50.0), [[1.0]]))\n",
      "  def test_almost_identical_vectors(self, exact_kernel_fn, expected_values):\n",
      "    \"\"\"Almost identical vectors give the identity kernel value.\"\"\"\n",
      "    x = constant_op.constant([1.0, 0.4, -2.1, -1.1])\n",
      "    y = constant_op.constant([1.01, 0.39, -2.099, -1.101])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    # x and y are almost identical and therefore K(x, y) will be almost equal to\n",
      "    # the identity value of the kernel.\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-3)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=1.0), [[0.99], [0.977]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=5.0), [[0.96], [0.94]]))\n",
      "  def test_similar_matrices(self, exact_kernel_fn, expected_values):\n",
      "    \"\"\"Pairwise \"close\" vectors give high kernel values (similarity scores).\"\"\"\n",
      "    x = constant_op.constant([1.0, 3.4, -2.1, 0.9, 3.3, -2.0], shape=[2, 3])\n",
      "    y = constant_op.constant([1.1, 3.35, -2.05])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    # The 2 rows of x are close to y. The pairwise kernel values (similarity\n",
      "    # scores) are somewhat close to the identity value of the kernel.\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-2)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=2.0), [[.997, .279], [.251, 1.],\n",
      "                                                 [.164, 0.019]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=2.0), [[.904, .128], [.116, 1.],\n",
      "                                                   [.07, 0.027]]))\n",
      "  def test_matrices_varying_similarity(self, exact_kernel_fn, expected_values):\n",
      "    \"\"\"Test matrices with row vectors of varying pairwise similarity.\"\"\"\n",
      "    x = constant_op.constant([1.0, 2., -2., 0.9, 3.3, -1.0], shape=[3, 2])\n",
      "    y = constant_op.constant([1.1, 2.1, -2., 0.9], shape=[2, 2])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-2)\n",
      "\n",
      "  @parameterized.named_parameters(\n",
      "      ('gaussian', _exact_gaussian(stddev=1.0), [[0.0]]),\n",
      "      ('laplacian', _exact_laplacian(stddev=1.0), [[0.0]]))\n",
      "  def test_completely_dissimilar_vectors(self, exact_kernel_fn,\n",
      "                                         expected_values):\n",
      "    \"\"\"Very dissimilar vectors give very low similarity scores.\"\"\"\n",
      "    x = constant_op.constant([1.0, 3.4, -2.1, -5.1])\n",
      "    y = constant_op.constant([0.5, 2.1, 1.0, 3.0])\n",
      "    exact_kernel = exact_kernel_fn(x, y)\n",
      "    shape = exact_kernel.shape.as_list()\n",
      "    self.assertLen(shape, 2)\n",
      "    # x and y are very \"far\" from each other and so the corresponding kernel\n",
      "    # value will be very low.\n",
      "    self.assertAllClose(expected_values, exact_kernel, atol=1e-2)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  test.main()\n",
      "\n",
      "Average Line Length: 43.09649122807018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def calculate_avg_line_length(example):\n",
    "    lines = example['content'].split('\\n')\n",
    "    avg_length = sum(len(line) for line in lines) / len(lines)\n",
    "    example['avg_line_length'] = avg_length\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(calculate_avg_line_length)\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    if i < 10:\n",
    "        print(f\"Example {i}:\")\n",
    "        print(example['content'])\n",
    "        print(f\"Average Line Length: {example['avg_line_length']}\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_dataset = dataset[:2500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(sliced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for avg_line_length:\n",
      "count    2.500000e+06\n",
      "mean     3.112980e+01\n",
      "std      8.800531e+00\n",
      "min      6.666667e-01\n",
      "25%      2.554167e+01\n",
      "50%      3.077778e+01\n",
      "75%      3.603093e+01\n",
      "max      1.252000e+02\n",
      "Name: avg_line_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "avg_line_length_stats = df['avg_line_length'].describe()\n",
    "print(\"Statistics for avg_line_length:\")\n",
    "print(avg_line_length_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=df['avg_line_length'])\n",
    "plt.title('Boxplot of Average Line Length')\n",
    "plt.xlabel('Average Line Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['avg_line_length'].quantile(0.25)\n",
    "Q3 = df['avg_line_length'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 54532\n"
     ]
    }
   ],
   "source": [
    "outliers = df[(df['avg_line_length'] < (Q1 - 1.5 * IQR)) | (df['avg_line_length'] > (Q3 + 1.5 * IQR))]\n",
    "print(f\"Number of outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    54532.000000\n",
       "mean        52.571576\n",
       "std         20.955035\n",
       "min          0.666667\n",
       "25%         52.632847\n",
       "50%         55.793607\n",
       "75%         62.000000\n",
       "max        125.200000\n",
       "Name: avg_line_length, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers['avg_line_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(outliers.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.445468e+06\n",
       "mean     3.065167e+01\n",
       "std      7.674895e+00\n",
       "min      9.809524e+00\n",
       "25%      2.549206e+01\n",
       "50%      3.063218e+01\n",
       "75%      3.570588e+01\n",
       "max      5.176471e+01\n",
       "Name: avg_line_length, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['avg_line_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.percentile(df['avg_line_length'], 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.445468e+06\n",
       "mean     1.385783e+02\n",
       "std      3.559914e+02\n",
       "min      1.000000e+00\n",
       "25%      2.900000e+01\n",
       "50%      6.700000e+01\n",
       "75%      1.500000e+02\n",
       "max      7.455100e+04\n",
       "Name: line_count, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['line_count'] = df['content'].apply(lambda x: len(x.split('\\n')))\n",
    "df['line_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_range_dict = {\n",
    "    \"0-100\": 0,\n",
    "    \"101-200\": 0,\n",
    "    \"201-300\": 0,\n",
    "    \"301-400\": 0,\n",
    "    \"401-500\": 0,\n",
    "    \"501-600\": 0,\n",
    "    \"601-700\": 0,\n",
    "    \"701-800\": 0,\n",
    "    \"801-900\": 0,\n",
    "    \"901-1000\": 0,\n",
    "    \"1000+\": 0\n",
    "}\n",
    "\n",
    "for count in df['line_count']:\n",
    "    if count <= 100:\n",
    "        count_range_dict[\"0-100\"] += 1\n",
    "    elif count <= 200:\n",
    "        count_range_dict[\"101-200\"] += 1\n",
    "    elif count <= 300:\n",
    "        count_range_dict[\"201-300\"] += 1\n",
    "    elif count <= 400:\n",
    "        count_range_dict[\"301-400\"] += 1\n",
    "    elif count <= 500:\n",
    "        count_range_dict[\"401-500\"] += 1\n",
    "    elif count <= 600:\n",
    "        count_range_dict[\"501-600\"] += 1\n",
    "    elif count <= 700:\n",
    "        count_range_dict[\"601-700\"] += 1\n",
    "    elif count <= 800:\n",
    "        count_range_dict[\"701-800\"] += 1\n",
    "    elif count <= 900:\n",
    "        count_range_dict[\"801-900\"] += 1\n",
    "    elif count <= 1000:\n",
    "        count_range_dict[\"901-1000\"] += 1\n",
    "    else:\n",
    "        count_range_dict[\"1000+\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples with line count in range 0-100: 1544537\n",
      "Number of examples with line count in range 101-200: 465364\n",
      "Number of examples with line count in range 201-300: 186112\n",
      "Number of examples with line count in range 301-400: 91379\n",
      "Number of examples with line count in range 401-500: 50533\n",
      "Number of examples with line count in range 501-600: 30436\n",
      "Number of examples with line count in range 601-700: 19614\n",
      "Number of examples with line count in range 701-800: 13167\n",
      "Number of examples with line count in range 801-900: 9431\n",
      "Number of examples with line count in range 901-1000: 6825\n",
      "Number of examples with line count in range 1000+: 28070\n"
     ]
    }
   ],
   "source": [
    "for key, value in count_range_dict.items():\n",
    "    print(f\"Number of examples with line count in range {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(count_range_dict.keys()), y=list(count_range_dict.values()))\n",
    "plt.title(\"Distribution of Python code lengths\")\n",
    "plt.xlabel(\"Length of Python code\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Count</th>\n",
       "      <th>Cumulative Count</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>Cumulative Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-100</td>\n",
       "      <td>1544537</td>\n",
       "      <td>1544537</td>\n",
       "      <td>63.159158</td>\n",
       "      <td>63.159158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101-200</td>\n",
       "      <td>465364</td>\n",
       "      <td>2009901</td>\n",
       "      <td>19.029650</td>\n",
       "      <td>82.188808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201-300</td>\n",
       "      <td>186112</td>\n",
       "      <td>2196013</td>\n",
       "      <td>7.610486</td>\n",
       "      <td>89.799294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>301-400</td>\n",
       "      <td>91379</td>\n",
       "      <td>2287392</td>\n",
       "      <td>3.736667</td>\n",
       "      <td>93.535961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401-500</td>\n",
       "      <td>50533</td>\n",
       "      <td>2337925</td>\n",
       "      <td>2.066394</td>\n",
       "      <td>95.602355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>501-600</td>\n",
       "      <td>30436</td>\n",
       "      <td>2368361</td>\n",
       "      <td>1.244588</td>\n",
       "      <td>96.846943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>601-700</td>\n",
       "      <td>19614</td>\n",
       "      <td>2387975</td>\n",
       "      <td>0.802055</td>\n",
       "      <td>97.648998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>701-800</td>\n",
       "      <td>13167</td>\n",
       "      <td>2401142</td>\n",
       "      <td>0.538425</td>\n",
       "      <td>98.187423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>801-900</td>\n",
       "      <td>9431</td>\n",
       "      <td>2410573</td>\n",
       "      <td>0.385652</td>\n",
       "      <td>98.573075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901-1000</td>\n",
       "      <td>6825</td>\n",
       "      <td>2417398</td>\n",
       "      <td>0.279088</td>\n",
       "      <td>98.852162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1000+</td>\n",
       "      <td>28070</td>\n",
       "      <td>2445468</td>\n",
       "      <td>1.147838</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length    Count  Cumulative Count  Percentage  Cumulative Percentage\n",
       "0      0-100  1544537           1544537   63.159158              63.159158\n",
       "1    101-200   465364           2009901   19.029650              82.188808\n",
       "2    201-300   186112           2196013    7.610486              89.799294\n",
       "3    301-400    91379           2287392    3.736667              93.535961\n",
       "4    401-500    50533           2337925    2.066394              95.602355\n",
       "5    501-600    30436           2368361    1.244588              96.846943\n",
       "6    601-700    19614           2387975    0.802055              97.648998\n",
       "7    701-800    13167           2401142    0.538425              98.187423\n",
       "8    801-900     9431           2410573    0.385652              98.573075\n",
       "9   901-1000     6825           2417398    0.279088              98.852162\n",
       "10     1000+    28070           2445468    1.147838             100.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_line = pd.DataFrame(list(count_range_dict.items()), columns=[\"Length\", \"Count\"])\n",
    "df_line[\"Cumulative Count\"] = df_line[\"Count\"].cumsum()\n",
    "df_line[\"Percentage\"] = df_line[\"Count\"] / df_line[\"Count\"].sum() * 100\n",
    "df_line[\"Cumulative Percentage\"] = df_line[\"Percentage\"].cumsum()\n",
    "df_line.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(df['line_count'], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id',\n",
       "       'content', 'avg_line_length', 'line_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>line_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from argparse import ArgumentParser, _HelpActi...</td>\n",
       "      <td>21.828571</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;reponame&gt;vegYY/react\\n{\\n  \"targets\": [\\n    ...</td>\n",
       "      <td>15.588235</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;reponame&gt;EricRemmerswaal/tensorflow&lt;filename&gt;...</td>\n",
       "      <td>40.898158</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># Copyright 2021 The TensorFlow Authors. All R...</td>\n",
       "      <td>34.339623</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;reponame&gt;EricRemmerswaal/tensorflow&lt;gh_stars&gt;...</td>\n",
       "      <td>36.655172</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  avg_line_length  \\\n",
       "0  from argparse import ArgumentParser, _HelpActi...        21.828571   \n",
       "1  <reponame>vegYY/react\\n{\\n  \"targets\": [\\n    ...        15.588235   \n",
       "2  <reponame>EricRemmerswaal/tensorflow<filename>...        40.898158   \n",
       "3  # Copyright 2021 The TensorFlow Authors. All R...        34.339623   \n",
       "4  <reponame>EricRemmerswaal/tensorflow<gh_stars>...        36.655172   \n",
       "\n",
       "   line_count  \n",
       "0         105  \n",
       "1          17  \n",
       "2         923  \n",
       "3          53  \n",
       "4          58  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df.drop(columns=['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id'], axis=1)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2445468, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2196013, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_filtered[df_filtered['line_count'] <= 300]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1808818, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_filtered[df_filtered['avg_line_length'] <= 37]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>line_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from argparse import ArgumentParser, _HelpActi...</td>\n",
       "      <td>21.828571</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;reponame&gt;vegYY/react\\n{\\n  \"targets\": [\\n    ...</td>\n",
       "      <td>15.588235</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># Copyright 2021 The TensorFlow Authors. All R...</td>\n",
       "      <td>34.339623</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;reponame&gt;EricRemmerswaal/tensorflow&lt;gh_stars&gt;...</td>\n",
       "      <td>36.655172</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td># Copyright 2021 The TensorFlow Authors. All R...</td>\n",
       "      <td>32.333333</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  avg_line_length  \\\n",
       "0  from argparse import ArgumentParser, _HelpActi...        21.828571   \n",
       "1  <reponame>vegYY/react\\n{\\n  \"targets\": [\\n    ...        15.588235   \n",
       "3  # Copyright 2021 The TensorFlow Authors. All R...        34.339623   \n",
       "4  <reponame>EricRemmerswaal/tensorflow<gh_stars>...        36.655172   \n",
       "5  # Copyright 2021 The TensorFlow Authors. All R...        32.333333   \n",
       "\n",
       "   line_count  \n",
       "0         105  \n",
       "1          17  \n",
       "3          53  \n",
       "4          58  \n",
       "5          45  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       avg_line_length    line_count\n",
      "count     2.445468e+06  2.445468e+06\n",
      "mean      3.065167e+01  1.385783e+02\n",
      "std       7.674895e+00  3.559914e+02\n",
      "min       9.809524e+00  1.000000e+00\n",
      "25%       2.549206e+01  2.900000e+01\n",
      "50%       3.063218e+01  6.700000e+01\n",
      "75%       3.570588e+01  1.500000e+02\n",
      "max       5.176471e+01  7.455100e+04\n",
      "\n",
      "       avg_line_length    line_count\n",
      "count     1.808818e+06  1.808818e+06\n",
      "mean      2.762792e+01  7.350773e+01\n",
      "std       5.809639e+00  6.622066e+01\n",
      "min       9.809524e+00  1.000000e+00\n",
      "25%       2.381818e+01  2.400000e+01\n",
      "50%       2.834194e+01  5.100000e+01\n",
      "75%       3.220408e+01  1.030000e+02\n",
      "max       3.700000e+01  3.000000e+02\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"avg_line_length\", \"line_count\"]].describe())\n",
    "print()\n",
    "print(df_filtered.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_python = {}\n",
    "\n",
    "for i, example in enumerate(df_filtered[\"content\"]):\n",
    "    data_dict_python[f\"python_{i}\"] = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_dict_python) == len(df_filtered), \"Data dictionary length does not match the number of examples in the dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example python_0:\n",
      "from argparse import ArgumentParser, _HelpAction\n",
      "from pkgutil import get_data\n",
      "from sys import exit\n",
      "\n",
      "# flie basename no extension\n",
      "LICENSES = [\n",
      "    \"agpl-3.0\",\n",
      "    \"apache-2.0\",\n",
      "    \"bsd-2-clause\",\n",
      "    \"bsd-3-clause\",\n",
      "    \"epl-2.0\",\n",
      "    \"gpl-2.0\",\n",
      "    \"gpl-3.0\",\n",
      "    \"lgpl-2.1\",\n",
      "    \"lgpl-3.0\",\n",
      "    \"mit\",\n",
      "    \"mpl-2.0\",\n",
      "    \"unlicenses\",\n",
      "    \"996icu-0.1\",\n",
      "]\n",
      "\n",
      "\n",
      "def getparser():\n",
      "    parser = ArgumentParser(\n",
      "        prog=\"gen-license\",\n",
      "        description=\"tools to create license file, support GitHub LICENSE code.\",\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"code\", help=\"LICENSE Code, --list to see\", choices=LICENSES,\n",
      "        nargs=\"?\", const=None\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--list\", dest=\"list\", help=\"Show supported LICENSE Codes\", required=False,\n",
      "        action=\"store_true\"\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--996icu\", dest=\"icu\", help=\"Expand LICENSE with 996ICU LICENSE, Choose a language vesion or default zh-cn\",\n",
      "        required=False, nargs=\"?\", const=\"zh-cn\", default=None,\n",
      "        choices=[\"en-us\", \"zh-cn\"]\n",
      "    )\n",
      "\n",
      "    return parser\n",
      "\n",
      "\n",
      "def select_template(language_code):\n",
      "    \"\"\"choose a 996icu LICENSE template according to *language_code*\n",
      "    \"\"\"\n",
      "    map_ = {\n",
      "        \"zh\": \"zh-cn\",\n",
      "        \"zh-cn\": \"zh-cn\",\n",
      "        \"zh-hans\": \"zh-cn\",\n",
      "        \"en\": \"en-us\",\n",
      "        \"en-us\": \"en-us\",\n",
      "    }\n",
      "\n",
      "    template = get_data(\n",
      "        __package__,\n",
      "        \"licenses/996.icu.template.{}.txt\".format(\n",
      "            map_.get(language_code, \"zh-cn\")\n",
      "        )\n",
      "    ).decode(\"utf-8\")\n",
      "\n",
      "    return template\n",
      "\n",
      "\n",
      "def main():\n",
      "    parser = getparser()\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    if args.list:\n",
      "        for license in LICENSES:\n",
      "            print(license)\n",
      "\n",
      "        exit(0)\n",
      "    else:  # main\n",
      "\n",
      "        # if no args input, show help and exit\n",
      "        if args.code is None:\n",
      "            parser.print_help()\n",
      "            parser.exit()\n",
      "\n",
      "        resource = get_data(\n",
      "            __package__,\n",
      "            \"licenses/{code}.txt\".format(code=args.code)\n",
      "        ).decode(\"utf-8\")\n",
      "\n",
      "        if args.icu is not None:  # --996icu option enabled\n",
      "            template = select_template(args.icu)\n",
      "\n",
      "            output = template.format(\n",
      "                other=args.code,\n",
      "                content=resource\n",
      "            ).encode(\"utf-8\")\n",
      "\n",
      "        else:  # common license\n",
      "            output = resource.encode(\"utf-8\")\n",
      "\n",
      "        with open(\"LICENSE\", \"wb\") as file:\n",
      "            file.write(output)\n",
      "\n",
      "        exit(0)\n",
      "\n",
      "\n",
      "\n",
      "Example python_1:\n",
      "<reponame>vegYY/react\n",
      "{\n",
      "  \"targets\": [\n",
      "    {\n",
      "      \"target_name\": \"perfcounters\",\n",
      "      \"sources\": [\n",
      "        \"src/hardware-counter.cpp\",\n",
      "        \"src/perf-counters.cpp\",\n",
      "        \"src/thread-local.cpp\",\n",
      "      ],\n",
      "      \"cflags\": [\n",
      "        \"-Wno-sign-compare\",\n",
      "      ],\n",
      "    },\n",
      "  ],\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example python_2:\n",
      "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"This is a Python API fuzzer for tf.raw_ops.DataFormatVecPermute.\"\"\"\n",
      "import atheris\n",
      "with atheris.instrument_imports():\n",
      "  import sys\n",
      "  from python_fuzzing import FuzzingHelper\n",
      "  import tensorflow as tf\n",
      "\n",
      "\n",
      "@atheris.instrument_func\n",
      "def TestOneInput(input_bytes):\n",
      "  \"\"\"Test randomized integer fuzzing input for tf.raw_ops.DataFormatVecPermute.\"\"\"\n",
      "  fh = FuzzingHelper(input_bytes)\n",
      "\n",
      "  dtype = fh.get_tf_dtype()\n",
      "  # Max shape can be 8 in length and randomized from 0-8 without running into\n",
      "  # a OOM error.\n",
      "  shape = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\n",
      "  seed = fh.get_int()\n",
      "  try:\n",
      "    x = tf.random.uniform(shape=shape, dtype=dtype, seed=seed)\n",
      "    src_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\n",
      "    dest_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\n",
      "    _ = tf.raw_ops.DataFormatVecPermute(\n",
      "        x,\n",
      "        src_format=src_format_digits,\n",
      "        dst_format=dest_format_digits,\n",
      "        name=fh.get_string())\n",
      "  except (tf.errors.InvalidArgumentError, ValueError, TypeError):\n",
      "    pass\n",
      "\n",
      "\n",
      "def main():\n",
      "  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n",
      "  atheris.Fuzz()\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  main()\n",
      "\n",
      "\n",
      "\n",
      "Example python_3:\n",
      "<reponame>EricRemmerswaal/tensorflow<gh_stars>1000+\n",
      "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"FuncGraphs for V2 control flow.\"\"\"\n",
      "\n",
      "from tensorflow.python.framework import func_graph\n",
      "from tensorflow.python.framework import ops\n",
      "\n",
      "\n",
      "class ControlFlowFuncGraph(func_graph.FuncGraph):\n",
      "  \"\"\"Contains control flow-specific FuncGraph logic.\"\"\"\n",
      "\n",
      "  def __init__(self, *args, **kwargs):\n",
      "    super(ControlFlowFuncGraph, self).__init__(*args, **kwargs)\n",
      "    outer_graph = self.outer_graph\n",
      "    # Unlike tf.function, control flow FuncGraphs are generally created one per\n",
      "    # op. This means hard-coding any outer device scopes in the body (rather\n",
      "    # than inspecting the call-time placement of the control flow op) makes\n",
      "    # sense.\n",
      "    self._device_function_stack = outer_graph._device_function_stack.copy()  # pylint: disable=protected-access\n",
      "    self.is_control_flow_graph = True\n",
      "    if ops.executing_eagerly_outside_functions():\n",
      "      func_graph.override_func_graph_name_scope(\n",
      "          self, self.outer_graph.get_name_scope())\n",
      "\n",
      "\n",
      "class CondBranchFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for branches of tf.cond().\n",
      "\n",
      "  This is used to distinguish cond branches from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "class WhileCondFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for the condition of tf.while_loop().\n",
      "\n",
      "  This is used to distinguish while conditions from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "class WhileBodyFuncGraph(ControlFlowFuncGraph):\n",
      "  \"\"\"FuncGraph for the body of tf.while_loop().\n",
      "\n",
      "  This is used to distinguish while bodies from other functions.\n",
      "  \"\"\"\n",
      "\n",
      "\n",
      "\n",
      "Example python_4:\n",
      "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"This is a Python API fuzzer for tf.raw_ops.RaggedCountSparseOutput.\"\"\"\n",
      "import atheris\n",
      "with atheris.instrument_imports():\n",
      "  import sys\n",
      "  from python_fuzzing import FuzzingHelper\n",
      "  import tensorflow as tf\n",
      "\n",
      "\n",
      "@atheris.instrument_func\n",
      "def TestOneInput(input_bytes):\n",
      "  \"\"\"Test randomized integer/float fuzzing input for tf.raw_ops.RaggedCountSparseOutput.\"\"\"\n",
      "  fh = FuzzingHelper(input_bytes)\n",
      "\n",
      "  splits = fh.get_int_list()\n",
      "  values = fh.get_int_or_float_list()\n",
      "  weights = fh.get_int_list()\n",
      "  try:\n",
      "    _, _, _, = tf.raw_ops.RaggedCountSparseOutput(\n",
      "        splits=splits, values=values, weights=weights, binary_output=False)\n",
      "  except tf.errors.InvalidArgumentError:\n",
      "    pass\n",
      "\n",
      "\n",
      "def main():\n",
      "  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n",
      "  atheris.Fuzz()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  main()\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (k,v) in enumerate(data_dict_python.items()):\n",
    "    if i < 5:\n",
    "        print(f\"Example {k}:\")\n",
    "        print(v)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to 'python_train.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('python_train.json', 'w') as f:\n",
    "    json.dump(data_dict_python, f)\n",
    "    print(\"Saved to 'python_train.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malcodeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
