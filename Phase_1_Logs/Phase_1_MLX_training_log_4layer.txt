Calling `python -m mlx_lm.lora...` directly is deprecated. Use `mlx_lm.lora...` or `python -m mlx_lm lora ...` instead.
Loading pretrained model
Loading datasets
Training
Trainable parameters: 0.007% (0.205M/3085.939M)
Starting training..., iters: 100
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 3245 will be truncated to 3072. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 3168 will be truncated to 3072. Consider pre-splitting your data to save memory.
Iter 1: Val loss 0.865, Val took 55.194s
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 3110 will be truncated to 3072. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 5283 will be truncated to 3072. Consider pre-splitting your data to save memory.
Iter 10: Train loss 0.841, Learning Rate 2.000e-05, It/sec 0.079, Tokens/sec 571.977, Trained Tokens 72753, Peak mem 30.695 GB
Iter 20: Train loss 0.853, Learning Rate 2.000e-05, It/sec 0.090, Tokens/sec 610.543, Trained Tokens 140446, Peak mem 30.695 GB
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 4060 will be truncated to 3072. Consider pre-splitting your data to save memory.
Iter 30: Train loss 0.908, Learning Rate 2.000e-05, It/sec 0.100, Tokens/sec 585.115, Trained Tokens 198802, Peak mem 30.695 GB
Iter 40: Train loss 0.760, Learning Rate 2.000e-05, It/sec 0.087, Tokens/sec 567.032, Trained Tokens 264011, Peak mem 30.695 GB
Iter 50: Train loss 0.778, Learning Rate 2.000e-05, It/sec 0.098, Tokens/sec 583.281, Trained Tokens 323501, Peak mem 30.695 GB
Iter 60: Train loss 0.691, Learning Rate 2.000e-05, It/sec 0.098, Tokens/sec 639.093, Trained Tokens 388907, Peak mem 30.695 GB
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 3110 will be truncated to 3072. Consider pre-splitting your data to save memory.
Iter 70: Train loss 0.662, Learning Rate 2.000e-05, It/sec 0.092, Tokens/sec 588.932, Trained Tokens 453219, Peak mem 30.695 GB
Iter 80: Train loss 0.583, Learning Rate 2.000e-05, It/sec 0.079, Tokens/sec 543.131, Trained Tokens 521796, Peak mem 30.695 GB
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 4060 will be truncated to 3072. Consider pre-splitting your data to save memory.
Iter 90: Train loss 0.563, Learning Rate 2.000e-05, It/sec 0.083, Tokens/sec 605.900, Trained Tokens 594653, Peak mem 30.695 GB
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 5283 will be truncated to 3072. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 3168 will be truncated to 3072. Consider pre-splitting your data to save memory.
[WARNING] Some sequences are longer than 3072 tokens. The longest sentence 3245 will be truncated to 3072. Consider pre-splitting your data to save memory.
Iter 100: Val loss 0.555, Val took 54.812s
Iter 100: Train loss 0.610, Learning Rate 2.000e-05, It/sec 0.089, Tokens/sec 590.504, Trained Tokens 660636, Peak mem 30.695 GB
Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.
Saved final weights to adapters/adapters.safetensors.
