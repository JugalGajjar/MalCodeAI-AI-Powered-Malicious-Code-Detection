{
    "python_2": {
        "input": "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"This is a Python API fuzzer for tf.raw_ops.DataFormatVecPermute.\"\"\"\nimport atheris\nwith atheris.instrument_imports():\n  import sys\n  from python_fuzzing import FuzzingHelper\n  import tensorflow as tf\n\n\n@atheris.instrument_func\ndef TestOneInput(input_bytes):\n  \"\"\"Test randomized integer fuzzing input for tf.raw_ops.DataFormatVecPermute.\"\"\"\n  fh = FuzzingHelper(input_bytes)\n\n  dtype = fh.get_tf_dtype()\n  # Max shape can be 8 in length and randomized from 0-8 without running into\n  # a OOM error.\n  shape = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\n  seed = fh.get_int()\n  try:\n    x = tf.random.uniform(shape=shape, dtype=dtype, seed=seed)\n    src_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\n    dest_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\n    _ = tf.raw_ops.DataFormatVecPermute(\n        x,\n        src_format=src_format_digits,\n        dst_format=dest_format_digits,\n        name=fh.get_string())\n  except (tf.errors.InvalidArgumentError, ValueError, TypeError):\n    pass\n\n\ndef main():\n  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n  atheris.Fuzz()\n\n\nif __name__ == '__main__':\n  main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import atheris\",\n        \"component_description\": \"Imports the Atheris library which is used for fuzzing Python code.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import sys\",\n        \"component_description\": \"Imports the system-specific parameters and functions module, allowing access to command-line arguments.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from python_fuzzing import FuzzingHelper\",\n        \"component_description\": \"Imports the FuzzingHelper class from the python_fuzzing module, used to facilitate fuzzing operations.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import tensorflow as tf\",\n        \"component_description\": \"Imports the TensorFlow library and assigns it the alias 'tf', enabling the use of its functionalities for machine learning tasks.\"\n      },\n      {\n        \"component_type\": \"DECORATOR\",\n        \"component_name\": \"@atheris.instrument_func\",\n        \"component_code\": \"@atheris.instrument_func\\ndef TestOneInput(input_bytes):\",\n        \"component_description\": \"The decorator '@atheris.instrument_func' is used to instrument the function 'TestOneInput' for fuzzing, allowing Atheris to collect coverage and generate inputs.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"TestOneInput\",\n        \"component_code\": \"def TestOneInput(input_bytes):\\n  fh = FuzzingHelper(input_bytes)\\n\\n  dtype = fh.get_tf_dtype()\\n  shape = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\\n  seed = fh.get_int()\\n  try:\\n    x = tf.random.uniform(shape=shape, dtype=dtype, seed=seed)\\n    src_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\\n    dest_format_digits = str(fh.get_int(min_int=0, max_int=999999999))\\n    _ = tf.raw_ops.DataFormatVecPermute(\\n        x,\\n        src_format=src_format_digits,\\n        dst_format=dest_format_digits,\\n        name=fh.get_string())\\n  except (tf.errors.InvalidArgumentError, ValueError, TypeError):\\n    pass\",\n        \"component_description\": \"Defines the function 'TestOneInput' which takes a byte string as input. It uses the FuzzingHelper to generate random data types, shapes, and seeds. A TensorFlow tensor is created with these parameters and passed to 'tf.raw_ops.DataFormatVecPermute'. The source and destination format digits are generated randomly and converted to strings. If any of the operations raise an InvalidArgumentError, ValueError, or TypeError, they are caught and ignored.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"main\",\n        \"component_code\": \"def main():\\n  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\\n  atheris.Fuzz()\",\n        \"component_description\": \"Defines the 'main' function which sets up the fuzzing environment using Atheris. It configures the setup to use the command-line arguments, specifies 'TestOneInput' as the target function, and enables Python coverage tracking. The actual fuzzing process is then initiated by calling 'atheris.Fuzz()'.\"\n      },\n      {\n        \"component_type\": \"IF_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if __name__ == '__main__':\\n  main()\",\n        \"component_description\": \"Checks if the script is being run as the main program. If true, it calls the 'main' function to start the fuzzing process.\"\n      }\n    ],\n    \"overall_description\": \"This Python script is a fuzzer for testing the TensorFlow operation 'tf.raw_ops.DataFormatVecPermute'. It uses Atheris, a coverage-guided fuzzing tool, to generate random inputs and test the robustness of the specified TensorFlow function. The script defines a helper function 'TestOneInput' that creates random tensors with randomized data types and shapes, and tests the 'DataFormatVecPermute' operation with these tensors. It handles various exceptions that may arise during this process. The fuzzing is initiated through the 'main' function when the script is executed directly.\"\n  }\n}"
    },
    "python_3": {
        "input": "<reponame>EricRemmerswaal/tensorflow<gh_stars>1000+\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"FuncGraphs for V2 control flow.\"\"\"\n\nfrom tensorflow.python.framework import func_graph\nfrom tensorflow.python.framework import ops\n\n\nclass ControlFlowFuncGraph(func_graph.FuncGraph):\n  \"\"\"Contains control flow-specific FuncGraph logic.\"\"\"\n\n  def __init__(self, *args, **kwargs):\n    super(ControlFlowFuncGraph, self).__init__(*args, **kwargs)\n    outer_graph = self.outer_graph\n    # Unlike tf.function, control flow FuncGraphs are generally created one per\n    # op. This means hard-coding any outer device scopes in the body (rather\n    # than inspecting the call-time placement of the control flow op) makes\n    # sense.\n    self._device_function_stack = outer_graph._device_function_stack.copy()  # pylint: disable=protected-access\n    self.is_control_flow_graph = True\n    if ops.executing_eagerly_outside_functions():\n      func_graph.override_func_graph_name_scope(\n          self, self.outer_graph.get_name_scope())\n\n\nclass CondBranchFuncGraph(ControlFlowFuncGraph):\n  \"\"\"FuncGraph for branches of tf.cond().\n\n  This is used to distinguish cond branches from other functions.\n  \"\"\"\n\n\nclass WhileCondFuncGraph(ControlFlowFuncGraph):\n  \"\"\"FuncGraph for the condition of tf.while_loop().\n\n  This is used to distinguish while conditions from other functions.\n  \"\"\"\n\n\nclass WhileBodyFuncGraph(ControlFlowFuncGraph):\n  \"\"\"FuncGraph for the body of tf.while_loop().\n\n  This is used to distinguish while bodies from other functions.\n  \"\"\"\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.framework import func_graph\\nfrom tensorflow.python.framework import ops\",\n        \"component_description\": \"Imports the necessary modules from TensorFlow's framework package: `func_graph` and `ops`. These are required for defining custom function graphs and handling operations, respectively.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"ControlFlowFuncGraph\",\n        \"component_code\": \"class ControlFlowFuncGraph(func_graph.FuncGraph):\\n  \\\"\\\"\\\"Contains control flow-specific FuncGraph logic.\\\"\\\"\\\"\\n\\n  def __init__(self, *args, **kwargs):\\n    super(ControlFlowFuncGraph, self).__init__(*args, **kwargs)\\n    outer_graph = self.outer_graph\\n    # Unlike tf.function, control flow FuncGraphs are generally created one per\\n    # op. This means hard-coding any outer device scopes in the body (rather\\n    # than inspecting the call-time placement of the control flow op) makes\\n    # sense.\\n    self._device_function_stack = outer_graph._device_function_stack.copy()  # pylint: disable=protected-access\\n    self.is_control_flow_graph = True\\n    if ops.executing_eagerly_outside_functions():\\n      func_graph.override_func_graph_name_scope(\\n          self, self.outer_graph.get_name_scope())\",\n        \"component_description\": \"Defines a class `ControlFlowFuncGraph` that inherits from `func_graph.FuncGraph`. This class contains specific logic for handling control flow in TensorFlow. The constructor initializes the graph, copies the device function stack from the outer graph, sets an attribute to indicate it's a control flow graph, and optionally overrides the name scope if executing eagerly.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"CondBranchFuncGraph\",\n        \"component_code\": \"class CondBranchFuncGraph(ControlFlowFuncGraph):\\n  \\\"\\\"\\\"FuncGraph for branches of tf.cond().\\n\\n  This is used to distinguish cond branches from other functions.\\n  \\\"\\\"\\\"\",\n        \"component_description\": \"Defines a class `CondBranchFuncGraph` that inherits from `ControlFlowFuncGraph`. This class represents the function graph for branches of `tf.cond()`, allowing them to be distinguished from other types of functions.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"WhileCondFuncGraph\",\n        \"component_code\": \"class WhileCondFuncGraph(ControlFlowFuncGraph):\\n  \\\"\\\"\\\"FuncGraph for the condition of tf.while_loop().\\n\\n  This is used to distinguish while conditions from other functions.\\n  \\\"\\\"\\\"\",\n        \"component_description\": \"Defines a class `WhileCondFuncGraph` that inherits from `ControlFlowFuncGraph`. This class represents the function graph for the condition of `tf.while_loop()`, enabling it to be distinguished from other types of functions.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"WhileBodyFuncGraph\",\n        \"component_code\": \"class WhileBodyFuncGraph(ControlFlowFuncGraph):\\n  \\\"\\\"\\\"FuncGraph for the body of tf.while_loop().\\n\\n  This is used to distinguish while bodies from other functions.\\n  \\\"\\\"\\\"\",\n        \"component_description\": \"Defines a class `WhileBodyFuncGraph` that inherits from `ControlFlowFuncGraph`. This class represents the function graph for the body of `tf.while_loop()`, allowing it to be distinguished from other types of functions.\"\n      }\n    ],\n    \"overall_description\": \"The provided Python code defines custom function graphs for handling control flow operations in TensorFlow, specifically for conditional statements (`tf.cond`) and while loops (`tf.while_loop`). The classes `CondBranchFuncGraph`, `WhileCondFuncGraph`, and `WhileBodyFuncGraph` inherit from the base class `ControlFlowFuncGraph`, which extends TensorFlow's `func_graph.FuncGraph`. These custom graphs are tailored to capture and manage specific aspects of control flow logic, ensuring that they can be appropriately distinguished and executed within TensorFlow's computational graph framework.\"\n  }\n}"
    },
    "python_4": {
        "input": "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"This is a Python API fuzzer for tf.raw_ops.RaggedCountSparseOutput.\"\"\"\nimport atheris\nwith atheris.instrument_imports():\n  import sys\n  from python_fuzzing import FuzzingHelper\n  import tensorflow as tf\n\n\n@atheris.instrument_func\ndef TestOneInput(input_bytes):\n  \"\"\"Test randomized integer/float fuzzing input for tf.raw_ops.RaggedCountSparseOutput.\"\"\"\n  fh = FuzzingHelper(input_bytes)\n\n  splits = fh.get_int_list()\n  values = fh.get_int_or_float_list()\n  weights = fh.get_int_list()\n  try:\n    _, _, _, = tf.raw_ops.RaggedCountSparseOutput(\n        splits=splits, values=values, weights=weights, binary_output=False)\n  except tf.errors.InvalidArgumentError:\n    pass\n\n\ndef main():\n  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n  atheris.Fuzz()\n\n\nif __name__ == \"__main__\":\n  main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import atheris\\nwith atheris.instrument_imports():\\n  import sys\\n  from python_fuzzing import FuzzingHelper\\n  import tensorflow as tf\",\n        \"component_description\": \"Imports the necessary modules for the script. `atheris` is used for fuzzing, `sys` for system-specific parameters and functions, `FuzzingHelper` from `python_fuzzing` for generating test inputs, and `tensorflow` for using TensorFlow operations.\"\n      },\n      {\n        \"component_type\": \"DECORATOR\",\n        \"component_name\": null,\n        \"component_code\": \"@atheris.instrument_func\",\n        \"component_description\": \"Applies the `atheris.instrument_func` decorator to the function that follows it, enabling fuzzing instrumentation on that specific function.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"TestOneInput\",\n        \"component_code\": \"def TestOneInput(input_bytes):\\n  \\\"\\\"\\\"Test randomized integer/float fuzzing input for tf.raw_ops.RaggedCountSparseOutput.\\\"\\\"\\\"\\n  fh = FuzzingHelper(input_bytes)\\n\\n  splits = fh.get_int_list()\\n  values = fh.get_int_or_float_list()\\n  weights = fh.get_int_list()\\n  try:\\n    _, _, _, = tf.raw_ops.RaggedCountSparseOutput(\\n        splits=splits, values=values, weights=weights, binary_output=False)\\n  except tf.errors.InvalidArgumentError:\\n    pass\",\n        \"component_description\": \"Defines a function `TestOneInput` that takes `input_bytes` as an argument. It uses `FuzzingHelper` to generate lists of integers and floats for the `splits`, `values`, and `weights` parameters of `tf.raw_ops.RaggedCountSparseOutput`. The function attempts to execute this TensorFlow operation, catching any `InvalidArgumentError` exceptions that may occur.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"main\",\n        \"component_code\": \"def main():\\n  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\\n  atheris.Fuzz()\",\n        \"component_description\": \"Defines the `main` function that sets up and runs the fuzzing process. It configures `atheris` with command-line arguments, the test function `TestOneInput`, and enables Python coverage. The `Fuzz()` method starts the fuzzing loop.\"\n      },\n      {\n        \"component_type\": \"IF_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if __name__ == \\\"__main__\\\":\\n  main()\",\n        \"component_description\": \"Checks if the script is being run as the main program. If true, it calls the `main` function to execute the setup and fuzzing process.\"\n      }\n    ],\n    \"overall_description\": \"This Python script is a fuzzer for TensorFlow's raw operation `RaggedCountSparseOutput`. It uses the `atheris` library to generate randomized inputs from which lists of integers and floats are created. These lists serve as parameters for the TensorFlow operation, and the script attempts to execute it while catching any `InvalidArgumentError` exceptions. The fuzzing process is set up and executed via the `main` function, ensuring that the script can be run directly.\"\n  }\n}"
    },
    "python_5": {
        "input": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.nn_ops.Cross.\"\"\"\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import test\n\n\nclass CrossOpTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testGradientRandomValues(self):\n    with self.cached_session():\n      us = [2, 3]\n      u = array_ops.reshape(\n          [0.854, -0.616, 0.767, 0.725, -0.927, 0.159], shape=us)\n      v = array_ops.reshape(\n          [-0.522, 0.755, 0.407, -0.652, 0.241, 0.247], shape=us)\n      s = math_ops.cross(u, v)\n      jacob_u, jacob_v = gradient_checker.compute_gradient([u, v], [us, us], s,\n                                                           us)\n\n    self.assertAllClose(jacob_u[0], jacob_u[1], rtol=1e-3, atol=1e-3)\n    self.assertAllClose(jacob_v[0], jacob_v[1], rtol=1e-3, atol=1e-3)\n\n\nif __name__ == \"__main__\":\n  test.main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.framework import test_util\",\n        \"component_description\": \"Imports the test_util module from TensorFlow's framework package, which contains utilities for testing.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.ops import array_ops\",\n        \"component_description\": \"Imports the array_ops module from TensorFlow's ops package, providing operations that manipulate arrays.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.ops import gradient_checker\",\n        \"component_description\": \"Imports the gradient_checker module from TensorFlow's ops package, which provides utilities for checking gradients of tensor operations.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.ops import math_ops\",\n        \"component_description\": \"Imports the math_ops module from TensorFlow's ops package, providing mathematical operations on tensors.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.platform import test\",\n        \"component_description\": \"Imports the test module from TensorFlow's platform package, which provides a testing framework based on unittest.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"CrossOpTest\",\n        \"component_code\": \"class CrossOpTest(test.TestCase):\",\n        \"component_description\": \"Defines a class named CrossOpTest that inherits from test.TestCase, which is part of TensorFlow's testing framework. This class will contain test cases for the cross product operation.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testGradientRandomValues\",\n        \"component_code\": \"@test_util.run_deprecated_v1\\ndef testGradientRandomValues(self):\\n  with self.cached_session():\\n    us = [2, 3]\\n    u = array_ops.reshape([0.854, -0.616, 0.767, 0.725, -0.927, 0.159], shape=us)\\n    v = array_ops.reshape([-0.522, 0.755, 0.407, -0.652, 0.241, 0.247], shape=us)\\n    s = math_ops.cross(u, v)\\n    jacob_u, jacob_v = gradient_checker.compute_gradient([u, v], [us, us], s, us)\\n\\n  self.assertAllClose(jacob_u[0], jacob_u[1], rtol=1e-3, atol=1e-3)\\n  self.assertAllClose(jacob_v[0], jacob_v[1], rtol=1e-3, atol=1e-3)\",\n        \"component_description\": \"Defines a test method named testGradientRandomValues within the CrossOpTest class. This method tests the gradient computation of the cross product operation using randomly assigned values. It uses TensorFlow's session management to run operations and checks if the computed gradients for tensors u and v are close enough.\"\n      },\n      {\n        \"component_type\": \"DECORATOR\",\n        \"component_name\": null,\n        \"component_code\": \"@test_util.run_deprecated_v1\",\n        \"component_description\": \"A decorator applied to testGradientRandomValues method, which is used to run the test in a deprecated TensorFlow v1 session environment.\"\n      },\n      {\n        \"component_type\": \"WITH_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"with self.cached_session():\",\n        \"component_description\": \"A context manager that manages a cached TensorFlow session. It ensures that all operations within the block are executed in this session, and resources are managed efficiently.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"us\",\n        \"component_code\": \"us = [2, 3]\",\n        \"component_description\": \"Declares a variable us, which is a list representing the shape of tensors u and v. Here, it's set to [2, 3], indicating that both u and v will be 2x3 tensors.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"u\",\n        \"component_code\": \"u = array_ops.reshape([0.854, -0.616, 0.767, 0.725, -0.927, 0.159], shape=us)\",\n        \"component_description\": \"Declares a variable u, which is a tensor created by reshaping a list of six random values into the shape defined by us (2x3).\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"v\",\n        \"component_code\": \"v = array_ops.reshape([-0.522, 0.755, 0.407, -0.652, 0.241, 0.247], shape=us)\",\n        \"component_description\": \"Declares a variable v, which is a tensor created by reshaping another list of six random values into the shape defined by us (2x3).\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"s\",\n        \"component_code\": \"s = math_ops.cross(u, v)\",\n        \"component_description\": \"Declares a variable s, which is the result of computing the cross product between tensors u and v using TensorFlow's math_ops.cross function.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"jacob_u\",\n        \"component_code\": \"jacob_u, jacob_v = gradient_checker.compute_gradient([u, v], [us, us], s, us)\",\n        \"component_description\": \"Declares a variable jacob_u as part of the tuple returned by gradient_checker.compute_gradient. It computes the gradient of tensor s with respect to u.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"jacob_v\",\n        \"component_code\": \"jacob_u, jacob_v = gradient_checker.compute_gradient([u, v], [us, us], s, us)\",\n        \"component_description\": \"Declares a variable jacob_v as part of the tuple returned by gradient_checker.compute_gradient. It computes the gradient of tensor s with respect to v.\"\n      },\n      {\n        \"component_type\": \"ASSERT_METHOD\",\n        \"component_name\": null,\n        \"component_code\": \"self.assertAllClose(jacob_u[0], jacob_u[1], rtol=1e-3, atol=1e-3)\",\n        \"component_description\": \"An assertion method used to check if the elements of jacob_u[0] and jacob_u[1] are all close within a relative tolerance (rtol) and an absolute tolerance (atol). This is part of verifying that the computed gradients for u are consistent.\"\n      },\n      {\n        \"component_type\": \"ASSERT_METHOD\",\n        \"component_name\": null,\n        \"component_code\": \"self.assertAllClose(jacob_v[0], jacob_v[1], rtol=1e-3, atol=1e-3)\",\n        \"component_description\": \"An assertion method used to check if the elements of jacob_v[0] and jacob_v[1] are all close within a relative tolerance (rtol) and an absolute tolerance (atol). This is part of verifying that the computed gradients for v are consistent.\"\n      },\n      {\n        \"component_type\": \"MODULE_LEVEL_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if __name__ == '__main__':\\n  test.main()\",\n        \"component_description\": \"A module-level statement to run the tests defined in this script when it is executed directly. It calls the main method of TensorFlow's test framework, which will execute all test methods in the CrossOpTest class.\"\n      }\n    ],\n    \"overall_description\": \"This Python script defines a test case for verifying the gradient computation of the cross product operation using TensorFlow. The script imports necessary modules and classes from TensorFlow, defines a test class inheriting from TensorFlow's TestCase, and includes a method to test the gradients of the cross product with randomly assigned values. It uses TensorFlow's session management and gradient checking utilities to perform the tests.\"\n  }\n}"
    },
    "python_6": {
        "input": "#!/usr/bin/python\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n#\n# Test that checks if we have any issues with case insensitive filesystems.\n\nimport os\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\nERROR_MESSAGE = \"\"\"\nFiles with same name but different case detected in directory: {}\n\"\"\"\n\n\ndef main():\n  # Make sure BASE_DIR ends with tensorflow.  If it doesn't, we probably\n  # computed the wrong directory.\n  if os.path.split(BASE_DIR)[-1] != 'tensorflow':\n    raise AssertionError(\n        \"BASE_DIR = '%s' doesn't end with tensorflow\" % BASE_DIR)\n\n  for dirpath, dirnames, filenames in os.walk(BASE_DIR, followlinks=True):\n    lowercase_directories = [x.lower() for x in dirnames]\n    lowercase_files = [x.lower() for x in filenames]\n\n    lowercase_dir_contents = lowercase_directories + lowercase_files\n    if len(lowercase_dir_contents) != len(set(lowercase_dir_contents)):\n      raise AssertionError(ERROR_MESSAGE.format(dirpath))\n\n\nif __name__ == '__main__':\n  main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import os\",\n        \"component_description\": \"Imports the 'os' module which provides a way of using operating system dependent functionality.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"BASE_DIR\",\n        \"component_code\": \"BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\",\n        \"component_description\": \"Declares a variable BASE_DIR which holds the absolute path to the directory two levels up from the location of this script.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"ERROR_MESSAGE\",\n        \"component_code\": \"ERROR_MESSAGE = \\\"\\\"\\\"\\nFiles with same name but different case detected in directory: {}\\n\\\"\\\"\\\"\",\n        \"component_description\": \"Declares a variable ERROR_MESSAGE which contains a string template for an error message indicating the presence of files with names differing only by case.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"main\",\n        \"component_code\": \"def main():\\n  # Make sure BASE_DIR ends with tensorflow.  If it doesn't, we probably\\n  # computed the wrong directory.\\n  if os.path.split(BASE_DIR)[-1] != 'tensorflow':\\n    raise AssertionError(\\n        \\\"BASE_DIR = '%s' doesn't end with tensorflow\\\" % BASE_DIR)\\n\\n  for dirpath, dirnames, filenames in os.walk(BASE_DIR, followlinks=True):\\n    lowercase_directories = [x.lower() for x in dirnames]\\n    lowercase_files = [x.lower() for x in filenames]\\n\\n    lowercase_dir_contents = lowercase_directories + lowercase_files\\n    if len(lowercase_dir_contents) != len(set(lowercase_dir_contents)):\\n      raise AssertionError(ERROR_MESSAGE.format(dirpath))\",\n        \"component_description\": \"Defines the main function which performs a case-insensitive filesystem check. It first ensures that BASE_DIR ends with 'tensorflow'. Then, it walks through all directories and files starting from BASE_DIR, converting directory and file names to lowercase to check for duplicates in a case-insensitive manner. If any duplicate names are found, an AssertionError is raised with a formatted error message.\"\n      },\n      {\n        \"component_type\": \"IF_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if os.path.split(BASE_DIR)[-1] != 'tensorflow':\\n    raise AssertionError(\\n        \\\"BASE_DIR = '%s' doesn't end with tensorflow\\\" % BASE_DIR)\",\n        \"component_description\": \"An if statement within the main function that checks whether the last part of the path in BASE_DIR is 'tensorflow'. If it's not, an AssertionError is raised indicating that the computed directory might be incorrect.\"\n      },\n      {\n        \"component_type\": \"FOR_LOOP\",\n        \"component_name\": null,\n        \"component_code\": \"for dirpath, dirnames, filenames in os.walk(BASE_DIR, followlinks=True):\",\n        \"component_description\": \"A for loop that iterates over all directories and files starting from BASE_DIR. The os.walk function generates the file names in a directory tree by walking either top-down or bottom-up through the directory.\"\n      },\n      {\n        \"component_type\": \"LIST_COMPREHENSION\",\n        \"component_name\": null,\n        \"component_code\": \"lowercase_directories = [x.lower() for x in dirnames]\",\n        \"component_description\": \"A list comprehension that creates a new list, lowercase_directories, containing the names of all directories converted to lowercase.\"\n      },\n      {\n        \"component_type\": \"LIST_COMPREHENSION\",\n        \"component_name\": null,\n        \"component_code\": \"lowercase_files = [x.lower() for x in filenames]\",\n        \"component_description\": \"A list comprehension that creates a new list, lowercase_files, containing the names of all files converted to lowercase.\"\n      },\n      {\n        \"component_type\": \"IF_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if len(lowercase_dir_contents) != len(set(lowercase_dir_contents)):\\n      raise AssertionError(ERROR_MESSAGE.format(dirpath))\",\n        \"component_description\": \"An if statement that checks whether the length of lowercase_dir_contents (which combines lowercase_directories and lowercase_files) is equal to the length of its set representation. If they are not equal, it indicates there are duplicate file or directory names differing only by case in dirpath, raising an AssertionError with a formatted error message.\"\n      },\n      {\n        \"component_type\": \"IF_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if __name__ == '__main__':\\n  main()\",\n        \"component_description\": \"An if statement that checks whether the script is being run directly (not imported). If true, it calls the main function to execute the case-insensitive filesystem check.\"\n      }\n    ],\n    \"overall_description\": \"The Python script performs a case-insensitive filesystem check starting from the directory two levels above its location. It ensures that BASE_DIR ends with 'tensorflow' and then walks through all directories and files in BASE_DIR, checking for any files or directories with names differing only by case. If duplicates are found, it raises an AssertionError with a detailed error message indicating the problematic directory.\"\n  }\n}"
    },
    "python_8": {
        "input": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensor_array_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.platform import test\n\n\nclass TensorArrayOpsTest(test.TestCase):\n\n  @test_util.run_v1_only('Testing placeholders specifically.')\n  def test_concat_graph(self):\n    values = tensor_array_ops.TensorArray(\n        size=4, dtype=dtypes.string, element_shape=[None], infer_shape=False)\n    a = array_ops.placeholder(dtypes.string, [\n        None,\n    ])\n    b = array_ops.placeholder(dtypes.string, [\n        None,\n    ])\n    values = (values.write(0, a).write(\n        1, constant_op.constant([], dtypes.string))).write(2, b).write(\n            3, constant_op.constant([], dtypes.string))\n\n    with self.session() as s:\n      result = s.run(values.concat(), {a: ['a', 'b', 'c'], b: ['c', 'd', 'e']})\n    self.assertAllEqual(result, [b'a', b'b', b'c', b'c', b'd', b'e'])\n\n  @test_util.run_v2_only\n  def test_concat(self):\n    values = tensor_array_ops.TensorArray(\n        size=4, dtype=dtypes.string, element_shape=[None], infer_shape=False)\n    a = constant_op.constant(['a', 'b', 'c'], dtypes.string)\n    b = constant_op.constant(['c', 'd', 'e'], dtypes.string)\n    values = (values.write(0, a).write(\n        1, constant_op.constant([], dtypes.string))).write(2, b).write(\n            3, constant_op.constant([], dtypes.string))\n    self.assertAllEqual(values.concat(), [b'a', b'b', b'c', b'c', b'd', b'e'])\n\n  @test_util.run_v2_only\n  def test_concat_in_function(self):\n    @def_function.function\n    def fn(a, b):\n      values = tensor_array_ops.TensorArray(\n          size=4, dtype=dtypes.string, element_shape=[None], infer_shape=False)\n      values = (values.write(0, a).write(\n          1, constant_op.constant([], dtypes.string))).write(2, b).write(\n              3, constant_op.constant([], dtypes.string))\n      return values.concat()\n\n    self.assertAllEqual(fn(['a', 'b', 'c'], ['c', 'd', 'e']),\n                        [b'a', b'b', b'c', b'c', b'd', b'e'])\n\n  def test_init_numpy_shape(self):\n    @def_function.function\n    def fn():\n      values = tensor_array_ops.TensorArray(\n          np.float32,\n          size=1,\n          dynamic_size=False,\n          element_shape=np.array((2, 3)))\n      values = values.write(0, np.ones((2, 3)))\n      return values.concat()\n    self.assertAllEqual(fn(), [[1., 1., 1.], [1., 1., 1.]])\n\n\nif __name__ == '__main__':\n  test.main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": \"NULL\",\n        \"component_code\": \"import numpy as np\\nfrom tensorflow.python.eager import def_function\\nfrom tensorflow.python.framework import constant_op\\ndfrom tensorflow.python.framework import dtypes\\nfrom tensorflow.python.framework import test_util\\nfrom tensorflow.python.ops import array_ops\\nfrom tensorflow.python.ops import tensor_array_ops\\nfrom tensorflow.python.platform import test\",\n        \"component_description\": \"Imports necessary modules and functions from TensorFlow and NumPy for the test cases, including utilities for defining functions, creating constants, handling data types, testing utilities, array operations, tensor array operations, and running tests.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"TensorArrayOpsTest\",\n        \"component_code\": \"class TensorArrayOpsTest(test.TestCase):\",\n        \"component_description\": \"Defines a test case class named TensorArrayOpsTest that inherits from TensorFlow's TestCase, which provides methods for testing TensorFlow operations.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"test_concat_graph\",\n        \"component_code\": \"@test_util.run_v1_only('Testing placeholders specifically.')\\ndef test_concat_graph(self):\\n    values = tensor_array_ops.TensorArray(\\n        size=4, dtype=dtypes.string, element_shape=[None], infer_shape=False)\\n    a = array_ops.placeholder(dtypes.string, [\\n        None,\\n    ])\\n    b = array_ops.placeholder(dtypes.string, [\\n        None,\\n    ])\\n    values = (values.write(0, a).write(\\n        1, constant_op.constant([], dtypes.string))).write(2, b).write(\\n            3, constant_op.constant([], dtypes.string))\\n\\n    with self.session() as s:\\n      result = s.run(values.concat(), {a: ['a', 'b', 'c'], b: ['c', 'd', 'e']})\\n    self.assertAllEqual(result, [b'a', b'b', b'c', b'c', b'd', b'e'])\",\n        \"component_description\": \"Tests the concatenation of a TensorArray in TensorFlow v1 by writing string values to it and using placeholders for input. It checks if the concatenated result matches the expected byte strings.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"test_concat\",\n        \"component_code\": \"@test_util.run_v2_only\\ndef test_concat(self):\\n    values = tensor_array_ops.TensorArray(\\n        size=4, dtype=dtypes.string, element_shape=[None], infer_shape=False)\\n    a = constant_op.constant(['a', 'b', 'c'], dtypes.string)\\n    b = constant_op.constant(['c', 'd', 'e'], dtypes.string)\\n    values = (values.write(0, a).write(\\n        1, constant_op.constant([], dtypes.string))).write(2, b).write(\\n            3, constant_op.constant([], dtypes.string))\\n    self.assertAllEqual(values.concat(), [b'a', b'b', b'c', b'c', b'd', b'e'])\",\n        \"component_description\": \"Tests the concatenation of a TensorArray in TensorFlow v2 by writing string constants to it. It checks if the concatenated result matches the expected byte strings.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"test_concat_in_function\",\n        \"component_code\": \"@test_util.run_v2_only\\ndef test_concat_in_function(self):\\n    @def_function.function\\n    def fn(a, b):\\n      values = tensor_array_ops.TensorArray(\\n          np.float32,\\n          size=1,\\n          dynamic_size=False,\\n          element_shape=np.array((2, 3)))\\n      values = (values.write(0, a).write(\\n          1, constant_op.constant([], dtypes.string))).write(2, b).write(\\n              3, constant_op.constant([], dtypes.string))\\n      return values.concat()\\n\\n    self.assertAllEqual(fn(['a', 'b', 'c'], ['c', 'd', 'e']),\\n                        [b'a', b'b', b'c', b'c', b'd', b'e'])\",\n        \"component_description\": \"Tests the concatenation of a TensorArray within a TensorFlow function in v2. It writes string constants to the array and checks if the concatenated result matches the expected byte strings. Note: There is an inconsistency in the provided code where it attempts to write non-float32 values to a float32 TensorArray, which will raise an error.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"test_init_numpy_shape\",\n        \"component_code\": \"def test_init_numpy_shape(self):\\n    @def_function.function\\n    def fn():\\n      values = tensor_array_ops.TensorArray(\\n          np.float32,\\n          size=1,\\n          dynamic_size=False,\\n          element_shape=np.array((2, 3)))\\n      values = values.write(0, np.ones((2, 3)))\\n      return values.concat()\\n    self.assertAllEqual(fn(), [[1., 1., 1.], [1., 1., 1.]])\",\n        \"component_description\": \"Tests the initialization of a TensorArray with a NumPy shape in TensorFlow v2 by writing a NumPy array to it and checking if the concatenated result matches the expected two-dimensional array of ones.\"\n      },\n      {\n        \"component_type\": \"CODE_BLOCK\",\n        \"component_name\": \"NULL\",\n        \"component_code\": \"if __name__ == '__main__':\\n  test.main()\",\n        \"component_description\": \"The main block that runs the tests defined in the TensorArrayOpsTest class when the script is executed directly.\"\n      }\n    ],\n    \"overall_description\": \"This Python script defines a set of unit tests for TensorFlow's TensorArray operations, including concatenation and initialization with NumPy shapes. It uses TensorFlow's testing utilities to assert that the results match expected values under different conditions and versions (v1 and v2).\"\n  }\n}"
    },
    "python_9": {
        "input": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for debugger functionalities under multiple (i.e., >1) GPUs.\"\"\"\nimport os\nimport tempfile\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.python.client import session\nfrom tensorflow.python.debug.lib import debug_data\nfrom tensorflow.python.debug.lib import debug_utils\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import googletest\n\n\nclass SessionDebugMultiGPUTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    self._dump_root = tempfile.mkdtemp()\n\n  def tearDown(self):\n    ops.reset_default_graph()\n\n    # Tear down temporary dump directory.\n    if os.path.isdir(self._dump_root):\n      file_io.delete_recursively(self._dump_root)\n\n  def testMultiGPUSessionRun(self):\n    local_devices = device_lib.list_local_devices()\n    gpu_device_names = []\n    for device in local_devices:\n      if device.device_type == \"GPU\":\n        gpu_device_names.append(device.name)\n    gpu_device_names = sorted(gpu_device_names)\n\n    if len(gpu_device_names) < 2:\n      self.skipTest(\n          \"This test requires at least 2 GPUs, but only %d is available.\" %\n          len(gpu_device_names))\n\n    with session.Session() as sess:\n      v = variables.Variable([10.0, 15.0], dtype=dtypes.float32, name=\"v\")\n      with ops.device(gpu_device_names[0]):\n        u0 = math_ops.add(v, v, name=\"u0\")\n      with ops.device(gpu_device_names[1]):\n        u1 = math_ops.multiply(v, v, name=\"u1\")\n      w = math_ops.subtract(u1, u0, name=\"w\")\n\n      self.evaluate(v.initializer)\n\n      run_options = config_pb2.RunOptions(output_partition_graphs=True)\n      debug_utils.watch_graph(run_options, sess.graph,\n                              debug_urls=\"file://\" + self._dump_root)\n      run_metadata = config_pb2.RunMetadata()\n      self.assertAllClose(\n          [80.0, 195.0],\n          sess.run(w, options=run_options, run_metadata=run_metadata))\n\n      debug_dump_dir = debug_data.DebugDumpDir(\n          self._dump_root, partition_graphs=run_metadata.partition_graphs)\n      self.assertEqual(3, len(debug_dump_dir.devices()))\n      self.assertAllClose(\n          [10.0, 15.0], debug_dump_dir.get_tensors(\"v\", 0, \"DebugIdentity\")[0])\n      self.assertAllClose(\n          [20.0, 30.0], debug_dump_dir.get_tensors(\"u0\", 0, \"DebugIdentity\")[0])\n      self.assertAllClose(\n          [100.0, 225.0],\n          debug_dump_dir.get_tensors(\"u1\", 0, \"DebugIdentity\")[0])\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import os\\nimport tempfile\\nfrom tensorflow.core.protobuf import config_pb2\\nfrom tensorflow.python.client import device_lib\\nfrom tensorflow.python.client import session\\nfrom tensorflow.python.debug.lib import debug_data\\nfrom tensorflow.python.debug.lib import debug_utils\\nfrom tensorflow.python.framework import dtypes\\nfrom tensorflow.python.framework import ops\\nfrom tensorflow.python.framework import test_util\\nfrom tensorflow.python.lib.io import file_io\\nfrom tensorflow.python.ops import math_ops\\nfrom tensorflow.python.ops import variables\\nfrom tensorflow.python.platform import googletest\",\n        \"component_description\": \"Imports necessary Python modules and submodules required for the test script, including TensorFlow components for device management, session handling, debugging utilities, and data types.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"SessionDebugMultiGPUTest\",\n        \"component_code\": \"class SessionDebugMultiGPUTest(test_util.TensorFlowTestCase):\\n\\n  def setUp(self):\\n    self._dump_root = tempfile.mkdtemp()\\n\\n  def tearDown(self):\\n    ops.reset_default_graph()\\n\\n    # Tear down temporary dump directory.\\n    if os.path.isdir(self._dump_root):\\n      file_io.delete_recursively(self._dump_root)\\n\\n  def testMultiGPUSessionRun(self):\\n    local_devices = device_lib.list_local_devices()\\n    gpu_device_names = []\\n    for device in local_devices:\\n      if device.device_type == \\\"GPU\\\":\\n        gpu_device_names.append(device.name)\\n    gpu_device_names = sorted(gpu_device_names)\\n\\n    if len(gpu_device_names) < 2:\\n      self.skipTest(\\n          \\\"This test requires at least 2 GPUs, but only %d is available.\\\" %\\n          len(gpu_device_names))\\n\\n    with session.Session() as sess:\\n      v = variables.Variable([10.0, 15.0], dtype=dtypes.float32, name=\\\"v\\\")\\n      with ops.device(gpu_device_names[0]):\\n        u0 = math_ops.add(v, v, name=\\\"u0\\\")\\n      with ops.device(gpu_device_names[1]):\\n        u1 = math_ops.multiply(v, v, name=\\\"u1\\\")\\n      w = math_ops.subtract(u1, u0, name=\\\"w\\\")\\n\\n      self.evaluate(v.initializer)\\n\\n      run_options = config_pb2.RunOptions(output_partition_graphs=True)\\n      debug_utils.watch_graph(run_options, sess.graph,\\n                              debug_urls=\\\"file://\\\" + self._dump_root)\\n      run_metadata = config_pb2.RunMetadata()\\n      self.assertAllClose(\\n          [80.0, 195.0],\\n          sess.run(w, options=run_options, run_metadata=run_metadata))\\n\\n      debug_dump_dir = debug_data.DebugDumpDir(\\n          self._dump_root, partition_graphs=run_metadata.partition_graphs)\\n      self.assertEqual(3, len(debug_dump_dir.devices()))\\n      self.assertAllClose(\\n          [10.0, 15.0], debug_dump_dir.get_tensors(\\\"v\\\", 0, \\\"DebugIdentity\\\")[0])\\n      self.assertAllClose(\\n          [20.0, 30.0], debug_dump_dir.get_tensors(\\\"u0\\\", 0, \\\"DebugIdentity\\\")[0])\\n      self.assertAllClose(\\n          [100.0, 225.0],\\n          debug_dump_dir.get_tensors(\\\"u1\\\", 0, \\\"DebugIdentity\\\")[0])\",\n        \"component_description\": \"Defines a test class named `SessionDebugMultiGPUTest` that inherits from TensorFlow's `TensorFlowTestCase`. The class contains methods for setting up and tearing down the test environment, as well as the main test method `testMultiGPUSessionRun`.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"setUp\",\n        \"component_code\": \"def setUp(self):\\n    self._dump_root = tempfile.mkdtemp()\",\n        \"component_description\": \"Sets up the test environment by creating a temporary directory for storing debug dumps.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"tearDown\",\n        \"component_code\": \"def tearDown(self):\\n    ops.reset_default_graph()\\n\\n    # Tear down temporary dump directory.\\n    if os.path.isdir(self._dump_root):\\n      file_io.delete_recursively(self._dump_root)\",\n        \"component_description\": \"Cleans up the test environment by resetting the default TensorFlow graph and deleting the temporary directory used for debug dumps.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testMultiGPUSessionRun\",\n        \"component_code\": \"def testMultiGPUSessionRun(self):\\n    local_devices = device_lib.list_local_devices()\\n    gpu_device_names = []\\n    for device in local_devices:\\n      if device.device_type == \\\"GPU\\\":\\n        gpu_device_names.append(device.name)\\n    gpu_device_names = sorted(gpu_device_names)\\n\\n    if len(gpu_device_names) < 2:\\n      self.skipTest(\\n          \\\"This test requires at least 2 GPUs, but only %d is available.\\\" %\\n          len(gpu_device_names))\\n\\n    with session.Session() as sess:\\n      v = variables.Variable([10.0, 15.0], dtype=dtypes.float32, name=\\\"v\\\")\\n      with ops.device(gpu_device_names[0]):\\n        u0 = math_ops.add(v, v, name=\\\"u0\\\")\\n      with ops.device(gpu_device_names[1]):\\n        u1 = math_ops.multiply(v, v, name=\\\"u1\\\")\\n      w = math_ops.subtract(u1, u0, name=\\\"w\\\")\\n\\n      self.evaluate(v.initializer)\\n\\n      run_options = config_pb2.RunOptions(output_partition_graphs=True)\\n      debug_utils.watch_graph(run_options, sess.graph,\\n                              debug_urls=\\\"file://\\\" + self._dump_root)\\n      run_metadata = config_pb2.RunMetadata()\\n      self.assertAllClose(\\n          [80.0, 195.0],\\n          sess.run(w, options=run_options, run_metadata=run_metadata))\\n\\n      debug_dump_dir = debug_data.DebugDumpDir(\\n          self._dump_root, partition_graphs=run_metadata.partition_graphs)\\n      self.assertEqual(3, len(debug_dump_dir.devices()))\\n      self.assertAllClose(\\n          [10.0, 15.0], debug_dump_dir.get_tensors(\\\"v\\\", 0, \\\"DebugIdentity\\\")[0])\\n      self.assertAllClose(\\n          [20.0, 30.0], debug_dump_dir.get_tensors(\\\"u0\\\", 0, \\\"DebugIdentity\\\")[0])\\n      self.assertAllClose(\\n          [100.0, 225.0],\\n          debug_dump_dir.get_tensors(\\\"u1\\\", 0, \\\"DebugIdentity\\\")[0])\",\n        \"component_description\": \"Executes the main test case by listing local devices to find available GPUs, verifying that at least two are present. It then creates a TensorFlow session and distributes computations across the GPUs, initializes variables, sets up debugging options, runs the computation graph, and verifies the results against expected values using assertions.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_CALL\",\n        \"component_name\": \"googletest.main\",\n        \"component_code\": \"if __name__ == \\\"__main__\\\":\\n  googletest.main()\",\n        \"component_description\": \"Runs the test script using TensorFlow's `googletest` framework when the script is executed directly.\"\n      }\n    ],\n    \"overall_description\": \"This Python script defines a test class `SessionDebugMultiGPUTest` to verify that TensorFlow computations can be correctly distributed and debugged across multiple GPUs. The test sets up the environment, checks for the presence of at least two GPUs, initializes variables, distributes operations across the GPUs, and validates the computation results against expected values.\"\n  }\n}"
    },
    "python_10": {
        "input": "<reponame>EricRemmerswaal/tensorflow\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Test cases for complex numbers division.\"\"\"\n\nimport os\n\nimport numpy as np\n\nfrom tensorflow.compiler.tests import xla_test\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_math_ops\nfrom tensorflow.python.platform import googletest\n\nos.environ[\"XLA_FLAGS\"] = (\"--xla_cpu_fast_math_honor_nans=true \"\n                           \"--xla_cpu_fast_math_honor_infs=true\")\n\n\nclass ComplexNumbersDivisionTest(xla_test.XLATestCase):\n  \"\"\"Test cases for complex numbers division operators.\"\"\"\n\n  def _testBinary(self, op, a, b, expected, equality_test=None):\n    with self.session() as session:\n      with self.test_scope():\n        pa = array_ops.placeholder(dtypes.as_dtype(a.dtype), a.shape, name=\"a\")\n        pb = array_ops.placeholder(dtypes.as_dtype(b.dtype), b.shape, name=\"b\")\n        output = op(pa, pb)\n      result = session.run(output, {pa: a, pb: b})\n      if equality_test is None:\n        equality_test = self.assertAllCloseAccordingToType\n      equality_test(np.real(result), np.real(expected), rtol=1e-3)\n      equality_test(np.imag(result), np.imag(expected), rtol=1e-3)\n\n  def testComplexOps(self):\n    for dtype in self.complex_types:\n      # Test division by 0 scenarios.\n      self._testBinary(\n          gen_math_ops.real_div,\n          np.array([\n              complex(1, 1),\n              complex(1, np.inf),\n              complex(1, np.nan),\n              complex(np.inf, 1),\n              complex(np.inf, np.inf),\n              complex(np.inf, np.nan),\n              complex(np.nan, 1),\n              complex(np.nan, np.inf),\n              complex(np.nan, np.nan),\n              complex(-np.inf, np.nan),\n          ],\n                   dtype=dtype),\n          np.array([\n              0 + 0j,\n              0 + 0j,\n              0 + 0j,\n              0 + 0j,\n              0 + 0j,\n              0 + 0j,\n              0 + 0j,\n              0 + 0j,\n              0 + 0j,\n              0.0 + 0j,\n          ],\n                   dtype=dtype),\n          expected=np.array([\n              complex(np.inf, np.inf),\n              complex(np.inf, np.inf),\n              complex(np.inf, np.nan),\n              complex(np.inf, np.inf),\n              complex(np.inf, np.inf),\n              complex(np.inf, np.nan),\n              complex(np.nan, np.inf),\n              complex(np.nan, np.inf),\n              complex(np.nan, np.nan),\n              complex(-np.inf, np.nan),\n          ],\n                            dtype=dtype))\n\n      # Test division with finite numerator, inf/nan denominator.\n      self._testBinary(\n          gen_math_ops.real_div,\n          np.array([\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n          ],\n                   dtype=dtype),\n          np.array(\n              [\n                  complex(1, np.inf),\n                  complex(1, np.nan),\n                  complex(np.inf, 1),\n                  complex(np.inf, np.inf),  # C++ and Python diverge here.\n                  complex(np.inf, np.nan),  # C++ and Python diverge here.\n                  complex(np.nan, 1),\n                  complex(np.nan, np.inf),  # C++ and Python diverge here.\n                  complex(np.nan, -np.inf),  # C++ and Python diverge here.\n                  complex(np.nan, np.nan),\n              ],\n              dtype=dtype),\n          expected=np.array(\n              [\n                  (1 + 1j) / complex(1, np.inf),\n                  (1 + 1j) / complex(1, np.nan),\n                  (1 + 1j) / complex(np.inf, 1),\n                  complex(0 + 0j),  # C++ and Python diverge here.\n                  complex(0 + 0j),  # C++ and Python diverge here.\n                  (1 + 1j) / complex(np.nan, 1),\n                  complex(0 + 0j),  # C++ and Python diverge here.\n                  complex(0 - 0j),  # C++ and Python diverge here.\n                  (1 + 1j) / complex(np.nan, np.nan),\n              ],\n              dtype=dtype))\n\n      # Test division with inf/nan numerator, infinite denominator.\n      self._testBinary(\n          gen_math_ops.real_div,\n          np.array([\n              complex(1, np.inf),\n              complex(1, np.nan),\n              complex(np.inf, 1),\n              complex(np.inf, np.inf),\n              complex(np.inf, np.nan),\n              complex(np.nan, 1),\n              complex(np.nan, np.inf),\n              complex(np.nan, np.nan),\n              complex(np.nan, -np.inf),\n          ],\n                   dtype=dtype),\n          np.array([\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              1 + 1j,\n              -1 - 1j,\n          ],\n                   dtype=dtype),\n          expected=np.array(\n              [\n                  complex(np.inf, np.inf),  # C++ and Python diverge here.\n                  complex(1 / np.nan) / (1 + 1j),\n                  complex(np.inf / 1) / (1 + 1j),\n                  complex(np.inf, -np.nan),  # C++ and Python diverge here.\n                  complex(np.inf, -np.inf),  # C++ and Python diverge here.\n                  complex(np.nan / 1) / (1 + 1j),\n                  complex(np.inf, np.inf),  # C++ and Python diverge here.\n                  complex(np.nan / np.nan) / (1 + 1j),\n                  complex(np.inf, np.inf),  # C++ and Python diverge here.\n              ],\n              dtype=dtype))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import os\\nimport numpy as np\\nfrom tensorflow.compiler.tests import xla_test\\nfrom tensorflow.python.framework import dtypes\\nfrom tensorflow.python.ops import array_ops\\ngen_math_ops\\nfrom tensorflow.python.platform import googletest\",\n        \"component_description\": \"Imports necessary modules and libraries including the operating system module, NumPy for numerical operations, TensorFlow's XLA test framework, data types, array operations, math operations, and Google Test for running tests.\"\n      },\n      {\n        \"component_type\": \"STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"os\",\n        \"component_description\": \"Represents the imported operating system module which is used to set environment variables or interact with the file system if needed.\"\n      },\n      {\n        \"component_type\": \"STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"np\",\n        \"component_description\": \"An alias for NumPy, a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\"\n      },\n      {\n        \"component_type\": \"STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"xla_test\",\n        \"component_description\": \"A module from TensorFlow's compiler tests which is used for testing XLA (Accelerated Linear Algebra) operations.\"\n      },\n      {\n        \"component_type\": \"STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"dtypes\",\n        \"component_description\": \"Contains data type definitions that are used to represent the types of arrays and tensors in TensorFlow.\"\n      },\n      {\n        \"component_type\": \"STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"array_ops\",\n        \"component_description\": \"Provides a suite of operations for manipulating arrays and tensors, such as concatenation, slicing, and splitting.\"\n      },\n      {\n        \"component_type\": \"STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"gen_math_ops\",\n        \"component_description\": \"Generates math operations that are used in TensorFlow's computation graphs.\"\n      },\n      {\n        \"component_type\": \"STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"googletest\",\n        \"component_description\": \"A module from Google Test framework which provides utilities and functions for writing test cases, suites, and running tests in Python.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"ComplexDivisionTests\",\n        \"component_code\": \"class ComplexDivisionTests(xla_test.XLATest):\\n    ...\",\n        \"component_description\": \"Defines a class named 'ComplexDivisionTests' that inherits from the 'XLATest' class. This class contains test methods to verify complex number division operations in TensorFlow.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"_testBinary\",\n        \"component_code\": \"def _testBinary(self, op, x_data, y_data, expected):\\n    ...\",\n        \"component_description\": \"A private method within the 'ComplexDivisionTests' class used to perform binary operations (in this case, division) on complex numbers and compare the results with expected values.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testDivisionCases\",\n        \"component_code\": \"def testDivisionCases(self):\\n    ...\",\n        \"component_description\": \"A method within the 'ComplexDivisionTests' class that tests various division scenarios involving complex numbers, including cases with infinite and NaN values.\"\n      },\n      {\n        \"component_type\": \"STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if __name__ == \\\"__main__\\\":\\n    googletest.main()\",\n        \"component_description\": \"Checks if the script is being run directly (not imported) and then runs the tests using Google Test's main function.\"\n      }\n    ],\n    \"overall_description\": \"This Python script defines a set of unit tests for complex number division operations in TensorFlow. It uses the TensorFlow XLA test framework and NumPy for numerical operations. The tests cover various scenarios, including division by zero (represented by infinity or NaN) to ensure that the results are as expected.\"\n  }\n}"
    },
    "python_11": {
        "input": "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Logical boolean operators: not, and, or.\"\"\"\n\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_math_ops\n\n\ndef not_(a):\n  \"\"\"Functional form of \"not\".\"\"\"\n  if tensor_util.is_tf_type(a):\n    return _tf_not(a)\n  return _py_not(a)\n\n\ndef _tf_not(a):\n  \"\"\"Implementation of the \"not_\" operator for TensorFlow.\"\"\"\n  return gen_math_ops.logical_not(a)\n\n\ndef _py_not(a):\n  \"\"\"Default Python implementation of the \"not_\" operator.\"\"\"\n  return not a\n\n\ndef and_(a, b):\n  \"\"\"Functional form of \"and\". Uses lazy evaluation semantics.\"\"\"\n  a_val = a()\n  if tensor_util.is_tf_type(a_val):\n    return _tf_lazy_and(a_val, b)\n  return _py_lazy_and(a_val, b)\n\n\ndef _tf_lazy_and(cond, b):\n  \"\"\"Lazy-eval equivalent of \"and\" for Tensors.\"\"\"\n  # TODO(mdan): Enforce cond is scalar here?\n  return control_flow_ops.cond(cond, b, lambda: cond)\n\n\ndef _py_lazy_and(cond, b):\n  \"\"\"Lazy-eval equivalent of \"and\" in Python.\"\"\"\n  return cond and b()\n\n\ndef or_(a, b):\n  \"\"\"Functional form of \"or\". Uses lazy evaluation semantics.\"\"\"\n  a_val = a()\n  if tensor_util.is_tf_type(a_val):\n    return _tf_lazy_or(a_val, b)\n  return _py_lazy_or(a_val, b)\n\n\ndef _tf_lazy_or(cond, b):\n  \"\"\"Lazy-eval equivalent of \"or\" for Tensors.\"\"\"\n  # TODO(mdan): Enforce cond is scalar here?\n  return control_flow_ops.cond(cond, lambda: cond, b)\n\n\ndef _py_lazy_or(cond, b):\n  \"\"\"Lazy-eval equivalent of \"or\" in Python.\"\"\"\n  return cond or b()\n\n\ndef eq(a, b):\n  \"\"\"Functional form of \"equal\".\"\"\"\n  if tensor_util.is_tf_type(a) or tensor_util.is_tf_type(b):\n    return _tf_equal(a, b)\n  return _py_equal(a, b)\n\n\ndef _tf_equal(a, b):\n  \"\"\"Overload of \"equal\" for Tensors.\"\"\"\n  return gen_math_ops.equal(a, b)\n\n\ndef _py_equal(a, b):\n  \"\"\"Overload of \"equal\" that falls back to Python's default implementation.\"\"\"\n  return a == b\n\n\ndef not_eq(a, b):\n  \"\"\"Functional form of \"not-equal\".\"\"\"\n  return not_(eq(a, b))\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.framework import tensor_util\\nfrom tensorflow.python.ops import control_flow_ops\\nfrom tensorflow.python.ops import gen_math_ops\",\n        \"component_description\": \"Imports necessary modules from TensorFlow for handling tensors, control flow operations, and mathematical operations.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"not_\",\n        \"component_code\": \"def not_(a):\\n  \\\"\\\"\\\"Functional form of 'not'.\\\"\\\"\\\"\\n  if tensor_util.is_tf_type(a):\\n    return _tf_not(a)\\n  return _py_not(a)\",\n        \"component_description\": \"Provides a functional form of the logical NOT operation. It checks if the input is a TensorFlow type and delegates to either _tf_not or _py_not accordingly.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"_tf_not\",\n        \"component_code\": \"def _tf_not(a):\\n  \\\"\\\"\\\"Implementation of the 'not_' operator for TensorFlow.\\\"\\\"\\\"\\n  return gen_math_ops.logical_not(a)\",\n        \"component_description\": \"Implements the logical NOT operation for TensorFlow tensors using the logical_not method from TensorFlow's math operations.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"_py_not\",\n        \"component_code\": \"def _py_not(a):\\n  \\\"\\\"\\\"Default Python implementation of the 'not_' operator.\\\"\\\"\\\"\\n  return not a\",\n        \"component_description\": \"Provides the default Python implementation of the logical NOT operation using Python's built-in not operator.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"and_\",\n        \"component_code\": \"def and_(a, b):\\n  \\\"\\\"\\\"Functional form of 'and'. Uses lazy evaluation semantics.\\\"\\\"\\\"\\n  a_val = a()\\n  if tensor_util.is_tf_type(a_val):\\n    return _tf_lazy_and(a_val, b)\\n  return _py_lazy_and(a_val, b)\",\n        \"component_description\": \"Provides a functional form of the logical AND operation with lazy evaluation. It evaluates the first operand and delegates to either _tf_lazy_and or _py_lazy_and based on whether the result is a TensorFlow type.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"_tf_lazy_and\",\n        \"component_code\": \"def _tf_lazy_and(cond, b):\\n  \\\"\\\"\\\"Lazy-eval equivalent of 'and' for Tensors.\\\"\\\"\\\"\\n  # TODO(mdan): Enforce cond is scalar here?\\n  return control_flow_ops.cond(cond, b, lambda: cond)\",\n        \"component_description\": \"Implements the logical AND operation with lazy evaluation for TensorFlow tensors. It uses TensorFlow's control_flow_ops.cond to conditionally evaluate the second operand.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"_py_lazy_and\",\n        \"component_code\": \"def _py_lazy_and(cond, b):\\n  \\\"\\\"\\\"Lazy-eval equivalent of 'and' in Python.\\\"\\\"\\\"\\n  return cond and b()\",\n        \"component_description\": \"Provides the lazy evaluation equivalent of the logical AND operation in Python. It evaluates the first operand and conditionally evaluates the second operand using Python's built-in and operator.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"or_\",\n        \"component_code\": \"def or_(a, b):\\n  \\\"\\\"\\\"Functional form of 'or'. Uses lazy evaluation semantics.\\\"\\\"\\\"\\n  a_val = a()\\n  if tensor_util.is_tf_type(a_val):\\n    return _tf_lazy_or(a_val, b)\\n  return _py_lazy_or(a_val, b)\",\n        \"component_description\": \"Provides a functional form of the logical OR operation with lazy evaluation. It evaluates the first operand and delegates to either _tf_lazy_or or _py_lazy_or based on whether the result is a TensorFlow type.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"_tf_lazy_or\",\n        \"component_code\": \"def _tf_lazy_or(cond, b):\\n  \\\"\\\"\\\"Lazy-eval equivalent of 'or' for Tensors.\\\"\\\"\\\"\\n  # TODO(mdan): Enforce cond is scalar here?\\n  return control_flow_ops.cond(cond, lambda: cond, b)\",\n        \"component_description\": \"Implements the logical OR operation with lazy evaluation for TensorFlow tensors. It uses TensorFlow's control_flow_ops.cond to conditionally evaluate the second operand.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"_py_lazy_or\",\n        \"component_code\": \"def _py_lazy_or(cond, b):\\n  \\\"\\\"\\\"Lazy-eval equivalent of 'or' in Python.\\\"\\\"\\\"\\n  return cond or b()\",\n        \"component_description\": \"Provides the lazy evaluation equivalent of the logical OR operation in Python. It evaluates the first operand and conditionally evaluates the second operand using Python's built-in or operator.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"eq\",\n        \"component_code\": \"def eq(a, b):\\n  \\\"\\\"\\\"Functional form of 'equal'.\\\"\\\"\\\"\\n  return not_(not_eq(a, b))\",\n        \"component_description\": \"Provides a functional form of the equality operation by negating the result of the not-equal operation.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"not_eq\",\n        \"component_code\": \"def not_eq(a, b):\\n  \\\"\\\"\\\"Functional form of 'not-equal'.\\\"\\\"\\\"\\n  return not_(eq(a, b))\",\n        \"component_description\": \"Provides a functional form of the not-equal operation by negating the result of the equality operation.\"\n      }\n    ],\n    \"overall_description\": \"This code defines functional forms of logical operations (NOT, AND, OR) and comparison operations (equal, not-equal) that can handle both TensorFlow tensors and Python values. It ensures lazy evaluation for AND and OR operations and uses appropriate methods from TensorFlow when dealing with tensors.\"\n  }\n}"
    },
    "python_12": {
        "input": "<gh_stars>1000+\n\"\"\"BUILD rules for generating saved model for testing.\"\"\"\n\nload(\"//tensorflow:tensorflow.bzl\", \"if_google\")\n\ndef gen_saved_model(model_name = \"\", script = \"\", **kwargs):\n    native.genrule(\n        name = \"saved_model_gen_\" + model_name,\n        srcs = [],\n        outs = [\n            model_name + \"/saved_model.pb\",\n            model_name + \"/variables/variables.data-00000-of-00001\",\n            model_name + \"/variables/variables.index\",\n        ],\n        cmd = if_google(\n            \"$(location \" + script + \") --saved_model_path=$(RULEDIR)/\" + model_name,\n            \"touch $(OUTS)\",  # TODO(b/188517768): fix model gen.\n        ),\n        exec_tools = [script],\n        **kwargs\n    )\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"MODULE_DEFINITION\",\n        \"component_name\": NULL,\n        \"component_code\": \"\"\"\\\"\\\"\\\"BUILD rules for generating saved model for testing.\\\"\\\"\\\"\n\nload(\"//tensorflow:tensorflow.bzl\", \"if_google\")\"\"\",\n        \"component_description\": \"This is the module definition part of the script. It includes a docstring describing the purpose of the file and loads the 'if_google' function from the tensorflow.bzl module.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"gen_saved_model\",\n        \"component_code\": \"\"\"def gen_saved_model(model_name = \"\", script = \"\", **kwargs):\n    native.genrule(\n        name = \"saved_model_gen_\" + model_name,\n        srcs = [],\n        outs = [\n            model_name + \"/saved_model.pb\",\n            model_name + \"/variables/variables.data-00000-of-00001\",\n            model_name + \"/variables/variables.index\",\n        ],\n        cmd = if_google(\n            \"$(location \" + script + \") --saved_model_path=$(RULEDIR)/\" + model_name,\n            \"touch $(OUTS)\",  # TODO(b/188517768): fix model gen.\n        ),\n        exec_tools = [script],\n        **kwargs\n    )\"\"\",\n        \"component_description\": \"This function defines a custom build rule named 'gen_saved_model'. It takes optional parameters 'model_name' and 'script', and additional keyword arguments. It creates a genrule using Bazel's native.genrule function to generate files for a saved model. The output files include 'saved_model.pb', 'variables.data-00000-of-00001', and 'variables.index' in the specified model directory. The command executed depends on whether the environment is Google or not, using the if_google function. If it's Google, it runs a script with the given path and arguments; otherwise, it touches the output files (which should be fixed as per TODO).\"\n      }\n    ],\n    \"overall_description\": \"The provided Python code defines a custom Bazel build rule for generating TensorFlow saved models. The main component is the 'gen_saved_model' function, which configures a genrule to produce necessary model files. It uses conditional logic based on the environment (Google or not) to determine the command executed during the build process. This setup is useful in automated testing and deployment pipelines where pre-built models are required.\"\n  }\n}"
    },
    "python_13": {
        "input": "<gh_stars>1000+\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.ops.math_ops.linspace.\"\"\"\n\n# Using distutils.version.LooseVersion was resulting in an error, so importing\n# directly.\nfrom distutils.version import LooseVersion  # pylint: disable=g-importing-member\n\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import googletest\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass LinspaceTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.parameters([\n      {\n          \"start_shape\": start_shape,\n          \"stop_shape\": stop_shape,\n          \"dtype\": dtype,\n          \"num\": num\n      }\n      for start_shape in [(), (2,), (2, 2)]\n      for stop_shape in [(), (2,), (2, 2)]\n      for dtype in [np.float64, np.int64]\n      for num in [0, 1, 2, 20]\n  ])\n  # pylint: enable=g-complex-comprehension\n  def testLinspaceBroadcasts(self, start_shape, stop_shape, dtype, num):\n    if LooseVersion(np.version.version) < LooseVersion(\"1.16.0\"):\n      self.skipTest(\"numpy doesn't support axes before version 1.16.0\")\n\n      ndims = max(len(start_shape), len(stop_shape))\n      for axis in range(-ndims, ndims):\n        start = np.ones(start_shape, dtype)\n        stop = 10 * np.ones(stop_shape, dtype)\n\n        np_ans = np.linspace(start, stop, num, axis=axis)\n        tf_ans = self.evaluate(\n            math_ops.linspace_nd(start, stop, num, axis=axis))\n\n        self.assertAllClose(np_ans, tf_ans)\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from distutils.version import LooseVersion  # pylint: disable=g-importing-member\",\n        \"component_description\": \"Imports the LooseVersion class from the distutils.version module to compare numpy versions.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from absl.testing import parameterized\",\n        \"component_description\": \"Imports the parameterized module from absl.testing for test parameterization.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import numpy as np\",\n        \"component_description\": \"Imports the numpy library and assigns it the alias 'np' for numerical operations.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.framework import test_util\",\n        \"component_description\": \"Imports the test_util module from TensorFlow's framework to provide testing utilities.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.ops import math_ops\",\n        \"component_description\": \"Imports the math_ops module from TensorFlow's operations to use mathematical functions.\"\n      },\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.platform import googletest\",\n        \"component_description\": \"Imports the googletest module from TensorFlow's platform for running tests using Google Test framework.\"\n      },\n      {\n        \"component_type\": \"DECORATOR\",\n        \"component_name\": \"@test_util.run_all_in_graph_and_eager_modes\",\n        \"component_code\": \"@test_util.run_all_in_graph_and_eager_modes\\nclass LinspaceTest(test_util.TensorFlowTestCase, parameterized.TestCase):\",\n        \"component_description\": \"Decorator that ensures the test class methods run in both graph and eager execution modes.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"LinspaceTest\",\n        \"component_code\": \"class LinspaceTest(test_util.TensorFlowTestCase, parameterized.TestCase):\",\n        \"component_description\": \"Defines a test class named LinspaceTest that inherits from TensorFlowTestCase and TestCase for parameterized testing.\"\n      },\n      {\n        \"component_type\": \"DECORATOR\",\n        \"component_name\": \"@parameterized.parameters\",\n        \"component_code\": \"@parameterized.parameters([\\n    {\\n        \\\"start_shape\\\": start_shape,\\n        \\\"stop_shape\\\": stop_shape,\\n        \\\"dtype\\\": dtype,\\n        \\\"num\\\": num\\n    }\\n    for start_shape in [(), (2,), (2, 2)]\\n    for stop_shape in [(), (2,), (2, 2)]\\n    for dtype in [np.float64, np.int64]\\n    for num in [0, 1, 2, 20]\\n])\",\n        \"component_description\": \"Decorator that parameterizes the testLinspaceBroadcasts method with various combinations of start_shape, stop_shape, dtype, and num.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testLinspaceBroadcasts\",\n        \"component_code\": \"def testLinspaceBroadcasts(self, start_shape, stop_shape, dtype, num):\",\n        \"component_description\": \"Defines a method named testLinspaceBroadcasts to test the linspace function with different parameters. It checks if numpy and TensorFlow's linspace functions produce the same results.\"\n      },\n      {\n        \"component_type\": \"IF_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if LooseVersion(np.version.version) < LooseVersion(\\\"1.16.0\\\"):\",\n        \"component_description\": \"Checks if the numpy version is less than 1.16.0 and skips the test if true, as older versions of numpy do not support certain features used in the test.\"\n      },\n      {\n        \"component_type\": \"FOR_LOOP\",\n        \"component_name\": null,\n        \"component_code\": \"for axis in range(-ndims, ndims):\",\n        \"component_description\": \"Loops over a range from negative to positive ndims (number of dimensions) to test linspace with different axes.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"start\",\n        \"component_code\": \"start = np.ones(start_shape, dtype)\",\n        \"component_description\": \"Declares a variable named start and initializes it with an array of ones having the shape specified by start_shape and the data type specified by dtype.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"stop\",\n        \"component_code\": \"stop = 10 * np.ones(stop_shape, dtype)\",\n        \"component_description\": \"Declares a variable named stop and initializes it with an array of tens (multiplied by 10) having the shape specified by stop_shape and the data type specified by dtype.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"ndims\",\n        \"component_code\": \"ndims = max(len(start_shape), len(stop_shape))\",\n        \"component_description\": \"Declares a variable named ndims and initializes it with the maximum length of start_shape and stop_shape to determine the number of dimensions.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"np_result\",\n        \"component_code\": \"np_result = np.linspace(start, stop, num, axis=axis)\",\n        \"component_description\": \"Declares a variable named np_result and initializes it with the result of numpy's linspace function applied to start, stop, num, and axis.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"tf_result\",\n        \"component_code\": \"tf_result = math_ops.linspace(start, stop, num).numpy()\",\n        \"component_description\": \"Declares a variable named tf_result and initializes it with the result of TensorFlow's linspace function applied to start, stop, and num. The .numpy() method is used to convert the result to a numpy array for comparison.\"\n      },\n      {\n        \"component_type\": \"ASSERTION\",\n        \"component_name\": null,\n        \"component_code\": \"self.assertAllClose(np_result, tf_result)\",\n        \"component_description\": \"Asserts that the numpy and TensorFlow results are close within a tolerance level, indicating that the linspace functions produce similar results.\"\n      },\n      {\n        \"component_type\": \"ENTRY_POINT\",\n        \"component_name\": null,\n        \"component_code\": \"if __name__ == '__main__':\\n    googletest.main()\",\n        \"component_description\": \"Entry point of the script. It checks if the script is run directly and not imported as a module, then runs the tests using Google Test framework.\"\n      }\n    ],\n    \"overall_description\": \"The provided code defines a test class LinspaceTest that inherits from TensorFlowTestCase and TestCase for parameterized testing. The class contains one method named testLinspaceBroadcasts which tests the linspace function with various combinations of start_shape, stop_shape, dtype, and num. It checks if numpy and TensorFlow's linspace functions produce the same results by comparing their outputs using the assertAllClose method. The script also ensures that the tests run in both graph and eager execution modes.\"\n  }\n}"
    },
    "python_14": {
        "input": "<filename>tensorflow/python/util/fast_module_type_test.py<gh_stars>1000+\n# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tensorflow.python.util.fast_module_type.\"\"\"\n\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.util import fast_module_type\nFastModuleType = fast_module_type.get_fast_module_type_class()\n\n\nclass ChildFastModule(FastModuleType):\n\n  def _getattribute1(self, name):  # pylint: disable=unused-argument\n    return 2\n\n  def _getattribute2(self, name):  # pylint: disable=unused-argument\n    raise AttributeError(\"Pass to getattr\")\n\n  def _getattr(self, name):  # pylint: disable=unused-argument\n    return 3\n\n\nclass FastModuleTypeTest(test.TestCase):\n\n  def testBaseGetattribute(self):\n    # Tests that the default attribute lookup works.\n    module = ChildFastModule(\"test\")\n    module.foo = 1\n    self.assertEqual(1, module.foo)\n\n  def testGetattributeCallback(self):\n    # Tests that functionality of __getattribute__ can be set as a callback.\n    module = ChildFastModule(\"test\")\n    FastModuleType.set_getattribute_callback(module,\n                                             ChildFastModule._getattribute1)\n    self.assertEqual(2, module.foo)\n\n  def testGetattrCallback(self):\n    # Tests that functionality of __getattr__ can be set as a callback.\n    module = ChildFastModule(\"test\")\n    FastModuleType.set_getattribute_callback(module,\n                                             ChildFastModule._getattribute2)\n    FastModuleType.set_getattr_callback(module, ChildFastModule._getattr)\n    self.assertEqual(3, module.foo)\n\n  def testFastdictApis(self):\n    module = ChildFastModule(\"test\")\n    # At first \"bar\" does not exist in the module's attributes\n    self.assertFalse(module._fastdict_key_in(\"bar\"))\n    with self.assertRaisesRegex(KeyError, \"module has no attribute 'bar'\"):\n      module._fastdict_get(\"bar\")\n\n    module._fastdict_insert(\"bar\", 1)\n    # After _fastdict_insert() the attribute is added.\n    self.assertTrue(module._fastdict_key_in(\"bar\"))\n    self.assertEqual(1, module.bar)\n\n\nif __name__ == \"__main__\":\n  test.main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"from tensorflow.python.platform import test\\nfrom tensorflow.python.util import fast_module_type\",\n        \"component_description\": \"Imports the 'test' module from TensorFlow's platform package and the 'fast_module_type' module from TensorFlow's util package. These modules are necessary for defining test cases and utilizing a specialized module type, respectively.\"\n      },\n      {\n        \"component_type\": \"VARIABLE_DECLARATION\",\n        \"component_name\": \"FastModuleType\",\n        \"component_code\": \"FastModuleType = fast_module_type.get_fast_module_type_class()\",\n        \"component_description\": \"Declares a variable 'FastModuleType' and assigns it the class returned by the function get_fast_module_type_class() from the imported module fast_module_type. This class is used to create modules with specialized attribute handling.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"ChildFastModule\",\n        \"component_code\": \"class ChildFastModule(FastModuleType):\\n\\n  def _getattribute1(self, name):  # pylint: disable=unused-argument\\n    return 2\\n\\n  def _getattribute2(self, name):  # pylint: disable=unused-argument\\n    raise AttributeError(\\\"Pass to getattr\\\")\\n\\n  def _getattr(self, name):  # pylint: disable=unused-argument\\n    return 3\",\n        \"component_description\": \"Defines a class 'ChildFastModule' that inherits from 'FastModuleType'. It overrides three methods: _getattribute1 which always returns 2, _getattribute2 which raises an AttributeError to pass control to __getattr__, and _getattr which always returns 3. These methods are intended to be used as callbacks for attribute access.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"_getattribute1\",\n        \"component_code\": \"def _getattribute1(self, name):  # pylint: disable=unused-argument\\n    return 2\",\n        \"component_description\": \"Method within 'ChildFastModule' that takes an attribute name as input and always returns the integer 2. The argument 'name' is not used in this method.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"_getattribute2\",\n        \"component_code\": \"def _getattribute2(self, name):  # pylint: disable=unused-argument\\n    raise AttributeError(\\\"Pass to getattr\\\")\",\n        \"component_description\": \"Method within 'ChildFastModule' that takes an attribute name as input and raises an AttributeError with the message 'Pass to getattr'. The argument 'name' is not used in this method.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"_getattr\",\n        \"component_code\": \"def _getattr(self, name):  # pylint: disable=unused-argument\\n    return 3\",\n        \"component_description\": \"Method within 'ChildFastModule' that takes an attribute name as input and always returns the integer 3. The argument 'name' is not used in this method.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"FastModuleTypeTest\",\n        \"component_code\": \"class FastModuleTypeTest(test.TestCase):\\n\\n  def testBaseGetattribute(self):\\n    # Tests that the default attribute lookup works.\\n    module = ChildFastModule(\\\"test\\\")\\n    module.foo = 1\\n    self.assertEqual(1, module.foo)\\n\\n  def testGetattributeCallback(self):\\n    # Tests that functionality of __getattribute__ can be set as a callback.\\n    module = ChildFastModule(\\\"test\\\")\\n    FastModuleType.set_getattribute_callback(module,\\n                                             ChildFastModule._getattribute1)\\n    self.assertEqual(2, module.foo)\\n\\n  def testGetattrCallback(self):\\n    # Tests that functionality of __getattr__ can be set as a callback.\\n    module = ChildFastModule(\\\"test\\\")\\n    FastModuleType.set_getattribute_callback(module,\\n                                             ChildFastModule._getattribute2)\\n    FastModuleType.set_getattr_callback(module, ChildFastModule._getattr)\\n    self.assertEqual(3, module.foo)\\n\\n  def testFastdictApis(self):\\n    module = ChildFastModule(\\\"test\\\")\\n    # At first \\\"bar\\\" does not exist in the module's attributes\\n    self.assertFalse(module._fastdict_key_in(\\\"bar\\\"))\\n    with self.assertRaisesRegex(KeyError, \\\"module has no attribute 'bar'\\\"):\\n      module._fastdict_get(\\\"bar\\\")\\n\\n    module._fastdict_insert(\\\"bar\\\", 1)\\n    # After _fastdict_insert() the attribute is added.\\n    self.assertTrue(module._fastdict_key_in(\\\"bar\\\"))\\n    self.assertEqual(1, module.bar)\",\n        \"component_description\": \"Defines a test class 'FastModuleTypeTest' that inherits from 'test.TestCase'. It includes four test methods to verify the behavior of the 'ChildFastModule' class and its interaction with the specialized attribute handling provided by 'FastModuleType'.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testBaseGetattribute\",\n        \"component_code\": \"def testBaseGetattribute(self):\\n    # Tests that the default attribute lookup works.\\n    module = ChildFastModule(\\\"test\\\")\\n    module.foo = 1\\n    self.assertEqual(1, module.foo)\",\n        \"component_description\": \"Test method within 'FastModuleTypeTest' that checks if the basic attribute assignment and retrieval work as expected in a 'ChildFastModule' instance.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testGetattributeCallback\",\n        \"component_code\": \"def testGetattributeCallback(self):\\n    # Tests that functionality of __getattribute__ can be set as a callback.\\n    module = ChildFastModule(\\\"test\\\")\\n    FastModuleType.set_getattribute_callback(module,\\n                                             ChildFastModule._getattribute1)\\n    self.assertEqual(2, module.foo)\",\n        \"component_description\": \"Test method within 'FastModuleTypeTest' that verifies setting the _getattribute1 callback results in all attribute accesses returning 2.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testGetattrCallback\",\n        \"component_code\": \"def testGetattrCallback(self):\\n    # Tests that functionality of __getattr__ can be set as a callback.\\n    module = ChildFastModule(\\\"test\\\")\\n    FastModuleType.set_getattribute_callback(module,\\n                                             ChildFastModule._getattribute2)\\n    FastModuleType.set_getattr_callback(module, ChildFastModule._getattr)\\n    self.assertEqual(3, module.foo)\",\n        \"component_description\": \"Test method within 'FastModuleTypeTest' that verifies setting the _getattribute2 and _getattr callbacks results in all attribute accesses returning 3 after _getattribute2 raises an AttributeError.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testFastdictApis\",\n        \"component_code\": \"def testFastdictApis(self):\\n    module = ChildFastModule(\\\"test\\\")\\n    # At first \\\"bar\\\" does not exist in the module's attributes\\n    self.assertFalse(module._fastdict_key_in(\\\"bar\\\"))\\n    with self.assertRaisesRegex(KeyError, \\\"module has no attribute 'bar'\\\"):\\n      module.bar\\n\\n    module._fastdict_insert(\\\"bar\\\", 1)\\n    # After _fastdict_insert() the attribute is added.\\n    self.assertTrue(module._fastdict_key_in(\\\"bar\\\"))\\n    self.assertEqual(1, module.bar)\",\n        \"component_description\": \"Test method within 'FastModuleTypeTest' that checks the functionality of fast dictionary operations such as key existence check (_fastdict_key_in), insertion (_fastdict_insert) and attribute access after insertion.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_CALL\",\n        \"component_name\": null,\n        \"component_code\": \"if __name__ == '__main__':\\n    test.main()\",\n        \"component_description\": \"Checks if the script is run as the main program and not imported as a module in another script. If true, it calls the 'test.main()' function to execute all defined tests within 'FastModuleTypeTest'.\"\n      }\n    ],\n    \"overall_description\": \"The provided code defines a specialized test class for verifying the behavior of a custom module type ('ChildFastModule') that inherits from 'FastModuleType', which is obtained from TensorFlow's utility modules. The 'ChildFastModule' overrides methods intended to customize attribute access behavior. The test class, 'FastModuleTypeTest', includes four test methods to ensure correct implementation and interaction between the overridden methods and expected behaviors of attributes in instances of 'ChildFastModule'.\"\n  }\n}"
    },
    "python_15": {
        "input": "\"\"\"A simple portable implementation of build_test.\"\"\"\n\ndef build_test(name, targets, visibility = None):\n    \"\"\"Generates a test that just verifies that the specified targets can be built.\"\"\"\n\n    # Generate an sh_test rule that lists the specified targets as data,\n    # (thus forcing those targets to be built before the test can be run)\n    # and that runs a script which always succeeds.\n    native.sh_test(\n        name = name,\n        srcs = [name + \".sh\"],\n        data = targets,\n        visibility = visibility,\n    )\n\n    # Generate the script which always succeeds.  We just generate an empty script.\n    native.genrule(\n        name = name + \"_gen_sh\",\n        outs = [name + \".sh\"],\n        cmd = \"> $@\",\n        visibility = [\"//visibility:private\"],\n    )\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"build_test\",\n        \"component_code\": \"def build_test(name, targets, visibility = None):\\n    \\\"\\\"\\\"Generates a test that just verifies that the specified targets can be built.\\\"\\\"\\\"\\n\\n    # Generate an sh_test rule that lists the specified targets as data,\\n    # (thus forcing those targets to be built before the test can be run)\\n    # and that runs a script which always succeeds.\\n    native.sh_test(\\n        name = name,\\n        srcs = [name + \\\".sh\\\"],\\n        data = targets,\\n        visibility = visibility,\\n    )\\n\\n    # Generate the script which always succeeds.  We just generate an empty script.\\n    native.genrule(\\n        name = name + \\\"_gen_sh\\\",\\n        outs = [name + \\\".sh\\\"],\\n        cmd = \\\"> $@\\\",\\n        visibility = [\\\"//visibility:private\\\"],\\n    )\",\n        \"component_description\": \"This function generates a test using Bazel's sh_test rule to verify that specified targets can be built. It creates an empty shell script that always succeeds, ensuring all dependencies (targets) are built before the test runs.\"\n      },\n      {\n        \"component_type\": \"NATIVE_FUNCTION_CALL\",\n        \"component_name\": \"sh_test\",\n        \"component_code\": \"native.sh_test(\\n    name = name,\\n    srcs = [name + \\\".sh\\\"],\\n    data = targets,\\n    visibility = visibility,\\n)\",\n        \"component_description\": \"This call to native.sh_test defines a shell test rule in Bazel. It specifies the test's name, source files (the generated script), data dependencies (targets that must be built before the test runs), and visibility settings.\"\n      },\n      {\n        \"component_type\": \"NATIVE_FUNCTION_CALL\",\n        \"component_name\": \"genrule\",\n        \"component_code\": \"native.genrule(\\n    name = name + \\\"_gen_sh\\\",\\n    outs = [name + \\\".sh\\\"],\\n    cmd = \\\"> $@\\\",\\n    visibility = [\\\"//visibility:private\\\"],\\n)\",\n        \"component_description\": \"This call to native.genrule generates an empty shell script file that is used by the sh_test rule. The command '> $@' creates an empty file, and the visibility is set to private to restrict access.\"\n      }\n    ],\n    \"overall_description\": \"The provided Python code defines a function `build_test` intended for use in Bazel build scripts. It generates a shell test (sh_test) that ensures specified targets are built successfully before running. The script associated with the test is generated using genrule, creating an empty file that always succeeds when executed.\"\n  }\n}"
    },
    "python_17": {
        "input": "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Test configs for is_finite.\"\"\"\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.lite.testing.zip_test_utils import create_tensor_data\nfrom tensorflow.lite.testing.zip_test_utils import make_zip_of_tests\nfrom tensorflow.lite.testing.zip_test_utils import register_make_test_function\n\n\n@register_make_test_function()\ndef make_is_finite_tests(options):\n  \"\"\"Make a set of tests to do is_finite.\"\"\"\n\n  test_parameters = [\n      {\n          \"input_shape\": [[100], [3, 15, 14, 3]],\n      },\n  ]\n\n  def build_graph(parameters):\n    \"\"\"Build the graph for the test case.\"\"\"\n\n    input_tensor = tf.compat.v1.placeholder(\n        dtype=tf.float32, name=\"input\", shape=parameters[\"input_shape\"])\n    out = tf.math.is_finite(input_tensor)\n    return [input_tensor], [out]\n\n  def build_inputs(parameters, sess, inputs, outputs):\n    \"\"\"Build the inputs for the test case.\"\"\"\n    input_values = create_tensor_data(\n        np.float32, parameters[\"input_shape\"], min_value=-10, max_value=10)\n\n    # Inject NaN and Inf value.\n    def random_index(shape):\n      result = []\n      for dim in shape:\n        result.append(np.random.randint(low=0, high=dim))\n      return tuple(result)\n\n    input_values[random_index(input_values.shape)] = np.Inf\n    input_values[random_index(input_values.shape)] = -np.Inf\n    input_values[random_index(input_values.shape)] = np.NAN\n    input_values[random_index(input_values.shape)] = tf.float32.max\n    input_values[random_index(input_values.shape)] = tf.float32.min\n\n    return [input_values], sess.run(\n        outputs, feed_dict=dict(zip(inputs, [input_values])))\n\n  make_zip_of_tests(options, test_parameters, build_graph, build_inputs)\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import numpy as np\\nimport tensorflow.compat.v1 as tf\\nfrom tensorflow.lite.testing.zip_test_utils import create_tensor_data\\nfrom tensorflow.lite.testing.zip_test_utils import make_zip_of_tests\\nfrom tensorflow.lite.testing.zip_test_utils import register_make_test_function\",\n        \"component_description\": \"Imports necessary modules and functions: numpy for numerical operations, TensorFlow v1 compatibility module for tensor operations, and specific utility functions from the TensorFlow Lite testing suite.\"\n      },\n      {\n        \"component_type\": \"DECORATOR\",\n        \"component_name\": \"@register_make_test_function()\",\n        \"component_code\": \"@register_make_test_function()\\ndef make_is_finite_tests(options):\",\n        \"component_description\": \"Registers the function 'make_is_finite_tests' to be used as a test case generator for TensorFlow Lite tests.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"make_is_finite_tests\",\n        \"component_code\": \"def make_is_finite_tests(options):\\n    \\\"\\\"\\\"Make a set of tests to do is_finite.\\\"\\\"\\\"\\n\\n    test_parameters = [\\n        {\\n            \\\"input_shape\\\": [[100], [3, 15, 14, 3]],\\n        },\\n    ]\\n\\n    def build_graph(parameters):\\n        \\\"\\\"\\\"Build the graph for the test case.\\\"\\\"\\\"\\n\\n        input_tensor = tf.compat.v1.placeholder(\\n            dtype=tf.float32, name=\\\"input\\\", shape=parameters[\\\"input_shape\\\"])\\n        out = tf.math.is_finite(input_tensor)\\n        return [input_tensor], [out]\\n\\n    def build_inputs(parameters, sess, inputs, outputs):\\n        \\\"\\\"\\\"Build the inputs for the test case.\\\"\\\"\\\"\\n        input_values = create_tensor_data(\\n            np.float32, parameters[\\\"input_shape\\\"], min_value=-10, max_value=10)\\n\\n        # Inject NaN and Inf value.\\n        def random_index(shape):\\n            result = []\\n            for dim in shape:\\n                result.append(np.random.randint(low=0, high=dim))\\n            return tuple(result)\\n\\n        input_values[random_index(input_values.shape)] = np.Inf\\n        input_values[random_index(input_values.shape)] = -np.Inf\\n        input_values[random_index(input_values.shape)] = np.NAN\\n        input_values[random_index(input_values.shape)] = tf.float32.max\\n        input_values[random_index(input_values.shape)] = tf.float32.min\\n\\n        return [input_values], sess.run(\\n            outputs, feed_dict=dict(zip(inputs, [input_values])))\\n\\n    make_zip_of_tests(options, test_parameters, build_graph, build_inputs)\",\n        \"component_description\": \"Defines a function to create tests for the TensorFlow Lite 'is_finite' operation. It sets up test parameters, builds a graph with a placeholder and is_finite operation, constructs inputs by injecting special values like NaN and Inf, and then creates test files using make_zip_of_tests.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"build_graph\",\n        \"component_code\": \"def build_graph(parameters):\\n    \\\"\\\"\\\"Build the graph for the test case.\\\"\\\"\\\"\\n\\n    input_tensor = tf.compat.v1.placeholder(\\n        dtype=tf.float32, name=\\\"input\\\", shape=parameters[\\\"input_shape\\\"])\\n    out = tf.math.is_finite(input_tensor)\\n    return [input_tensor], [out]\",\n        \"component_description\": \"Builds a TensorFlow graph for the test case. It creates an input placeholder and applies the is_finite operation to it, returning both the input tensor and the output tensor.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"build_inputs\",\n        \"component_code\": \"def build_inputs(parameters, sess, inputs, outputs):\\n    \\\"\\\"\\\"Build the inputs for the test case.\\\"\\\"\\\"\\n    input_values = create_tensor_data(\\n        np.float32, parameters[\\\"input_shape\\\"], min_value=-10, max_value=10)\\n\\n    # Inject NaN and Inf value.\\n    def random_index(shape):\\n        result = []\\n        for dim in shape:\\n            result.append(np.random.randint(low=0, high=dim))\\n        return tuple(result)\\n\\n    input_values[random_index(input_values.shape)] = np.Inf\\n    input_values[random_index(input_values.shape)] = -np.Inf\\n    input_values[random_index(input_values.shape)] = np.NAN\\n    input_values[random_index(input_values.shape)] = tf.float32.max\\n    input_values[random_index(input_values.shape)] = tf.float32.min\\n\\n    return [input_values], sess.run(\\n        outputs, feed_dict=dict(zip(inputs, [input_values])))\",\n        \"component_description\": \"Builds the inputs for the test case by generating random data within specified bounds and injecting special values like NaN, Inf, max float, and min float at random positions. It then runs the session with these inputs to get the output.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"random_index\",\n        \"component_code\": \"def random_index(shape):\\n    result = []\\n    for dim in shape:\\n        result.append(np.random.randint(low=0, high=dim))\\n    return tuple(result)\",\n        \"component_description\": \"Generates a random index within the given shape dimensions. Used to determine where special values like NaN and Inf should be injected into the input data.\"\n      }\n    ],\n    \"overall_description\": \"This code defines a test case generator for TensorFlow Lite's 'is_finite' operation. It sets up various test configurations, builds corresponding TensorFlow graphs, constructs inputs with special numerical values, and then generates test files for validation using make_zip_of_tests.\"\n  }\n}"
    },
    "python_19": {
        "input": "<reponame>EricRemmerswaal/tensorflow\n# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"This is a Python API fuzzer for tf.raw_ops.SparseCountSparseOutput.\"\"\"\nimport atheris\nwith atheris.instrument_imports():\n  import sys\n  from python_fuzzing import FuzzingHelper\n  import tensorflow as tf\n\n\n@atheris.instrument_func\ndef TestOneInput(input_bytes):\n  \"\"\"Test randomized integer fuzzing input for tf.raw_ops.SparseCountSparseOutput.\"\"\"\n  fh = FuzzingHelper(input_bytes)\n\n  shape1 = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\n  shape2 = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\n  shape3 = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\n  shape4 = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\n\n  seed = fh.get_int()\n  indices = tf.random.uniform(\n      shape=shape1, minval=0, maxval=1000, dtype=tf.int64, seed=seed)\n  values = tf.random.uniform(\n      shape=shape2, minval=0, maxval=1000, dtype=tf.int64, seed=seed)\n  dense_shape = tf.random.uniform(\n      shape=shape3, minval=0, maxval=1000, dtype=tf.int64, seed=seed)\n  weights = tf.random.uniform(\n      shape=shape4, minval=0, maxval=1000, dtype=tf.int64, seed=seed)\n\n  binary_output = fh.get_bool()\n  minlength = fh.get_int()\n  maxlength = fh.get_int()\n  name = fh.get_string()\n  try:\n    _, _, _, = tf.raw_ops.SparseCountSparseOutput(\n        indices=indices,\n        values=values,\n        dense_shape=dense_shape,\n        weights=weights,\n        binary_output=binary_output,\n        minlength=minlength,\n        maxlength=maxlength,\n        name=name)\n  except tf.errors.InvalidArgumentError:\n    pass\n\n\ndef main():\n  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\n  atheris.Fuzz()\n\n\nif __name__ == \"__main__\":\n  main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import atheris\\nwith atheris.instrument_imports():\\n  import sys\\n  from python_fuzzing import FuzzingHelper\\n  import tensorflow as tf\",\n        \"component_description\": \"Imports necessary modules for the script, including TensorFlow and Atheris for fuzzing. The 'with atheris.instrument_imports()' context manager ensures that imports are instrumented for fuzzing.\"\n      },\n      {\n        \"component_type\": \"DECORATOR\",\n        \"component_name\": null,\n        \"component_code\": \"@atheris.instrument_func\",\n        \"component_description\": \"Decorator used to instrument the following function 'TestOneInput' for fuzzing, allowing Atheris to generate inputs and trace execution.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"TestOneInput\",\n        \"component_code\": \"def TestOneInput(input_bytes):\\n  \\\"\\\"\\\"Test randomized integer fuzzing input for tf.raw_ops.SparseCountSparseOutput.\\\"\\\"\\\"\\n  fh = FuzzingHelper(input_bytes)\\n\\n  shape1 = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\\n  shape2 = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\\n  shape3 = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\\n  shape4 = fh.get_int_list(min_length=0, max_length=8, min_int=0, max_int=8)\\n\\n  seed = fh.get_int()\\n  indices = tf.random.uniform(\\n      shape=shape1, minval=0, maxval=1000, dtype=tf.int64, seed=seed)\\n  values = tf.random.uniform(\\n      shape=shape2, minval=0, maxval=1000, dtype=tf.int64, seed=seed)\\n  dense_shape = tf.random.uniform(\\n      shape=shape3, minval=0, maxval=1000, dtype=tf.int64, seed=seed)\\n  weights = tf.random.uniform(\\n      shape=shape4, minval=0, maxval=1000, dtype=tf.int64, seed=seed)\\n\\n  binary_output = fh.get_bool()\\n  minlength = fh.get_int()\\n  maxlength = fh.get_int()\\n  name = fh.get_string()\\n  try:\\n    _, _, _, = tf.raw_ops.SparseCountSparseOutput(\\n        indices=indices,\\n        values=values,\\n        dense_shape=dense_shape,\\n        weights=weights,\\n        binary_output=binary_output,\\n        minlength=minlength,\\n        maxlength=maxlength,\\n        name=name)\\n  except tf.errors.InvalidArgumentError:\\n    pass\",\n        \"component_description\": \"Function that tests randomized input for the TensorFlow operation `tf.raw_ops.SparseCountSparseOutput`. It uses `FuzzingHelper` to generate random shapes, seed, indices, values, dense shape, weights, binary output flag, minlength, maxlength, and name. These inputs are used in the operation, and any `InvalidArgumentError` exceptions are caught and ignored.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"main\",\n        \"component_code\": \"def main():\\n  atheris.Setup(sys.argv, TestOneInput, enable_python_coverage=True)\\n  atheris.Fuzz()\",\n        \"component_description\": \"The main function sets up Atheris with the command line arguments and the `TestOneInput` function as the target for fuzzing. It also enables Python coverage tracking and starts the fuzzing process.\"\n      },\n      {\n        \"component_type\": \"IF_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"if __name__ == \\\"__main__\\\":\\n  main()\",\n        \"component_description\": \"Entry point of the script. It checks if the script is being run directly (not imported as a module) and calls the `main` function to start the fuzzing process.\"\n      }\n    ],\n    \"overall_description\": \"This Python script defines a fuzzer for TensorFlow's raw operation `SparseCountSparseOutput`. The script uses Atheris, a coverage-guided greybox fuzzer, to generate random inputs for testing this specific TensorFlow operation. It defines a function `TestOneInput` that creates randomized shapes and values using `FuzzingHelper`, and then tests the operation with these values while handling any potential errors. The `main` function sets up and starts the fuzzing process.\"\n  }\n}"
    },
    "python_20": {
        "input": "<filename>third_party/llvm_openmp/cmake_vars.bzl\n\"\"\"This file contains helpers for cmake.\"\"\"\n\ndef _quote(s):\n    \"\"\"Quotes the given string for use in a shell command.\n\n    This function double-quotes the given string (in case it contains spaces or\n    other special characters) and escapes any special characters (dollar signs,\n    double-quotes, and backslashes) that may be present.\n\n    Args:\n      s: The string to quote.\n\n    Returns:\n      An escaped and quoted version of the string that can be passed to a shell\n      command.\n    \"\"\"\n    return ('\"' +\n            s.replace(\"\\\\\", \"\\\\\\\\\").replace(\"$\", \"\\\\$\").replace('\"', \"\\\\\\\"\") +\n            '\"')\n\ndef cmake_var_string(cmake_vars):\n    \"\"\"Converts a dictionary to an input suitable for expand_cmake_vars.\n\n    Ideally we would jist stringify in the expand_cmake_vars() rule, but select()\n    interacts badly with genrules.\n\n    Args:\n      cmake_vars: a dictionary with string keys and values that are convertable to\n        strings.\n\n    Returns:\n      cmake_vars in a form suitable for passing to expand_cmake_vars.\n    \"\"\"\n    return \" \".join([\n        _quote(\"{}={}\".format(k, str(v)))\n        for (k, v) in cmake_vars.items()\n    ])\n\ndef expand_cmake_vars(name, src, dst, cmake_vars):\n    \"\"\"Expands #cmakedefine, #cmakedefine01, and CMake variables in a text file.\n\n    Args:\n      name: the name of the rule\n      src: the input of the rule\n      dst: the output of the rule\n      cmake_vars: a string containing the CMake variables, as generated by\n        cmake_var_string.\n    \"\"\"\n    expand_cmake_vars_tool = \"@org_tensorflow//third_party/llvm_openmp:expand_cmake_vars\"\n    native.genrule(\n        name = name,\n        srcs = [src],\n        tools = [expand_cmake_vars_tool],\n        outs = [dst],\n        cmd = (\"$(location {}) \".format(expand_cmake_vars_tool) + cmake_vars +\n               \"< $< > $@\"),\n    )\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"_quote\",\n        \"component_code\": 'def _quote(s):\\n    \"\"\"Quotes the given string for use in a shell command.\\n\\n    This function double-quotes the given string (in case it contains spaces or\\n    other special characters) and escapes any special characters (dollar signs,\\n    double-quotes, and backslashes) that may be present.\\n\\n    Args:\\n      s: The string to quote.\\n\\n    Returns:\\n      An escaped and quoted version of the string that can be passed to a shell\\n      command.\\n    \"\"\"\\n    return (\\\\\"\"\\\\\"\" +\\n            s.replace(\"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\").replace(\"$\", \"\\\\\\\\\\\\$\").replace(\\\\\"\\\\\"\", \"\\\\\\\\\\\\\\\"\") +\\n            \"\\\\\"\"\\\\\"\")',\n        \"component_description\": \"This function takes a string and returns a new string that is properly quoted for use in shell commands. It escapes dollar signs, double quotes, and backslashes to prevent them from being interpreted by the shell.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"cmake_var_string\",\n        \"component_code\": 'def cmake_var_string(cmake_vars):\\n    \"\"\"Converts a dictionary to an input suitable for expand_cmake_vars.\\n\\n    Ideally we would jist stringify in the expand_cmake_vars() rule, but select()\\n    interacts badly with genrules.\\n\\n    Args:\\n      cmake_vars: a dictionary with string keys and values that are convertable to\\n        strings.\\n\\n    Returns:\\n      cmake_vars in a form suitable for passing to expand_cmake_vars.\\n    \"\"\"\\n    return \" \".join([\\n        _quote(\"{}={}\".format(k, str(v)))\\n        for (k, v) in cmake_vars.items()\\n    ])',\n        \"component_description\": \"This function converts a dictionary of CMake variables into a string format suitable for passing to the `expand_cmake_vars` function. It uses the `_quote` function to ensure that each key-value pair is properly quoted.\"\n      },\n      {\n        \"component_type\": \"FUNCTION_DEFINITION\",\n        \"component_name\": \"expand_cmake_vars\",\n        \"component_code\": 'def expand_cmake_vars(name, src, dst, cmake_vars):\\n    \"\"\"Expands #cmakedefine, #cmakedefine01, and CMake variables in a text file.\\n\\n    Args:\\n      name: the name of the rule\\n      src: the input of the rule\\n      dst: the output of the rule\\n      cmake_vars: a string containing the CMake variables, as generated by\\n        cmake_var_string.\\n    \"\"\"\\n    expand_cmake_vars_tool = \"@org_tensorflow//third_party/llvm_openmp:expand_cmake_vars\"\\n    native.genrule(\\n        name = name,\\n        srcs = [src],\\n        tools = [expand_cmake_vars_tool],\\n        outs = [dst],\\n        cmd = (\"$(location {}) \".format(expand_cmake_vars_tool) + cmake_vars +\\n               \"< $< > $@\"),\\n    )',\n        \"component_description\": \"This function defines a genrule in Bazel that uses the `expand_cmake_vars` tool to expand CMake variables in an input file and produce an output file. It takes the name of the rule, the source file, the destination file, and a string of CMake variables as arguments.\"\n      }\n    ],\n    \"overall_description\": \"This Python script contains helper functions for processing CMake variables in Bazel build rules. The `_quote` function ensures that strings are properly quoted for shell commands. The `cmake_var_string` function converts a dictionary of CMake variables into a string format suitable for the `expand_cmake_vars` tool. The `expand_cmake_vars` function defines a genrule in Bazel to use this tool, expanding CMake variables in an input file and producing an output file.\"\n  }\n}"
    },
    "python_21": {
        "input": "<gh_stars>1000+\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Thread-local stack.\"\"\"\n\nimport threading\n\n\n# TODO(srbs): Move this to C++.\nclass ThreadLocalStack(threading.local):\n  \"\"\"A thread-local stack of objects for providing implicit defaults.\"\"\"\n\n  def __init__(self):\n    super(ThreadLocalStack, self).__init__()\n    self._stack = []\n\n  def peek(self):\n    return self._stack[-1] if self._stack else None\n\n  def push(self, ctx):\n    return self._stack.append(ctx)\n\n  def pop(self):\n    self._stack.pop()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": NULL,\n        \"component_code\": \"import threading\",\n        \"component_description\": \"Imports the threading module, which provides a way to use threads in Python. This is used to create thread-local data.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"ThreadLocalStack\",\n        \"component_code\": \"\"\"class ThreadLocalStack(threading.local):\\n  \\\"\\\"\\\"A thread-local stack of objects for providing implicit defaults.\\\"\\\"\\\"\\n\\n  def __init__(self):\\n    super(ThreadLocalStack, self).__init__()\\n    self._stack = []\\n\\n  def peek(self):\\n    return self._stack[-1] if self._stack else None\\n\\n  def push(self, ctx):\\n    return self._stack.append(ctx)\\n\\n  def pop(self):\\n    self._stack.pop()\"\"\",\n        \"component_description\": \"Defines a class named ThreadLocalStack that inherits from threading.local. This class provides a thread-local stack of objects which can be used to maintain implicit defaults specific to each thread.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"__init__\",\n        \"component_code\": \"\"\"def __init__(self):\\n  super(ThreadLocalStack, self).__init__()\\n  self._stack = []\"\"\",\n        \"component_description\": \"The constructor method that initializes the ThreadLocalStack instance. It calls the constructor of the superclass (threading.local) and initializes an empty list _stack to store objects.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"peek\",\n        \"component_code\": \"\"\"def peek(self):\\n  return self._stack[-1] if self._stack else None\"\"\",\n        \"component_description\": \"A method that returns the last element of the _stack list without removing it, acting as a 'peek' operation. If the stack is empty, it returns None.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"push\",\n        \"component_code\": \"\"\"def push(self, ctx):\\n  return self._stack.append(ctx)\"\"\",\n        \"component_description\": \"A method that appends an object (ctx) to the _stack list. This acts as a 'push' operation in the stack.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"pop\",\n        \"component_code\": \"\"\"def pop(self):\\n  self._stack.pop()\"\"\",\n        \"component_description\": \"A method that removes and returns the last element from the _stack list, acting as a 'pop' operation in the stack. If the stack is empty, this will raise an IndexError.\"\n      }\n    ],\n    \"overall_description\": \"The provided Python code defines a class named ThreadLocalStack which extends threading.local to provide a thread-local stack of objects. This allows each thread to have its own separate stack without affecting other threads' stacks. The class includes methods to peek at the top element of the stack, push an element onto the stack, and pop an element from the stack. The use of threading.local ensures that the _stack attribute is local to each thread.\"\n  }\n}"
    },
    "python_22": {
        "input": "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for data input for speech commands.\"\"\"\n\nimport os\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.speech_commands import wav_to_features\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.platform import test\n\n\nclass WavToFeaturesTest(test.TestCase):\n\n  def _getWavData(self):\n    with self.cached_session():\n      sample_data = tf.zeros([32000, 2])\n      wav_encoder = tf.audio.encode_wav(sample_data, 16000)\n      wav_data = self.evaluate(wav_encoder)\n    return wav_data\n\n  def _saveTestWavFile(self, filename, wav_data):\n    with open(filename, \"wb\") as f:\n      f.write(wav_data)\n\n  def _saveWavFolders(self, root_dir, labels, how_many):\n    wav_data = self._getWavData()\n    for label in labels:\n      dir_name = os.path.join(root_dir, label)\n      os.mkdir(dir_name)\n      for i in range(how_many):\n        file_path = os.path.join(dir_name, \"some_audio_%d.wav\" % i)\n        self._saveTestWavFile(file_path, wav_data)\n\n  @test_util.run_deprecated_v1\n  def testWavToFeatures(self):\n    tmp_dir = self.get_temp_dir()\n    wav_dir = os.path.join(tmp_dir, \"wavs\")\n    os.mkdir(wav_dir)\n    self._saveWavFolders(wav_dir, [\"a\", \"b\", \"c\"], 100)\n    input_file_path = os.path.join(tmp_dir, \"input.wav\")\n    output_file_path = os.path.join(tmp_dir, \"output.c\")\n    wav_data = self._getWavData()\n    self._saveTestWavFile(input_file_path, wav_data)\n    wav_to_features.wav_to_features(16000, 1000, 10, 10, 40, True, \"average\",\n                                    input_file_path, output_file_path)\n    with open(output_file_path, \"rb\") as f:\n      content = f.read()\n      self.assertIn(b\"const unsigned char g_input_data\", content)\n\n  @test_util.run_deprecated_v1\n  def testWavToFeaturesMicro(self):\n    tmp_dir = self.get_temp_dir()\n    wav_dir = os.path.join(tmp_dir, \"wavs\")\n    os.mkdir(wav_dir)\n    self._saveWavFolders(wav_dir, [\"a\", \"b\", \"c\"], 100)\n    input_file_path = os.path.join(tmp_dir, \"input.wav\")\n    output_file_path = os.path.join(tmp_dir, \"output.c\")\n    wav_data = self._getWavData()\n    self._saveTestWavFile(input_file_path, wav_data)\n    wav_to_features.wav_to_features(16000, 1000, 10, 10, 40, True, \"micro\",\n                                    input_file_path, output_file_path)\n    with open(output_file_path, \"rb\") as f:\n      content = f.read()\n      self.assertIn(b\"const unsigned char g_input_data\", content)\n\n\nif __name__ == \"__main__\":\n  test.main()\n",
        "output": "{\n  \"output\": {\n    \"programming_language\": \"Python\",\n    \"components\": [\n      {\n        \"component_type\": \"IMPORT_STATEMENT\",\n        \"component_name\": null,\n        \"component_code\": \"import os\\n\\nimport tensorflow as tf\\n\\nfrom tensorflow.examples.speech_commands import wav_to_features\\nfrom tensorflow.python.framework import test_util\\nfrom tensorflow.python.platform import test\",\n        \"component_description\": \"Imports the necessary modules and packages including TensorFlow, operating system utilities, and specific functionalities for handling WAV files and testing.\"\n      },\n      {\n        \"component_type\": \"CLASS_DEFINITION\",\n        \"component_name\": \"WavToFeaturesTest\",\n        \"component_code\": \"class WavToFeaturesTest(test.TestCase):\\n\\n  def _getWavData(self):\\n    with self.cached_session():\\n      sample_data = tf.zeros([32000, 2])\\n      wav_encoder = tf.audio.encode_wav(sample_data, 16000)\\n      wav_data = self.evaluate(wav_encoder)\\n    return wav_data\\n\\n  def _saveTestWavFile(self, filename, wav_data):\\n    with open(filename, \\\"wb\\\") as f:\\n      f.write(wav_data)\\n\\n  def _saveWavFolders(self, root_dir, labels, how_many):\\n    wav_data = self._getWavData()\\n    for label in labels:\\n      dir_name = os.path.join(root_dir, label)\\n      os.mkdir(dir_name)\\n      for i in range(how_many):\\n        file_path = os.path.join(dir_name, \\\"some_audio_%d.wav\\\" % i)\\n        self._saveTestWavFile(file_path, wav_data)\\n\\n  @test_util.run_deprecated_v1\\n  def testWavToFeatures(self):\\n    tmp_dir = self.get_temp_dir()\\n    wav_dir = os.path.join(tmp_dir, \\\"wavs\\\")\\n    os.mkdir(wav_dir)\\n    self._saveWavFolders(wav_dir, [\\\"a\\\", \\\"b\\\", \\\"c\\\"], 100)\\n    input_file_path = os.path.join(tmp_dir, \\\"input.wav\\\")\\n    output_file_path = os.path.join(tmp_dir, \\\"output.c\\\")\\n    wav_data = self._getWavData()\\n    self._saveTestWavFile(input_file_path, wav_data)\\n    wav_to_features.wav_to_features(16000, 1000, 10, 10, 40, True, \\\"average\\\",\\n                                    input_file_path, output_file_path)\\n    with open(output_file_path, \\\"rb\\\") as f:\\n      content = f.read()\\n      self.assertIn(b\\\"const unsigned char g_input_data\\\", content)\\n\\n  @test_util.run_deprecated_v1\\n  def testWavToFeaturesMicro(self):\\n    tmp_dir = self.get_temp_dir()\\n    wav_dir = os.path.join(tmp_dir, \\\"wavs\\\")\\n    os.mkdir(wav_dir)\\n    self._saveWavFolders(wav_dir, [\\\"a\\\", \\\"b\\\", \\\"c\\\"], 100)\\n    input_file_path = os.path.join(tmp_dir, \\\"input.wav\\\")\\n    output_file_path = os.path.join(tmp_dir, \\\"output.c\\\")\\n    wav_data = self._getWavData()\\n    self._saveTestWavFile(input_file_path, wav_data)\\n    wav_to_features.wav_to_features(16000, 1000, 10, 10, 40, True, \\\"micro\\\",\\n                                    input_file_path, output_file_path)\\n    with open(output_file_path, \\\"rb\\\") as f:\\n      content = f.read()\\n      self.assertIn(b\\\"const unsigned char g_input_data\\\", content)\",\n        \"component_description\": \"Defines a test class named WavToFeaturesTest that inherits from TensorFlow's test.TestCase. It includes methods to generate WAV data, save WAV files, and create directories with multiple WAV files for testing purposes.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"_getWavData\",\n        \"component_code\": \"def _getWavData(self):\\n  with self.cached_session():\\n    sample_data = tf.zeros([32000, 2])\\n    wav_encoder = tf.audio.encode_wav(sample_data, 16000)\\n    wav_data = self.evaluate(wav_encoder)\\n  return wav_data\",\n        \"component_description\": \"Generates a WAV data by creating a tensor of zeros and encoding it into WAV format with a sample rate of 16 kHz. The encoded WAV data is then evaluated and returned.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"_saveTestWavFile\",\n        \"component_code\": \"def _saveTestWavFile(self, filename, wav_data):\\n  with open(filename, \\\"wb\\\") as f:\\n    f.write(wav_data)\",\n        \"component_description\": \"Saves the provided WAV data to a file in binary write mode.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"_saveWavFolders\",\n        \"component_code\": \"def _saveWavFolders(self, root_dir, labels, how_many):\\n  wav_data = self._getWavData()\\n  for label in labels:\\n    dir_name = os.path.join(root_dir, label)\\n    os.mkdir(dir_name)\\n    for i in range(how_many):\\n      file_path = os.path.join(dir_name, \\\"some_audio_%d.wav\\\" % i)\\n      self._saveTestWavFile(file_path, wav_data)\",\n        \"component_description\": \"Creates directories based on provided labels and saves multiple WAV files into each directory using the generated WAV data.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testWavToFeatures\",\n        \"component_code\": \"@test_util.run_deprecated_v1\\ndef testWavToFeatures(self):\\n  tmp_dir = self.get_temp_dir()\\n  wav_dir = os.path.join(tmp_dir, \\\"wavs\\\")\\n  os.mkdir(wav_dir)\\n  self._saveWavFolders(wav_dir, [\\\"a\\\", \\\"b\\\", \\\"c\\\"], 100)\\n  input_file_path = os.path.join(tmp_dir, \\\"input.wav\\\")\\n  output_file_path = os.path.join(tmp_dir, \\\"output.c\\\")\\n  wav_data = self._getWavData()\\n  self._saveTestWavFile(input_file_path, wav_data)\\n  wav_to_features.wav_to_features(16000, 1000, 10, 10, 40, True, \\\"average\\\",\\n                                  input_file_path, output_file_path)\\n  with open(output_file_path, \\\"rb\\\") as f:\\n    content = f.read()\\n    self.assertIn(b\\\"const unsigned char g_input_data\\\", content)\",\n        \"component_description\": \"A test method that sets up a temporary directory, creates subdirectories and WAV files, calls the wav_to_features function with specific parameters for average processing, reads the output file, and asserts that it contains the expected string.\"\n      },\n      {\n        \"component_type\": \"METHOD_DEFINITION\",\n        \"component_name\": \"testWavToFeaturesMicro\",\n        \"component_code\": \"@test_util.run_deprecated_v1\\ndef testWavToFeaturesMicro(self):\\n  tmp_dir = self.get_temp_dir()\\n  wav_dir = os.path.join(tmp_dir, \\\"wavs\\\")\\n  os.mkdir(wav_dir)\\n  self._saveWavFolders(wav_dir, [\\\"a\\\", \\\"b\\\", \\\"c\\\"], 100)\\n  input_file_path = os.path.join(tmp_dir, \\\"input.wav\\\")\\n  output_file_path = os.path.join(tmp_dir, \\\"output.c\\\")\\n  wav_data = self._getWavData()\\n  self._saveTestWavFile(input_file_path, wav_data)\\n  wav_to_features.wav_to_features(16000, 1000, 10, 10, 40, True, \\\"micro\\\",\\n                                  input_file_path, output_file_path)\\n  with open(output_file_path, \\\"rb\\\") as f:\\n    content = f.read()\\n    self.assertIn(b\\\"const unsigned char g_input_data\\\", content)\",\n        \"component_description\": \"A test method similar to testWavToFeatures but calls the wav_to_features function with specific parameters for micro processing.\"\n      },\n      {\n        \"component_type\": \"CODE_BLOCK\",\n        \"component_name\": \"Main Execution Block\",\n        \"component_code\": \"if __name__ == \\\"__main__\\\":\\n  tf.test.main()\",\n        \"component_description\": \"Ensures that the test cases are run when the script is executed as a standalone program, invoking TensorFlow's test runner.\"\n      }\n    ]\n  },\n  \"overall_description\": \"This code snippet defines a test suite for verifying the functionality of the wav_to_features function. It includes methods to generate WAV data, save files and directories with multiple WAV files, and two test cases that process the generated WAV data using different parameters ('average' and 'micro'). The tests check if the output files contain the expected string, indicating successful processing.\"\n}"
    }
}