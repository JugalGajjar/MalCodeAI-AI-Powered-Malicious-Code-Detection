{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "# First, download the dataset with caching\n",
    "def download_starcoderdata_cpp(save_directory, split=\"train\", download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS):\n",
    "    try:\n",
    "        ds = load_dataset(\n",
    "            \"bigcode/starcoderdata\",\n",
    "            data_dir=\"cpp\",\n",
    "            split=split,\n",
    "            cache_dir=save_directory,\n",
    "            download_mode=download_mode,\n",
    "        )\n",
    "\n",
    "        # Save the dataset properly for later reloading\n",
    "        output_path = f\"{save_directory}/cpp_{split}_dataset\"\n",
    "        ds.save_to_disk(output_path)\n",
    "\n",
    "        print(f\"Dataset 'bigcode/starcoderdata' (C++, {split}) successfully downloaded and saved to '{output_path}'.\")\n",
    "        return ds\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset 'bigcode/starcoderdata' (C++, {split}): {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00198ce276f24100b13bd4e151298153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261af6436f364da283c85154a051e73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/48 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e4b93adade450186a92b4ee8adc3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0c27ea11c04e098e649e26d947d293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa330609f3c54815803455d026b2fe39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/100 shards):   0%|          | 0/6353527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'bigcode/starcoderdata' (C++, train) successfully downloaded and saved to './/cpp_train_dataset'.\n"
     ]
    }
   ],
   "source": [
    "ds = download_starcoderdata_cpp(save_directory=\"./\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22e12d3eb7d4e27a2b0e85e4da51b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Later, load the dataset from the saved location\n",
    "dataset = load_from_disk(\"./cpp_train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id', 'content'],\n",
       "    num_rows: 6353527\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_stars_repo_path': 'button_events_game.cpp', 'max_stars_repo_name': 'darkoppressor/cosmic-runner', 'max_stars_count': 0.0, 'id': '0', 'content': '/* Copyright (c) 2012 Cheese and Bacon Games, LLC */\\n/* This file is licensed under the MIT License. */\\n/* See the file docs/LICENSE.txt for the full license text. */\\n\\n#include \"game.h\"\\n#include \"android_leaderboard.h\"\\n\\n#include <button_events.h>\\n#include <window_manager.h>\\n#include <game_manager.h>\\n#include <android.h>\\n#include <gui_manager.h>\\n\\n#include <boost/algorithm/string.hpp>\\n\\nusing namespace std;\\n\\nbool Button_Events::handle_button_event_game (string button_event, Window* parent_window, bool& window_opened_on_top) {\\n    if (button_event == \"game_over\") {\\n        Window_Manager::close_all_windows();\\n\\n        if (Game::is_score_high()) {\\n            Window_Manager::get_window(\"input_name\")->toggle_on();\\n\\n            GUI_Manager::confirm_gui_object();\\n        } else {\\n            Android_Leaderboard::submit_highscore(Android_Leaderboard::HIGH_SCORES, Game::get_score());\\n            Android_Leaderboard::submit_highscore(Android_Leaderboard::BEST_KILL_COUNT, Game::get_kills());\\n            Android_Leaderboard::submit_highscore(Android_Leaderboard::DEBRIS_DODGED, Game::get_dodges());\\n\\n            Game_Manager::stop();\\n\\n            Window_Manager::get_window(\"main_menu\")->toggle_on();\\n\\n            Window_Manager::get_window(\"high_scores\")->toggle_on();\\n        }\\n\\n        window_opened_on_top = true;\\n\\n        return true;\\n    } else if (button_event == \"gpg_toggle_sign_in\") {\\n        if (Android::gpg_is_silent_sign_in_attempt_complete()) {\\n            if (!Android::gpg_is_signed_in()) {\\n                Android::gpg_sign_in();\\n\\n                Game::android_gpg_signing_in();\\n            } else {\\n                Android::gpg_sign_out();\\n            }\\n        }\\n\\n        return true;\\n    } else if (boost::algorithm::starts_with(button_event, \"gpg_show_leaderboard_\")) {\\n        boost::algorithm::erase_first(button_event, \"gpg_show_leaderboard_\");\\n\\n        if (Android::gpg_is_signed_in()) {\\n            Android::gpg_show_leaderboard(button_event);\\n        }\\n\\n        return true;\\n    } else if (button_event == \"gpg_show_all_leaderboards\") {\\n        if (Android::gpg_is_signed_in()) {\\n            Android::gpg_show_all_leaderboards();\\n        }\\n\\n        return true;\\n    } else if (button_event == \"gpg_show_achievements\") {\\n        if (Android::gpg_is_signed_in()) {\\n            Android::gpg_show_achievements();\\n        }\\n\\n        return true;\\n    } else if (button_event == \"name\") {\\n        Window_Manager::close_all_windows();\\n\\n        if (parent_window != 0) {\\n            Game::add_high_score(parent_window->get_info_text(0));\\n\\n            Android_Leaderboard::submit_highscore(Android_Leaderboard::HIGH_SCORES, Game::get_score());\\n            Android_Leaderboard::submit_highscore(Android_Leaderboard::BEST_KILL_COUNT, Game::get_kills());\\n            Android_Leaderboard::submit_highscore(Android_Leaderboard::DEBRIS_DODGED, Game::get_dodges());\\n        }\\n\\n        Game_Manager::stop();\\n\\n        Window_Manager::get_window(\"main_menu\")->toggle_on();\\n\\n        Window_Manager::get_window(\"high_scores\")->toggle_on();\\n\\n        window_opened_on_top = true;\\n\\n        return true;\\n    } else if (boost::algorithm::starts_with(button_event, \"select_upgrade_\")) {\\n        boost::algorithm::erase_first(button_event, \"select_upgrade_\");\\n\\n        Window_Manager::close_all_windows();\\n\\n        Game::player_add_upgrade(button_event);\\n\\n        Game::complete_contract();\\n\\n        return true;\\n    } else if (button_event == \"skip_upgrade\") {\\n        Window_Manager::close_all_windows();\\n\\n        Game::restore_hull_from_contract();\\n\\n        Game::complete_contract();\\n\\n        return true;\\n    }\\n\\n    return false;\\n}\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sort(column_names=[\"max_stars_count\", \"max_stars_repo_name\"], reverse=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "/**\n",
      " * Copyright (c) Facebook, Inc. and its affiliates.\n",
      " *\n",
      " * This source code is licensed under the MIT license found in the\n",
      " * LICENSE file in the root directory of this source tree.\n",
      " */\n",
      "\n",
      "#include \"thread-local.h\"\n",
      "\n",
      "#ifdef __linux__\n",
      "#include <link.h>\n",
      "#include <asm/prctl.h>\n",
      "#include <sys/prctl.h>\n",
      "extern \"C\" {\n",
      "extern int arch_prctl(int, unsigned long*);\n",
      "}\n",
      "#endif //__linux__\n",
      "\n",
      "namespace HPHP {\n",
      "\n",
      "#ifdef USE_GCC_FAST_TLS\n",
      "\n",
      "void ThreadLocalManager::OnThreadExit(void* p) {\n",
      "  auto list = getList(p);\n",
      "  p = list->head;\n",
      "  delete list;\n",
      "  while (p != nullptr) {\n",
      "    auto* pNode = static_cast<ThreadLocalNode<void>*>(p);\n",
      "    if (pNode->m_on_thread_exit_fn) {\n",
      "      pNode->m_on_thread_exit_fn(p);\n",
      "    }\n",
      "    p = pNode->m_next;\n",
      "  }\n",
      "}\n",
      "\n",
      "void ThreadLocalManager::PushTop(void* nodePtr, size_t nodeSize) {\n",
      "  auto& node = *static_cast<ThreadLocalNode<void>*>(nodePtr);\n",
      "  auto key = GetManager().m_key;\n",
      "  auto list = getList(pthread_getspecific(key));\n",
      "  if (UNLIKELY(!list)) {\n",
      "    ThreadLocalSetValue(key, list = new ThreadLocalList);\n",
      "  }\n",
      "  node.m_next = list->head;\n",
      "  node.m_size = nodeSize;\n",
      "  list->head = node.m_next;\n",
      "}\n",
      "\n",
      "ThreadLocalManager& ThreadLocalManager::GetManager() {\n",
      "  static ThreadLocalManager m;\n",
      "  return m;\n",
      "}\n",
      "\n",
      "#ifdef __APPLE__\n",
      "ThreadLocalManager::ThreadLocalList::ThreadLocalList() {\n",
      "  pthread_t self = pthread_self();\n",
      "  handler.__routine = ThreadLocalManager::OnThreadExit;\n",
      "  handler.__arg = this;\n",
      "  handler.__next = self->__cleanup_stack;\n",
      "  self->__cleanup_stack = &handler;\n",
      "}\n",
      "#endif\n",
      "\n",
      "#endif\n",
      "\n",
      "#ifdef __linux__\n",
      "\n",
      "static int visit_phdr(dl_phdr_info* info, size_t, void*) {\n",
      "  for (size_t i = 0, n = info->dlpi_phnum; i < n; ++i) {\n",
      "    const auto& hdr = info->dlpi_phdr[i];\n",
      "    auto addr = info->dlpi_addr + hdr.p_vaddr;\n",
      "    if (addr < 0x100000000LL && hdr.p_type == PT_TLS) {\n",
      "      // found the main thread-local section\n",
      "      assert(int(hdr.p_memsz) == hdr.p_memsz); // ensure no truncation\n",
      "      return hdr.p_memsz;\n",
      "    }\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "std::pair<void*,size_t> getCppTdata() {\n",
      "  uintptr_t addr;\n",
      "  if (!arch_prctl(ARCH_GET_FS, &addr)) {\n",
      "    // fs points to the end of the threadlocal area.\n",
      "    size_t size = dl_iterate_phdr(&visit_phdr, nullptr);\n",
      "    return {(void*)(addr - size), size};\n",
      "  }\n",
      "  return {nullptr, 0};\n",
      "}\n",
      "\n",
      "#else\n",
      "\n",
      "// how do you find the thread local section on your system?\n",
      "std::pair<void*,size_t> getCppTdata() {\n",
      "  return {nullptr, 0};\n",
      "}\n",
      "\n",
      "#endif //__linux__\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example 1:\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n",
      "\n",
      "#define EIGEN_USE_GPU\n",
      "\n",
      "#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n",
      "#include \"tensorflow/core/framework/register_types.h\"\n",
      "#include \"tensorflow/core/framework/tensor.h\"\n",
      "#include \"tensorflow/core/framework/tensor_shape.h\"\n",
      "#include \"tensorflow/core/kernels/gpu_device_array.h\"\n",
      "#include \"tensorflow/core/kernels/gpu_device_array_gpu.h\"\n",
      "#include \"tensorflow/core/kernels/gpu_prim_helpers.h\"\n",
      "#include \"tensorflow/core/kernels/sparse_concat_op.h\"\n",
      "#include \"tensorflow/core/lib/core/bits.h\"\n",
      "#include \"tensorflow/core/util/gpu_kernel_helper.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "typedef Eigen::GpuDevice GPUDevice;\n",
      "\n",
      "namespace functor {\n",
      "\n",
      "namespace {\n",
      "\n",
      "template <typename T>\n",
      "__global__ void SparseConcatKernel(\n",
      "    int64 output_nnz, int rank, int concat_dim, bool need_to_sort,\n",
      "    GpuDeviceArrayStruct<const int64*> ind_ptrs_data,\n",
      "    GpuDeviceArrayStruct<const T*> val_ptrs_data,\n",
      "    GpuDeviceArrayStruct<int64_t> nnz_scan_data,\n",
      "    GpuDeviceArrayStruct<int64_t> concat_size_scan_data,\n",
      "    GpuDeviceArrayStruct<int64_t> output_shape_data,\n",
      "    int64* __restrict__ output_inds, T* __restrict__ output_vals,\n",
      "    int64* __restrict__ output_flat_inds) {\n",
      "  const int64* __restrict__* __restrict__ ind_ptrs =\n",
      "      GetGpuDeviceArrayOnDevice(&ind_ptrs_data);\n",
      "  const T* __restrict__* __restrict__ val_ptrs =\n",
      "      GetGpuDeviceArrayOnDevice(&val_ptrs_data);\n",
      "  const int64* __restrict__ nnz_scan =\n",
      "      GetGpuDeviceArrayOnDevice(&nnz_scan_data);\n",
      "  const int64* __restrict__ concat_size_scan =\n",
      "      GetGpuDeviceArrayOnDevice(&concat_size_scan_data);\n",
      "  const int64* __restrict__ output_shape =\n",
      "      GetGpuDeviceArrayOnDevice(&output_shape_data);\n",
      "  const int64 num_inputs = ind_ptrs_data.size;\n",
      "\n",
      "  for (int64 nz : GpuGridRangeX<int64_t>(output_nnz)) {\n",
      "    const int64 input_num =\n",
      "        gpu_helper::upper_bound<int64_t>(nnz_scan, num_inputs, nz) - 1;\n",
      "    const int64 input_nz = nz - nnz_scan[input_num];\n",
      "    const int64 ind_offset = concat_size_scan[input_num];\n",
      "    if (!need_to_sort) {\n",
      "      output_vals[nz] = val_ptrs[input_num][input_nz];\n",
      "    }\n",
      "    int64 flat_ind = 0;\n",
      "    for (int j = 0; j < rank; ++j) {\n",
      "      const int64 output_ind = ind_ptrs[input_num][input_nz * rank + j] +\n",
      "                               (j == concat_dim ? ind_offset : 0);\n",
      "      if (!need_to_sort) {\n",
      "        output_inds[nz * rank + j] = output_ind;\n",
      "      } else {\n",
      "        flat_ind = flat_ind * output_shape[j] + output_ind;\n",
      "        output_flat_inds[nz] = flat_ind;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "template <typename T>\n",
      "__global__ void SparseConcatPermuteKernel(\n",
      "    int64 output_nnz, int rank, GpuDeviceArrayStruct<const T*> val_ptrs_data,\n",
      "    GpuDeviceArrayStruct<int64_t> nnz_scan_data,\n",
      "    GpuDeviceArrayStruct<int64_t> output_shape_data,\n",
      "    const int64* __restrict__ output_flat_inds,\n",
      "    const int64* __restrict__ permutation, int64* __restrict__ output_inds,\n",
      "    T* __restrict__ output_vals) {\n",
      "  const T* __restrict__* __restrict__ val_ptrs =\n",
      "      GetGpuDeviceArrayOnDevice(&val_ptrs_data);\n",
      "  const int64* __restrict__ nnz_scan =\n",
      "      GetGpuDeviceArrayOnDevice(&nnz_scan_data);\n",
      "  const int64* __restrict__ output_shape =\n",
      "      GetGpuDeviceArrayOnDevice(&output_shape_data);\n",
      "  const int64 num_inputs = val_ptrs_data.size;\n",
      "\n",
      "  for (int64 nz : GpuGridRangeX<int64_t>(output_nnz)) {\n",
      "    const int64 permuted_nz = permutation[nz];\n",
      "    const int64 input_num =\n",
      "        gpu_helper::upper_bound<int64_t>(nnz_scan, num_inputs, permuted_nz) - 1;\n",
      "    const int64 input_nz = permuted_nz - nnz_scan[input_num];\n",
      "    output_vals[nz] = val_ptrs[input_num][input_nz];\n",
      "    int64 output_flat_ind = output_flat_inds[permuted_nz];\n",
      "    for (int j = rank - 1; j >= 0; --j) {\n",
      "      const int64 output_dim_size = output_shape[j];\n",
      "      output_inds[nz * rank + j] = output_flat_ind % output_dim_size;\n",
      "      output_flat_ind /= output_dim_size;\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "}  // namespace\n",
      "\n",
      "template <typename T>\n",
      "struct SparseConcatFunctor<GPUDevice, T> {\n",
      "  void operator()(OpKernelContext* context, const OpInputList& inds,\n",
      "                  const OpInputList& vals, const OpInputList& shapes,\n",
      "                  int concat_dim) {\n",
      "    const int N = inds.size();\n",
      "    const TensorShape input_shape0(shapes[0].vec<int64_t>());\n",
      "    const int rank = input_shape0.dims();\n",
      "\n",
      "    // The input non-zeros are assumed to be sorted by increasing dimension\n",
      "    // number (i.e., row-major order), so if the concatenation is along the\n",
      "    // first dimension then they remain in order and we can directly compute the\n",
      "    // output indices and values. To concatenate along other dimensions, we\n",
      "    // first compute the flattened (1D) row-major output indices, then sort\n",
      "    // these to obtain the required permutation, and finally gather the permuted\n",
      "    // input values.\n",
      "\n",
      "    GpuDeviceArrayOnHost<const int64*> ind_ptrs(context, N);\n",
      "    GpuDeviceArrayOnHost<const T*> val_ptrs(context, N);\n",
      "    GpuDeviceArrayOnHost<int64_t> nnz_scan(context, N + 1);\n",
      "    GpuDeviceArrayOnHost<int64_t> concat_size_scan(context, N + 1);\n",
      "    OP_REQUIRES_OK(context, ind_ptrs.Init());\n",
      "    OP_REQUIRES_OK(context, val_ptrs.Init());\n",
      "    OP_REQUIRES_OK(context, nnz_scan.Init());\n",
      "    OP_REQUIRES_OK(context, concat_size_scan.Init());\n",
      "    int64 nnz_sum = 0;\n",
      "    int64 concat_size_sum = 0;\n",
      "    nnz_scan.Set(0, nnz_sum);\n",
      "    concat_size_scan.Set(0, concat_size_sum);\n",
      "    for (int i = 0; i < N; ++i) {\n",
      "      ind_ptrs.Set(i, inds[i].matrix<int64_t>().data());\n",
      "      val_ptrs.Set(i, vals[i].vec<T>().data());\n",
      "      nnz_sum += inds[i].dim_size(0);\n",
      "      nnz_scan.Set(i + 1, nnz_sum);\n",
      "      const TensorShape current_shape(shapes[i].vec<int64_t>());\n",
      "      concat_size_sum += current_shape.dim_size(concat_dim);\n",
      "      concat_size_scan.Set(i + 1, concat_size_sum);\n",
      "    }\n",
      "    OP_REQUIRES_OK(context, ind_ptrs.Finalize());\n",
      "    OP_REQUIRES_OK(context, val_ptrs.Finalize());\n",
      "    OP_REQUIRES_OK(context, nnz_scan.Finalize());\n",
      "    OP_REQUIRES_OK(context, concat_size_scan.Finalize());\n",
      "    const int64 output_nnz = nnz_sum;\n",
      "    const int64 output_concat_size = concat_size_sum;\n",
      "\n",
      "    const bool need_to_sort = concat_dim != 0;\n",
      "\n",
      "    GpuDeviceArrayOnHost<int64_t> output_shape(context, rank);\n",
      "    int64 output_dense_elements;\n",
      "    if (need_to_sort) {\n",
      "      OP_REQUIRES_OK(context, output_shape.Init());\n",
      "      output_dense_elements = 1;\n",
      "      for (int j = 0; j < rank; ++j) {\n",
      "        int64 output_dim_size =\n",
      "            j == concat_dim ? output_concat_size : input_shape0.dim_size(j);\n",
      "        output_shape.Set(j, output_dim_size);\n",
      "        output_dense_elements *= output_dim_size;\n",
      "      }\n",
      "      OP_REQUIRES_OK(context, output_shape.Finalize());\n",
      "    }\n",
      "\n",
      "    int64* output_inds_ptr = nullptr;\n",
      "    T* output_vals_ptr = nullptr;\n",
      "    int64* output_flat_inds_ptr = nullptr;\n",
      "    Tensor output_flat_inds;\n",
      "    if (need_to_sort) {\n",
      "      // SparseConcatKernel will (only) produce output_flat_inds.\n",
      "      OP_REQUIRES_OK(context,\n",
      "                     context->allocate_temp(DT_INT64, TensorShape({output_nnz}),\n",
      "                                            &output_flat_inds));\n",
      "      output_flat_inds_ptr = output_flat_inds.vec<int64_t>().data();\n",
      "    } else {\n",
      "      OP_REQUIRES_OK(\n",
      "          context, allocate_outputs(context, rank, output_nnz, &output_inds_ptr,\n",
      "                                    &output_vals_ptr));\n",
      "    }\n",
      "\n",
      "    const GPUDevice& device = context->eigen_gpu_device();\n",
      "\n",
      "    GpuLaunchConfig config = GetGpuLaunchConfig(\n",
      "        output_nnz, device, &SparseConcatKernel<T>,\n",
      "        /*dynamic_shared_memory_size=*/0, /*block_size_limit=*/0);\n",
      "    OP_REQUIRES_OK(\n",
      "        context, GpuLaunchKernel(\n",
      "                     SparseConcatKernel<T>, config.block_count,\n",
      "                     config.thread_per_block, 0, device.stream(), output_nnz,\n",
      "                     rank, concat_dim, need_to_sort, ind_ptrs.data(),\n",
      "                     val_ptrs.data(), nnz_scan.data(), concat_size_scan.data(),\n",
      "                     (need_to_sort ? output_shape.data()\n",
      "                                   : GpuDeviceArrayStruct<int64_t>()),\n",
      "                     output_inds_ptr, output_vals_ptr, output_flat_inds_ptr));\n",
      "\n",
      "    if (!need_to_sort) return;\n",
      "\n",
      "    OP_REQUIRES_OK(context,\n",
      "                   allocate_outputs(context, rank, output_nnz, &output_inds_ptr,\n",
      "                                    &output_vals_ptr));\n",
      "\n",
      "    Tensor permutation;\n",
      "    OP_REQUIRES_OK(context,\n",
      "                   context->allocate_temp(DT_INT64, TensorShape({output_nnz}),\n",
      "                                          &permutation));\n",
      "    int64* permutation_ptr = permutation.vec<int64_t>().data();\n",
      "    OP_REQUIRES_OK(\n",
      "        context,\n",
      "        GpuRadixSort(context, /*size=*/output_nnz,\n",
      "                     /*keys_in=*/output_flat_inds_ptr,\n",
      "                     /*keys_out=*/static_cast<int64*>(nullptr),\n",
      "                     /*indices_in=*/static_cast<const int64*>(nullptr),\n",
      "                     /*indices_out=*/permutation_ptr,\n",
      "                     /*num_bits=*/Log2Ceiling64(output_dense_elements)));\n",
      "\n",
      "    config = GetGpuLaunchConfig(\n",
      "        output_nnz, device, &SparseConcatPermuteKernel<T>,\n",
      "        /*dynamic_shared_memory_size=*/0, /*block_size_limit=*/0);\n",
      "    OP_REQUIRES_OK(\n",
      "        context,\n",
      "        GpuLaunchKernel(SparseConcatPermuteKernel<T>, config.block_count,\n",
      "                        config.thread_per_block, 0, device.stream(), output_nnz,\n",
      "                        rank, val_ptrs.data(), nnz_scan.data(),\n",
      "                        output_shape.data(), output_flat_inds_ptr,\n",
      "                        permutation_ptr, output_inds_ptr, output_vals_ptr));\n",
      "  }\n",
      "\n",
      " private:\n",
      "  Status allocate_outputs(OpKernelContext* context, int rank, int64 output_nnz,\n",
      "                          int64** output_inds_ptr, T** output_vals_ptr) const {\n",
      "    Tensor* output_inds = nullptr;\n",
      "    TF_RETURN_IF_ERROR(context->allocate_output(\n",
      "        0, TensorShape({output_nnz, rank}), &output_inds));\n",
      "    *output_inds_ptr = output_inds->matrix<int64_t>().data();\n",
      "    Tensor* output_vals = nullptr;\n",
      "    TF_RETURN_IF_ERROR(\n",
      "        context->allocate_output(1, TensorShape({output_nnz}), &output_vals));\n",
      "    *output_vals_ptr = output_vals->vec<T>().data();\n",
      "    return Status::OK();\n",
      "  }\n",
      "};\n",
      "\n",
      "#define DEFINE_SPARSE_CONCAT_FUNCTOR(T) \\\n",
      "  template struct SparseConcatFunctor<GPUDevice, T>;\n",
      "TF_CALL_POD_TYPES(DEFINE_SPARSE_CONCAT_FUNCTOR);\n",
      "\n",
      "#undef DEFINE_SPARSE_CONCAT_FUNCTOR\n",
      "\n",
      "}  // namespace functor\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n",
      "\n",
      "\n",
      "\n",
      "Example 2:\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#include \"tensorflow/core/tpu/kernels/tpu_compilation_cache_grpc.h\"\n",
      "\n",
      "#include <functional>\n",
      "\n",
      "#include \"grpcpp/impl/codegen/async_stream.h\"\n",
      "#include \"grpcpp/impl/codegen/async_unary_call.h\"\n",
      "#include \"grpcpp/impl/codegen/channel_interface.h\"\n",
      "#include \"grpcpp/impl/codegen/client_callback.h\"\n",
      "#include \"grpcpp/impl/codegen/client_unary_call.h\"\n",
      "#include \"grpcpp/impl/codegen/method_handler.h\"\n",
      "#include \"grpcpp/impl/codegen/rpc_service_method.h\"\n",
      "#include \"grpcpp/impl/codegen/server_callback.h\"\n",
      "#include \"grpcpp/impl/codegen/service_type.h\"\n",
      "#include \"grpcpp/impl/codegen/sync_stream.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace tpu {\n",
      "\n",
      "static const char* grpcTpuCompilationCacheService_method_names[] = {\n",
      "#if defined(LIBTPU_ON_GCE)\n",
      "    \"/tensorflow.tpu.TpuCompilationCacheServiceExternal/GetTpuProgram\",\n",
      "#else  // LIBTPU_ON_GCE\n",
      "    \"/tensorflow.tpu.TpuCompilationCacheService/GetTpuProgram\",\n",
      "#endif  // LIBTPU_ON_GCE\n",
      "};\n",
      "\n",
      "std::unique_ptr<grpc::TpuCompilationCacheService::Stub>\n",
      "grpc::TpuCompilationCacheService::NewStub(\n",
      "    const std::shared_ptr< ::grpc::ChannelInterface>& channel,\n",
      "    const ::grpc::StubOptions& options) {\n",
      "  (void)options;\n",
      "  std::unique_ptr<grpc::TpuCompilationCacheService::Stub> stub(\n",
      "      new grpc::TpuCompilationCacheService::Stub(channel));\n",
      "  return stub;\n",
      "}\n",
      "\n",
      "grpc::TpuCompilationCacheService::Stub::Stub(\n",
      "    const std::shared_ptr< ::grpc::ChannelInterface>& channel)\n",
      "    : channel_(channel),\n",
      "      rpcmethod_get_tpu_program_(grpcTpuCompilationCacheService_method_names[0],\n",
      "                                 ::grpc::internal::RpcMethod::NORMAL_RPC,\n",
      "                                 channel) {}\n",
      "\n",
      "::grpc::Status grpc::TpuCompilationCacheService::Stub::GetTpuProgram(\n",
      "    ::grpc::ClientContext* context, const RequestType& request,\n",
      "    ResponseType* response) {\n",
      "  return ::grpc::internal::BlockingUnaryCall(\n",
      "      channel_.get(), rpcmethod_get_tpu_program_, context, request, response);\n",
      "}\n",
      "\n",
      "::grpc::ClientAsyncResponseReader<\n",
      "    grpc::TpuCompilationCacheService::ResponseType>*\n",
      "grpc::TpuCompilationCacheService::Stub::AsyncGetTpuProgramRaw(\n",
      "    ::grpc::ClientContext* context, const RequestType& request,\n",
      "    ::grpc::CompletionQueue* cq) {\n",
      "  return ::grpc::internal::ClientAsyncResponseReaderFactory<\n",
      "      ResponseType>::Create(channel_.get(), cq, rpcmethod_get_tpu_program_,\n",
      "                            context, request, true);\n",
      "}\n",
      "\n",
      "::grpc::ClientAsyncResponseReader<\n",
      "    grpc::TpuCompilationCacheService::ResponseType>*\n",
      "grpc::TpuCompilationCacheService::Stub::PrepareAsyncGetTpuProgramRaw(\n",
      "    ::grpc::ClientContext* context, const RequestType& request,\n",
      "    ::grpc::CompletionQueue* cq) {\n",
      "  return ::grpc::internal::ClientAsyncResponseReaderFactory<\n",
      "      ResponseType>::Create(channel_.get(), cq, rpcmethod_get_tpu_program_,\n",
      "                            context, request, false);\n",
      "}\n",
      "\n",
      "grpc::TpuCompilationCacheService::Service::Service() {\n",
      "  AddMethod(new ::grpc::internal::RpcServiceMethod(\n",
      "      grpcTpuCompilationCacheService_method_names[0],\n",
      "      ::grpc::internal::RpcMethod::NORMAL_RPC,\n",
      "      new ::grpc::internal::RpcMethodHandler<\n",
      "          grpc::TpuCompilationCacheService::Service, RequestType, ResponseType>(\n",
      "          std::mem_fn(\n",
      "              &grpc::TpuCompilationCacheService::Service::GetTpuProgram),\n",
      "          this)));\n",
      "}\n",
      "\n",
      "grpc::TpuCompilationCacheService::Service::~Service() {}\n",
      "\n",
      "::grpc::Status grpc::TpuCompilationCacheService::Service::GetTpuProgram(\n",
      "    ::grpc::ServerContext* context, const RequestType* request,\n",
      "    ResponseType* response) {\n",
      "  (void)context;\n",
      "  (void)request;\n",
      "  (void)response;\n",
      "  return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, \"\");\n",
      "}\n",
      "\n",
      "}  // namespace tpu\n",
      "}  // namespace tensorflow\n",
      "\n",
      "\n",
      "\n",
      "Example 3:\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/compiler/xla/service/memory_space_assignment_utils.h\"\n",
      "\n",
      "#include \"tensorflow/compiler/xla/service/hlo_casting_utils.h\"\n",
      "#include \"tensorflow/compiler/xla/service/hlo_instructions.h\"\n",
      "\n",
      "namespace xla {\n",
      "\n",
      "bool MemorySpaceAssignmentUtils::IsValueAllowedInAlternateMemory(\n",
      "    const HloValue* value) {\n",
      "  // If the buffer is a tuple, don't use this algorithm for now. The buffers\n",
      "  // that are pointed to by the tuple will still use this algorithm.  Because\n",
      "  // tuples are cheap to place in the alternate memory (they are just pointers)\n",
      "  // we don't need to use prefetch/evict logic.\n",
      "  if (value->shape().IsTuple()) {\n",
      "    VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "            << \" in default mem because it is a tuple.\";\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  // Don't place scalars in the alternate memory.\n",
      "  if (ShapeUtil::IsEffectiveScalar(value->shape())) {\n",
      "    VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "            << \" in default mem because it is a scalar.\";\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  // The semantics of TupleSelect are weird: TupleSelect doesn't define a\n",
      "  // buffer, but just forwards the buffers in the either left or right side.\n",
      "  // This means the two different inputs to TupleSelect must not alias, yet they\n",
      "  // should be allocated in the same memory space, and both buffers must be kept\n",
      "  // alive for the entire live range of TupleSelect. Instead, just don't\n",
      "  // allocate TupleSelect in the alternate memory space.\n",
      "  // TODO(berkin): Not allocating add-dependencies either since they need to be\n",
      "  // treated specially. We should revisit this later.\n",
      "  for (const HloPosition& position : value->positions()) {\n",
      "    if (position.instruction->opcode() == HloOpcode::kTupleSelect ||\n",
      "        position.instruction->opcode() == HloOpcode::kAddDependency) {\n",
      "      VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "              << \" in default mem because it has a tuple-select or \"\n",
      "              << \"add-dependency position.\";\n",
      "      return false;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  // Send and Recv HLOs return a request identifier. These should not be\n",
      "  // allocated in the alternate memory.\n",
      "  for (const HloPosition& position : value->positions()) {\n",
      "    if ((position.instruction->opcode() == HloOpcode::kSend ||\n",
      "         position.instruction->opcode() == HloOpcode::kRecv) &&\n",
      "        DynCast<HloSendRecvInstruction>(position.instruction)\n",
      "            ->is_host_transfer()) {\n",
      "      // TODO(berkin): Host transfers using alternate memory space doesn't seem\n",
      "      // to work at the moment.\n",
      "      VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "              << \" in default mem because it is a send/recv buffer used for \"\n",
      "                 \"host transfer.\";\n",
      "      return false;\n",
      "    }\n",
      "\n",
      "    if (auto* custom_call =\n",
      "            DynCast<HloCustomCallInstruction>(position.instruction)) {\n",
      "      for (const auto& pair : custom_call->output_to_operand_aliasing()) {\n",
      "        if (position.index == pair.first) {\n",
      "          VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "                  << \" in default mem because it is a custom-call output that \"\n",
      "                     \"aliases an operand buffer.\";\n",
      "          return false;\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return true;\n",
      "}\n",
      "\n",
      "bool MemorySpaceAssignmentUtils::IsIntervalAllowedInAlternateMemory(\n",
      "    const GlobalDecreasingSizeBestFitHeap<HloValue>::BufferInterval& interval) {\n",
      "  return IsValueAllowedInAlternateMemory(interval.buffer) &&\n",
      "         absl::c_all_of(interval.colocations, IsValueAllowedInAlternateMemory);\n",
      "}\n",
      "\n",
      "/*static*/ void MemorySpaceAssignmentUtils::HoistConstantOperations(\n",
      "    HloModule& module) {\n",
      "  CHECK(module.has_schedule());\n",
      "  HloSchedule& schedule = module.schedule();\n",
      "  for (const HloComputation* computation : module.MakeNonfusionComputations()) {\n",
      "    CHECK(schedule.is_computation_scheduled(computation));\n",
      "    const HloInstructionSequence& sequence = schedule.sequence(computation);\n",
      "    // Conservatively don't modify the schedule if any instruction has a control\n",
      "    // successor or predecessor on a constant op. Computations with these\n",
      "    // dependencies should be very rare anyway.\n",
      "    bool contains_constant_successor_or_predecessors = false;\n",
      "    for (HloInstruction* instruction : sequence.instructions()) {\n",
      "      if (instruction->opcode() == HloOpcode::kConstant) {\n",
      "        contains_constant_successor_or_predecessors |=\n",
      "            !instruction->control_predecessors().empty();\n",
      "        contains_constant_successor_or_predecessors |=\n",
      "            !instruction->control_successors().empty();\n",
      "      } else {\n",
      "        auto is_constant = [](const HloInstruction* inst) {\n",
      "          return inst->opcode() == HloOpcode::kConstant;\n",
      "        };\n",
      "        contains_constant_successor_or_predecessors |=\n",
      "            absl::c_find_if(instruction->control_predecessors(), is_constant) !=\n",
      "            instruction->control_predecessors().end();\n",
      "        contains_constant_successor_or_predecessors |=\n",
      "            absl::c_find_if(instruction->control_successors(), is_constant) !=\n",
      "            instruction->control_successors().end();\n",
      "      }\n",
      "    }\n",
      "    if (contains_constant_successor_or_predecessors) {\n",
      "      continue;\n",
      "    }\n",
      "    HloInstructionSequence new_sequence;\n",
      "\n",
      "    for (HloInstruction* instruction : sequence.instructions()) {\n",
      "      if (instruction->opcode() == HloOpcode::kConstant) {\n",
      "        new_sequence.push_back(instruction);\n",
      "      }\n",
      "    }\n",
      "    for (HloInstruction* instruction : sequence.instructions()) {\n",
      "      if (instruction->opcode() != HloOpcode::kConstant) {\n",
      "        new_sequence.push_back(instruction);\n",
      "      }\n",
      "    }\n",
      "    CHECK_EQ(new_sequence.size(), sequence.size());\n",
      "    schedule.set_sequence(computation, new_sequence);\n",
      "  }\n",
      "}\n",
      "\n",
      "}  // namespace xla\n",
      "\n",
      "\n",
      "\n",
      "Example 4:\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/lite/delegates/gpu/common/tasks/add_test_util.h\"\n",
      "\n",
      "#include \"tensorflow/lite/delegates/gpu/common/operations.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/status.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/task/testing_util.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/tasks/add.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace gpu {\n",
      "\n",
      "absl::Status AddTwoEqualTensorsTest(TestExecutionEnvironment* env) {\n",
      "  TensorFloat32 src0, src1;\n",
      "  src0.shape = BHWC(1, 2, 1, 2);\n",
      "  src0.data = {0.0f, -1.0f, -0.05f, 0.045f};\n",
      "  src1.shape = BHWC(1, 2, 1, 2);\n",
      "  src1.data = {0.0f, 1.0f, -0.05f, -0.045f};\n",
      "  std::vector<int> channels = {2, 2};\n",
      "\n",
      "  for (auto storage : env->GetSupportedStorages()) {\n",
      "    for (auto precision : env->GetSupportedPrecisions()) {\n",
      "      const float eps = precision == CalculationsPrecision::F32 ? 1e-6f : 1e-3f;\n",
      "      OperationDef op_def;\n",
      "      op_def.precision = precision;\n",
      "      auto data_type = DeduceDataTypeFromPrecision(precision);\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.dst_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      TensorFloat32 dst_tensor;\n",
      "      GPUOperation operation = CreateAdd(op_def, channels, channels[0]);\n",
      "      RETURN_IF_ERROR(env->ExecuteGPUOperation(\n",
      "          {src0, src1}, absl::make_unique<GPUOperation>(std::move(operation)),\n",
      "          BHWC(1, 2, 1, 2), &dst_tensor));\n",
      "      RETURN_IF_ERROR(\n",
      "          PointWiseNear({0.0f, 0.0f, -0.1f, 0.0f}, dst_tensor.data, eps));\n",
      "    }\n",
      "  }\n",
      "  return absl::OkStatus();\n",
      "}\n",
      "\n",
      "absl::Status AddFirstTensorHasMoreChannelsThanSecondTest(\n",
      "    TestExecutionEnvironment* env) {\n",
      "  TensorFloat32 src0, src1;\n",
      "  src0.shape = BHWC(1, 2, 1, 6);\n",
      "  src0.data = {0.0f,   -1.0f,  -0.05f, 0.045f, 1.0f,   -2.0f,\n",
      "               -1.05f, 1.045f, 2.0f,   -3.0f,  -2.05f, 2.045f};\n",
      "  src1.shape = BHWC(1, 2, 1, 2);\n",
      "  src1.data = {0.0f, 1.0f, -0.05f, -0.045f};\n",
      "  std::vector<int> channels = {6, 2};\n",
      "  for (auto storage : env->GetSupportedStorages()) {\n",
      "    for (auto precision : env->GetSupportedPrecisions()) {\n",
      "      const float eps = precision == CalculationsPrecision::F32 ? 1e-6f : 1e-3f;\n",
      "      OperationDef op_def;\n",
      "      op_def.precision = precision;\n",
      "      auto data_type = DeduceDataTypeFromPrecision(precision);\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.dst_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      TensorFloat32 dst_tensor;\n",
      "      GPUOperation operation = CreateAdd(op_def, channels, channels[0]);\n",
      "      RETURN_IF_ERROR(env->ExecuteGPUOperation(\n",
      "          {src0, src1}, absl::make_unique<GPUOperation>(std::move(operation)),\n",
      "          BHWC(1, 2, 1, 6), &dst_tensor));\n",
      "      RETURN_IF_ERROR(PointWiseNear({0.0f, 0.0f, -0.05f, 0.045f, 1.0f, -2.0f,\n",
      "                                     -1.1f, 1.0f, 2.0f, -3.0f, -2.05f, 2.045f},\n",
      "                                    dst_tensor.data, eps));\n",
      "    }\n",
      "  }\n",
      "  return absl::OkStatus();\n",
      "}\n",
      "\n",
      "absl::Status AddFirstTensorHasLessChannelsThanSecond(\n",
      "    TestExecutionEnvironment* env) {\n",
      "  TensorFloat32 src0, src1;\n",
      "  src1.shape = BHWC(1, 2, 1, 6);\n",
      "  src1.data = {0.0f,   -1.0f,  -0.05f, 0.045f, 1.0f,   -2.0f,\n",
      "               -1.05f, 1.045f, 2.0f,   -3.0f,  -2.05f, 2.045f};\n",
      "  src0.shape = BHWC(1, 2, 1, 2);\n",
      "  src0.data = {0.0f, 1.0f, -0.05f, -0.045f};\n",
      "  std::vector<int> channels = {2, 6};\n",
      "  for (auto storage : env->GetSupportedStorages()) {\n",
      "    for (auto precision : env->GetSupportedPrecisions()) {\n",
      "      const float eps = precision == CalculationsPrecision::F32 ? 1e-6f : 1e-3f;\n",
      "      OperationDef op_def;\n",
      "      op_def.precision = precision;\n",
      "      auto data_type = DeduceDataTypeFromPrecision(precision);\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.dst_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      TensorFloat32 dst_tensor;\n",
      "      GPUOperation operation = CreateAdd(op_def, channels, 6);\n",
      "      RETURN_IF_ERROR(env->ExecuteGPUOperation(\n",
      "          {src0, src1}, absl::make_unique<GPUOperation>(std::move(operation)),\n",
      "          BHWC(1, 2, 1, 6), &dst_tensor));\n",
      "      RETURN_IF_ERROR(PointWiseNear({0.0f, 0.0f, -0.05f, 0.045f, 1.0f, -2.0f,\n",
      "                                     -1.1f, 1.0f, 2.0f, -3.0f, -2.05f, 2.045f},\n",
      "                                    dst_tensor.data, eps));\n",
      "    }\n",
      "  }\n",
      "  return absl::OkStatus();\n",
      "}\n",
      "\n",
      "}  // namespace gpu\n",
      "}  // namespace tflite\n",
      "\n",
      "\n",
      "\n",
      "Example 5:\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/core/framework/common_shape_fns.h\"\n",
      "#include \"tensorflow/core/framework/op.h\"\n",
      "#include \"tensorflow/core/framework/shape_inference.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace {\n",
      "\n",
      "// TODO(kttian): Support non-scalar values\n",
      "REGISTER_OP(\"EmptyTensorMap\")\n",
      "    .Output(\"handle: variant\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapSize\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Output(\"size: int32\")\n",
      "    .SetShapeFn(shape_inference::ScalarShape);\n",
      "\n",
      "REGISTER_OP(\"TensorMapLookup\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"value: value_dtype\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->UnknownShape());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapInsert\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Input(\"value: value_dtype\")\n",
      "    .Output(\"output_handle: variant\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapErase\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"output_handle: variant\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());  // output map\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapHasKey\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"has_key: bool\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .SetShapeFn(shape_inference::ScalarShape);\n",
      "\n",
      "REGISTER_OP(\"TensorMapStackKeys\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Output(\"keys: key_dtype\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->UnknownShape());  // output keys\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "}  // namespace\n",
      "}  // namespace tensorflow\n",
      "\n",
      "\n",
      "\n",
      "Example 6:\n",
      "<gh_stars>1000+\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#include \"tensorflow/c/eager/unified_api_testutil.h\"\n",
      "\n",
      "#include \"absl/container/flat_hash_set.h\"\n",
      "#include \"tensorflow/c/eager/c_api_experimental.h\"\n",
      "#include \"tensorflow/c/eager/c_api_test_util.h\"\n",
      "#include \"tensorflow/c/eager/c_api_unified_experimental.h\"\n",
      "#include \"tensorflow/c/eager/c_api_unified_experimental_internal.h\"\n",
      "#include \"tensorflow/c/tf_status.h\"\n",
      "#include \"tensorflow/c/tf_status_helper.h\"\n",
      "#include \"tensorflow/core/framework/tensor_shape.h\"\n",
      "#include \"tensorflow/core/lib/llvm_rtti/llvm_rtti.h\"\n",
      "#include \"tensorflow/core/platform/errors.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "AbstractContext* BuildFunction(const char* fn_name) {\n",
      "  std::unique_ptr<TF_Status, decltype(&TF_DeleteStatus)> status(\n",
      "      TF_NewStatus(), TF_DeleteStatus);\n",
      "  TF_ExecutionContext* graph_ctx = TF_CreateFunction(fn_name, status.get());\n",
      "  return unwrap(graph_ctx);\n",
      "}\n",
      "\n",
      "Status CreateParamsForInputs(AbstractContext* ctx,\n",
      "                             absl::Span<AbstractTensorHandle* const> inputs,\n",
      "                             std::vector<AbstractTensorHandle*>* params) {\n",
      "  tracing::TracingTensorHandle* handle = nullptr;\n",
      "  for (auto input : inputs) {\n",
      "    PartialTensorShape shape;\n",
      "    TF_RETURN_IF_ERROR(input->Shape(&shape));\n",
      "    TF_RETURN_IF_ERROR(dyn_cast<tracing::TracingContext>(ctx)->AddParameter(\n",
      "        input->DataType(), shape, &handle));\n",
      "    params->emplace_back(handle);\n",
      "  }\n",
      "  return Status::OK();\n",
      "}\n",
      "\n",
      "// Runs `model` maybe wrapped in a function.\n",
      "Status RunModel(Model model, AbstractContext* ctx,\n",
      "                absl::Span<AbstractTensorHandle* const> inputs,\n",
      "                absl::Span<AbstractTensorHandle*> outputs, bool use_function) {\n",
      "  if (use_function) {\n",
      "    const char* fn_name = \"test_fn\";\n",
      "    core::RefCountPtr<AbstractFunction> scoped_func;\n",
      "    // Returning null tensors from a tf.function is not supported, so we keep\n",
      "    // track of indices in the model's outputs are nullptr in this set.\n",
      "    // The FunctionDef only outputs the non-null tensors. We later pad the\n",
      "    // function op outputs to have nullptrs at the `null_indices`.\n",
      "    absl::flat_hash_set<int> null_indices;\n",
      "    {\n",
      "      AbstractContextPtr func_ctx(BuildFunction(fn_name));\n",
      "      std::vector<AbstractTensorHandle*> func_inputs;\n",
      "      func_inputs.reserve(inputs.size());\n",
      "      TF_RETURN_IF_ERROR(\n",
      "          CreateParamsForInputs(func_ctx.get(), inputs, &func_inputs));\n",
      "      std::vector<AbstractTensorHandle*> model_outputs;\n",
      "      model_outputs.resize(outputs.size());\n",
      "      TF_RETURN_IF_ERROR(model(func_ctx.get(), absl::MakeSpan(func_inputs),\n",
      "                               absl::MakeSpan(model_outputs)));\n",
      "      for (auto func_input : func_inputs) {\n",
      "        func_input->Unref();\n",
      "      }\n",
      "      AbstractFunction* func = nullptr;\n",
      "      OutputList output_list;\n",
      "      output_list.expected_num_outputs = 0;\n",
      "      output_list.outputs.reserve(outputs.size());\n",
      "      for (int i = 0; i < model_outputs.size(); i++) {\n",
      "        if (model_outputs[i]) {\n",
      "          output_list.outputs.emplace_back(model_outputs[i]);\n",
      "          output_list.expected_num_outputs += 1;\n",
      "        } else {\n",
      "          null_indices.insert(i);\n",
      "        }\n",
      "      }\n",
      "      TF_RETURN_IF_ERROR(dyn_cast<tracing::TracingContext>(func_ctx.get())\n",
      "                             ->Finalize(&output_list, &func));\n",
      "      scoped_func.reset(func);\n",
      "      for (auto output : output_list.outputs) {\n",
      "        output->Unref();\n",
      "      }\n",
      "      TF_RETURN_IF_ERROR(ctx->RegisterFunction(func));\n",
      "    }\n",
      "\n",
      "    AbstractOperationPtr fn_op(ctx->CreateOperation());\n",
      "    TF_RETURN_IF_ERROR(fn_op->Reset(fn_name, /*raw_device_name=*/nullptr));\n",
      "    for (auto input : inputs) {\n",
      "      TF_RETURN_IF_ERROR(fn_op->AddInput(input));\n",
      "    }\n",
      "    int retvals = outputs.size() - null_indices.size();\n",
      "    std::vector<AbstractTensorHandle*> fn_outputs(retvals);\n",
      "    TF_RETURN_IF_ERROR(fn_op->Execute(\n",
      "        absl::Span<AbstractTensorHandle*>(fn_outputs.data(), fn_outputs.size()),\n",
      "        &retvals));\n",
      "    int skipped_indices = 0;\n",
      "    for (int i = 0; i < outputs.size(); i++) {\n",
      "      if (!null_indices.contains(i)) {\n",
      "        outputs[i] = fn_outputs[i - skipped_indices];\n",
      "      } else {\n",
      "        skipped_indices += 1;\n",
      "      }\n",
      "    }\n",
      "    TF_RETURN_IF_ERROR(ctx->RemoveFunction(fn_name));\n",
      "    return Status::OK();\n",
      "  } else {\n",
      "    return model(ctx, inputs, outputs);\n",
      "  }\n",
      "}\n",
      "\n",
      "Status BuildImmediateExecutionContext(bool use_tfrt, AbstractContext** ctx) {\n",
      "  std::unique_ptr<TF_Status, decltype(&TF_DeleteStatus)> status(\n",
      "      TF_NewStatus(), TF_DeleteStatus);\n",
      "  TFE_ContextOptions* opts = TFE_NewContextOptions();\n",
      "  TFE_ContextOptionsSetTfrt(opts, use_tfrt);\n",
      "  *ctx = unwrap(TF_NewEagerExecutionContext(opts, status.get()));\n",
      "  TF_RETURN_IF_ERROR(StatusFromTF_Status(status.get()));\n",
      "  TFE_DeleteContextOptions(opts);\n",
      "  return Status::OK();\n",
      "}\n",
      "\n",
      "Status GetValue(AbstractTensorHandle* t, TF_Tensor** result_tensor) {\n",
      "  std::unique_ptr<TF_Status, decltype(&TF_DeleteStatus)> status(\n",
      "      TF_NewStatus(), TF_DeleteStatus);\n",
      "  TFE_TensorHandle* result_t =\n",
      "      TF_AbstractTensorGetEagerTensor(wrap(t), status.get());\n",
      "  TF_RETURN_IF_ERROR(StatusFromTF_Status(status.get()));\n",
      "  *result_tensor = TFE_TensorHandleResolve(result_t, status.get());\n",
      "  return StatusFromTF_Status(status.get());\n",
      "}\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "\n",
      "\n",
      "Example 7:\n",
      "<filename>tensorflow/c/experimental/saved_model/core/revived_types/asset.cc\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/c/experimental/saved_model/core/revived_types/asset.h\"\n",
      "\n",
      "#include <string>\n",
      "\n",
      "#include \"tensorflow/c/eager/immediate_execution_context.h\"\n",
      "#include \"tensorflow/c/eager/immediate_execution_tensor_handle.h\"\n",
      "#include \"tensorflow/c/tensor_interface.h\"\n",
      "#include \"tensorflow/cc/saved_model/constants.h\"\n",
      "#include \"tensorflow/core/platform/errors.h\"\n",
      "#include \"tensorflow/core/platform/path.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "Asset::Asset(ImmediateTensorHandlePtr handle)\n",
      "    : TensorHandleConvertible(std::move(handle)) {}\n",
      "\n",
      "Status Asset::Create(ImmediateExecutionContext* ctx,\n",
      "                     const std::string& saved_model_dir,\n",
      "                     const std::string& asset_filename,\n",
      "                     std::unique_ptr<Asset>* output) {\n",
      "  std::string abs_path =\n",
      "      io::JoinPath(saved_model_dir, kSavedModelAssetsDirectory, asset_filename);\n",
      "  AbstractTensorPtr tensor(ctx->CreateStringScalar(abs_path));\n",
      "  if (tensor.get() == nullptr) {\n",
      "    return errors::Internal(\n",
      "        \"Failed to create scalar string tensor for Asset at path \", abs_path);\n",
      "  }\n",
      "\n",
      "  ImmediateTensorHandlePtr handle(ctx->CreateLocalHandle(tensor.get()));\n",
      "  output->reset(new Asset(std::move(handle)));\n",
      "  return Status();\n",
      "}\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "\n",
      "\n",
      "Example 8:\n",
      "<filename>tensorflow/core/tfrt/eager/backends/gpu/gpu_registration.cc\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/core/common_runtime/device/device_id.h\"\n",
      "#include \"tensorflow/core/common_runtime/device_mgr.h\"\n",
      "#include \"tensorflow/core/common_runtime/gpu/gpu_id_manager.h\"\n",
      "#include \"tensorflow/core/runtime_fallback/runtime/runtime_fallback_op_handler.h\"\n",
      "#include \"tensorflow/core/tfrt/eager/core_runtime/op_handler_registry.h\"\n",
      "#include \"tfrt/gpu/core_runtime/gpu_op_handler.h\"  // from @tf_runtime\n",
      "#include \"tfrt/gpu/device/device.h\"  // from @tf_runtime\n",
      "#include \"tfrt/gpu/device/device_util.h\"  // from @tf_runtime\n",
      "#include \"tfrt/core_runtime/core_runtime.h\"  // from @tf_runtime\n",
      "#include \"tfrt/host_context/resource_context.h\"  // from @tf_runtime\n",
      "\n",
      "namespace tfrt {\n",
      "namespace tf {\n",
      "namespace gpu {\n",
      "\n",
      "using ::tfrt::CoreRuntime;\n",
      "\n",
      "static void RegisterGpuOpHandler(CoreRuntime* core_runtime,\n",
      "                                 ResourceContext* resource_context,\n",
      "                                 const DeviceMgr* device_mgr) {\n",
      "  for (auto& device : device_mgr->ListDevices()) {\n",
      "    auto& parsed_name = device->parsed_name();\n",
      "    assert(parsed_name.has_id && parsed_name.has_type);\n",
      "    if (parsed_name.type == \"GPU\") {\n",
      "      // Please see the difference between tf_device_id and platform_device_id\n",
      "      // here in tensorflow/core/common_runtime/device/device_id.h\n",
      "      tensorflow::TfDeviceId tf_device_id(parsed_name.id);\n",
      "      tensorflow::PlatformDeviceId platform_device_id;\n",
      "      tensorflow::Status s = tensorflow::GpuIdManager::TfToPlatformDeviceId(\n",
      "          tf_device_id, &platform_device_id);\n",
      "      if (!s.ok()) {\n",
      "        LOG(ERROR) << \"Failed to convert gpu device [\" << device->name()\n",
      "                   << \"] to platform device id due to error: \"\n",
      "                   << s.error_message();\n",
      "        continue;\n",
      "      }\n",
      "      auto gpu = tfrt::gpu::GetOrCreateGpuDevice(\n",
      "          device->name(), platform_device_id.value(),\n",
      "          core_runtime->GetHostContext());\n",
      "      if (!gpu) {\n",
      "        LOG(ERROR) << \"Failed to create gpu device [\" << device->name()\n",
      "                   << \"]. Error: \" << StrCat(gpu.takeError());\n",
      "        continue;\n",
      "      }\n",
      "      LOG(INFO) << \"Found a GPU device: \" << device->name();\n",
      "      auto expected_fallback_op_handler =\n",
      "          tensorflow::tfd::CreateRuntimeFallbackOpHandler(core_runtime,\n",
      "                                                          device->name());\n",
      "      assert(expected_fallback_op_handler);\n",
      "\n",
      "      auto expected_gpu_op_handler =\n",
      "          ::tfrt::gpu::CreateGpuOpHandler(core_runtime, std::move(gpu.get()),\n",
      "                                          expected_fallback_op_handler.get());\n",
      "      assert(expected_gpu_op_handler);\n",
      "\n",
      "      core_runtime->RegisterOpHandler(device->name(),\n",
      "                                      expected_gpu_op_handler.get());\n",
      "\n",
      "      // TODO(fishx): Remove this when lowering pass can use full device name.\n",
      "      if (parsed_name.id == 0) {\n",
      "        core_runtime->RegisterOpHandler(\"gpu\", expected_gpu_op_handler.get());\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "static OpHandlerRegistration register_gpu(RegisterGpuOpHandler);\n",
      "\n",
      "}  // namespace gpu\n",
      "}  // namespace tf\n",
      "}  // namespace tfrt\n",
      "\n",
      "\n",
      "\n",
      "Example 9:\n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#include \"tensorflow/core/kernels/data/experimental/random_dataset_op.h\"\n",
      "\n",
      "#include \"tensorflow/core/data/dataset_test_base.h\"\n",
      "#include \"tensorflow/core/lib/random/philox_random.h\"\n",
      "#include \"tensorflow/core/lib/random/random_distributions.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace data {\n",
      "namespace experimental {\n",
      "namespace {\n",
      "\n",
      "constexpr char kNodeName[] = \"random_dataset\";\n",
      "constexpr char kIteratorPrefix[] = \"Iterator\";\n",
      "\n",
      "// Number of random samples generated per test\n",
      "constexpr int kCount = 10;\n",
      "\n",
      "// Generate the first `count` random numbers that the kernel should produce\n",
      "// for a given seed/seed2 combo.\n",
      "// For compatibility with the test harness, return value is a vector of scalar\n",
      "// Tensors.\n",
      "std::vector<Tensor> GenerateExpectedData(int64_t seed, int64_t seed2,\n",
      "                                         int count) {\n",
      "  std::vector<Tensor> ret;\n",
      "  auto parent_generator = random::PhiloxRandom(seed, seed2);\n",
      "  auto generator =\n",
      "      random::SingleSampleAdapter<random::PhiloxRandom>(&parent_generator);\n",
      "\n",
      "  for (int i = 0; i < count; ++i) {\n",
      "    ret.push_back(CreateTensor<int64_t>(TensorShape({}), {generator()}));\n",
      "  }\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "class RandomDatasetParams : public DatasetParams {\n",
      " public:\n",
      "  RandomDatasetParams(int64_t seed, int64_t seed2, DataTypeVector output_dtypes,\n",
      "                      std::vector<PartialTensorShape> output_shapes,\n",
      "                      string node_name)\n",
      "      : DatasetParams(std::move(output_dtypes), std::move(output_shapes),\n",
      "                      std::move(node_name)),\n",
      "        seed_(CreateTensor<int64_t>(TensorShape({}), {seed})),\n",
      "        seed2_(CreateTensor<int64_t>(TensorShape({}), {seed2})) {}\n",
      "\n",
      "  virtual std::vector<Tensor> GetInputTensors() const override {\n",
      "    return {seed_, seed2_};\n",
      "  }\n",
      "\n",
      "  virtual Status GetInputNames(\n",
      "      std::vector<string>* input_names) const override {\n",
      "    *input_names = {RandomDatasetOp::kSeed, RandomDatasetOp::kSeed2};\n",
      "    return Status::OK();\n",
      "  }\n",
      "\n",
      "  virtual Status GetAttributes(AttributeVector* attributes) const override {\n",
      "    *attributes = {{\"output_types\", output_dtypes_},\n",
      "                   {\"output_shapes\", output_shapes_},\n",
      "                   {\"metadata\", \"\"}};\n",
      "    return Status::OK();\n",
      "  }\n",
      "\n",
      "  virtual string dataset_type() const override {\n",
      "    return RandomDatasetOp::kDatasetType;\n",
      "  }\n",
      "\n",
      " private:\n",
      "  Tensor seed_;\n",
      "  Tensor seed2_;\n",
      "};\n",
      "\n",
      "class RandomDatasetOpTest : public DatasetOpsTestBase {};\n",
      "\n",
      "RandomDatasetParams FortyTwo() {\n",
      "  return {/*seed=*/42,\n",
      "          /*seed2=*/42,\n",
      "          /*output_dtypes=*/{DT_INT64},\n",
      "          /*output_shapes=*/{PartialTensorShape({})},\n",
      "          /*node_name=*/kNodeName};\n",
      "}\n",
      "\n",
      "// Change just first seed relative to FortyTwo\n",
      "RandomDatasetParams ChangeSeed() {\n",
      "  return {/*seed=*/1000,\n",
      "          /*seed2=*/42,\n",
      "          /*output_dtypes=*/{DT_INT64},\n",
      "          /*output_shapes=*/{PartialTensorShape({})},\n",
      "          /*node_name=*/kNodeName};\n",
      "}\n",
      "\n",
      "// Change just second seed relative to FortyTwo\n",
      "RandomDatasetParams ChangeSeed2() {\n",
      "  return {/*seed=*/42,\n",
      "          /*seed2=*/1000,\n",
      "          /*output_dtypes=*/{DT_INT64},\n",
      "          /*output_shapes=*/{PartialTensorShape({})},\n",
      "          /*node_name=*/kNodeName};\n",
      "}\n",
      "\n",
      "class ParameterizedGetNextTest : public RandomDatasetOpTest,\n",
      "                                 public ::testing::WithParamInterface<\n",
      "                                     GetNextTestCase<RandomDatasetParams>> {};\n",
      "\n",
      "std::vector<GetNextTestCase<RandomDatasetParams>> GetNextTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_outputs=*/GenerateExpectedData(42, 42, kCount)},\n",
      "          {/*dataset_params=*/ChangeSeed(),\n",
      "           /*expected_outputs=*/GenerateExpectedData(1000, 42, kCount)},\n",
      "          {/*dataset_params=*/ChangeSeed2(),\n",
      "           /*expected_outputs=*/GenerateExpectedData(42, 1000, kCount)}};\n",
      "}\n",
      "\n",
      "TEST_P(ParameterizedGetNextTest, GetNext) {\n",
      "  auto test_case = GetParam();\n",
      "  TF_ASSERT_OK(Initialize(test_case.dataset_params));\n",
      "\n",
      "  // Can't use DatasetOpsTestBase::CheckIteratorGetNext because the kernel\n",
      "  // under test produces unbounded input.\n",
      "  bool end_of_sequence = false;\n",
      "  std::vector<Tensor> out_tensors;\n",
      "  while (out_tensors.size() < test_case.expected_outputs.size()) {\n",
      "    std::vector<Tensor> next;\n",
      "    TF_ASSERT_OK(\n",
      "        iterator_->GetNext(iterator_ctx_.get(), &next, &end_of_sequence));\n",
      "\n",
      "    ASSERT_FALSE(end_of_sequence);  // Dataset should never stop\n",
      "\n",
      "    out_tensors.insert(out_tensors.end(), next.begin(), next.end());\n",
      "  }\n",
      "\n",
      "  TF_ASSERT_OK(ExpectEqual(out_tensors, test_case.expected_outputs,\n",
      "                           /*compare_order=*/true));\n",
      "}\n",
      "\n",
      "INSTANTIATE_TEST_SUITE_P(\n",
      "    RandomDatasetOpTest, ParameterizedGetNextTest,\n",
      "    ::testing::ValuesIn(\n",
      "        std::vector<GetNextTestCase<RandomDatasetParams>>(GetNextTestCases())));\n",
      "\n",
      "std::vector<DatasetNodeNameTestCase<RandomDatasetParams>>\n",
      "DatasetNodeNameTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(), /*expected_node_name=*/kNodeName}};\n",
      "}\n",
      "\n",
      "DATASET_NODE_NAME_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                         DatasetNodeNameTestCases());\n",
      "\n",
      "std::vector<DatasetTypeStringTestCase<RandomDatasetParams>>\n",
      "DatasetTypeStringTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_dataset_type_string=*/name_utils::OpName(\n",
      "               RandomDatasetOp::kDatasetType)}};\n",
      "}\n",
      "\n",
      "DATASET_TYPE_STRING_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                           DatasetTypeStringTestCases());\n",
      "\n",
      "std::vector<DatasetOutputDtypesTestCase<RandomDatasetParams>>\n",
      "DatasetOutputDtypesTestCases() {\n",
      "  return {\n",
      "      {/*dataset_params=*/FortyTwo(), /*expected_output_dtypes=*/{DT_INT64}}};\n",
      "}\n",
      "\n",
      "DATASET_OUTPUT_DTYPES_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                             DatasetOutputDtypesTestCases());\n",
      "\n",
      "std::vector<DatasetOutputShapesTestCase<RandomDatasetParams>>\n",
      "DatasetOutputShapesTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_output_shapes=*/{PartialTensorShape({})}}};\n",
      "}\n",
      "\n",
      "DATASET_OUTPUT_SHAPES_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                             DatasetOutputShapesTestCases());\n",
      "\n",
      "std::vector<CardinalityTestCase<RandomDatasetParams>> CardinalityTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_cardinality=*/kInfiniteCardinality}};\n",
      "}\n",
      "\n",
      "DATASET_CARDINALITY_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                           CardinalityTestCases());\n",
      "\n",
      "std::vector<IteratorOutputDtypesTestCase<RandomDatasetParams>>\n",
      "IteratorOutputDtypesTestCases() {\n",
      "  return {\n",
      "      {/*dataset_params=*/FortyTwo(), /*expected_output_dtypes=*/{DT_INT64}}};\n",
      "}\n",
      "\n",
      "ITERATOR_OUTPUT_DTYPES_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                              IteratorOutputDtypesTestCases());\n",
      "\n",
      "std::vector<IteratorOutputShapesTestCase<RandomDatasetParams>>\n",
      "IteratorOutputShapesTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_output_shapes=*/{PartialTensorShape({})}}};\n",
      "}\n",
      "\n",
      "ITERATOR_OUTPUT_SHAPES_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                              IteratorOutputShapesTestCases());\n",
      "\n",
      "std::vector<IteratorPrefixTestCase<RandomDatasetParams>>\n",
      "IteratorOutputPrefixTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_iterator_prefix=*/name_utils::IteratorPrefix(\n",
      "               RandomDatasetOp::kDatasetType, kIteratorPrefix)}};\n",
      "}\n",
      "\n",
      "ITERATOR_PREFIX_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                       IteratorOutputPrefixTestCases());\n",
      "\n",
      "std::vector<IteratorSaveAndRestoreTestCase<RandomDatasetParams>>\n",
      "IteratorSaveAndRestoreTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(), /*breakpoints=*/{2, 5, 8},\n",
      "           /*expected_outputs=*/GenerateExpectedData(42, 42, 9 /* 8 + 1 */)}};\n",
      "}\n",
      "\n",
      "ITERATOR_SAVE_AND_RESTORE_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                                 IteratorSaveAndRestoreTestCases());\n",
      "\n",
      "}  // namespace\n",
      "}  // namespace experimental\n",
      "}  // namespace data\n",
      "}  // namespace tensorflow\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 examples\n",
    "for i, example in enumerate(dataset):\n",
    "    if i < 10:\n",
    "        print(f\"Example {i}:\")\n",
    "        print(example['content'])\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4ee247d6af434383e5e4be8f4a9381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6353527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "/**\n",
      " * Copyright (c) Facebook, Inc. and its affiliates.\n",
      " *\n",
      " * This source code is licensed under the MIT license found in the\n",
      " * LICENSE file in the root directory of this source tree.\n",
      " */\n",
      "\n",
      "#include \"thread-local.h\"\n",
      "\n",
      "#ifdef __linux__\n",
      "#include <link.h>\n",
      "#include <asm/prctl.h>\n",
      "#include <sys/prctl.h>\n",
      "extern \"C\" {\n",
      "extern int arch_prctl(int, unsigned long*);\n",
      "}\n",
      "#endif //__linux__\n",
      "\n",
      "namespace HPHP {\n",
      "\n",
      "#ifdef USE_GCC_FAST_TLS\n",
      "\n",
      "void ThreadLocalManager::OnThreadExit(void* p) {\n",
      "  auto list = getList(p);\n",
      "  p = list->head;\n",
      "  delete list;\n",
      "  while (p != nullptr) {\n",
      "    auto* pNode = static_cast<ThreadLocalNode<void>*>(p);\n",
      "    if (pNode->m_on_thread_exit_fn) {\n",
      "      pNode->m_on_thread_exit_fn(p);\n",
      "    }\n",
      "    p = pNode->m_next;\n",
      "  }\n",
      "}\n",
      "\n",
      "void ThreadLocalManager::PushTop(void* nodePtr, size_t nodeSize) {\n",
      "  auto& node = *static_cast<ThreadLocalNode<void>*>(nodePtr);\n",
      "  auto key = GetManager().m_key;\n",
      "  auto list = getList(pthread_getspecific(key));\n",
      "  if (UNLIKELY(!list)) {\n",
      "    ThreadLocalSetValue(key, list = new ThreadLocalList);\n",
      "  }\n",
      "  node.m_next = list->head;\n",
      "  node.m_size = nodeSize;\n",
      "  list->head = node.m_next;\n",
      "}\n",
      "\n",
      "ThreadLocalManager& ThreadLocalManager::GetManager() {\n",
      "  static ThreadLocalManager m;\n",
      "  return m;\n",
      "}\n",
      "\n",
      "#ifdef __APPLE__\n",
      "ThreadLocalManager::ThreadLocalList::ThreadLocalList() {\n",
      "  pthread_t self = pthread_self();\n",
      "  handler.__routine = ThreadLocalManager::OnThreadExit;\n",
      "  handler.__arg = this;\n",
      "  handler.__next = self->__cleanup_stack;\n",
      "  self->__cleanup_stack = &handler;\n",
      "}\n",
      "#endif\n",
      "\n",
      "#endif\n",
      "\n",
      "#ifdef __linux__\n",
      "\n",
      "static int visit_phdr(dl_phdr_info* info, size_t, void*) {\n",
      "  for (size_t i = 0, n = info->dlpi_phnum; i < n; ++i) {\n",
      "    const auto& hdr = info->dlpi_phdr[i];\n",
      "    auto addr = info->dlpi_addr + hdr.p_vaddr;\n",
      "    if (addr < 0x100000000LL && hdr.p_type == PT_TLS) {\n",
      "      // found the main thread-local section\n",
      "      assert(int(hdr.p_memsz) == hdr.p_memsz); // ensure no truncation\n",
      "      return hdr.p_memsz;\n",
      "    }\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "std::pair<void*,size_t> getCppTdata() {\n",
      "  uintptr_t addr;\n",
      "  if (!arch_prctl(ARCH_GET_FS, &addr)) {\n",
      "    // fs points to the end of the threadlocal area.\n",
      "    size_t size = dl_iterate_phdr(&visit_phdr, nullptr);\n",
      "    return {(void*)(addr - size), size};\n",
      "  }\n",
      "  return {nullptr, 0};\n",
      "}\n",
      "\n",
      "#else\n",
      "\n",
      "// how do you find the thread local section on your system?\n",
      "std::pair<void*,size_t> getCppTdata() {\n",
      "  return {nullptr, 0};\n",
      "}\n",
      "\n",
      "#endif //__linux__\n",
      "\n",
      "}\n",
      "\n",
      "Average Line Length: 22.73\n",
      "\n",
      "Example 1:\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n",
      "\n",
      "#define EIGEN_USE_GPU\n",
      "\n",
      "#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n",
      "#include \"tensorflow/core/framework/register_types.h\"\n",
      "#include \"tensorflow/core/framework/tensor.h\"\n",
      "#include \"tensorflow/core/framework/tensor_shape.h\"\n",
      "#include \"tensorflow/core/kernels/gpu_device_array.h\"\n",
      "#include \"tensorflow/core/kernels/gpu_device_array_gpu.h\"\n",
      "#include \"tensorflow/core/kernels/gpu_prim_helpers.h\"\n",
      "#include \"tensorflow/core/kernels/sparse_concat_op.h\"\n",
      "#include \"tensorflow/core/lib/core/bits.h\"\n",
      "#include \"tensorflow/core/util/gpu_kernel_helper.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "typedef Eigen::GpuDevice GPUDevice;\n",
      "\n",
      "namespace functor {\n",
      "\n",
      "namespace {\n",
      "\n",
      "template <typename T>\n",
      "__global__ void SparseConcatKernel(\n",
      "    int64 output_nnz, int rank, int concat_dim, bool need_to_sort,\n",
      "    GpuDeviceArrayStruct<const int64*> ind_ptrs_data,\n",
      "    GpuDeviceArrayStruct<const T*> val_ptrs_data,\n",
      "    GpuDeviceArrayStruct<int64_t> nnz_scan_data,\n",
      "    GpuDeviceArrayStruct<int64_t> concat_size_scan_data,\n",
      "    GpuDeviceArrayStruct<int64_t> output_shape_data,\n",
      "    int64* __restrict__ output_inds, T* __restrict__ output_vals,\n",
      "    int64* __restrict__ output_flat_inds) {\n",
      "  const int64* __restrict__* __restrict__ ind_ptrs =\n",
      "      GetGpuDeviceArrayOnDevice(&ind_ptrs_data);\n",
      "  const T* __restrict__* __restrict__ val_ptrs =\n",
      "      GetGpuDeviceArrayOnDevice(&val_ptrs_data);\n",
      "  const int64* __restrict__ nnz_scan =\n",
      "      GetGpuDeviceArrayOnDevice(&nnz_scan_data);\n",
      "  const int64* __restrict__ concat_size_scan =\n",
      "      GetGpuDeviceArrayOnDevice(&concat_size_scan_data);\n",
      "  const int64* __restrict__ output_shape =\n",
      "      GetGpuDeviceArrayOnDevice(&output_shape_data);\n",
      "  const int64 num_inputs = ind_ptrs_data.size;\n",
      "\n",
      "  for (int64 nz : GpuGridRangeX<int64_t>(output_nnz)) {\n",
      "    const int64 input_num =\n",
      "        gpu_helper::upper_bound<int64_t>(nnz_scan, num_inputs, nz) - 1;\n",
      "    const int64 input_nz = nz - nnz_scan[input_num];\n",
      "    const int64 ind_offset = concat_size_scan[input_num];\n",
      "    if (!need_to_sort) {\n",
      "      output_vals[nz] = val_ptrs[input_num][input_nz];\n",
      "    }\n",
      "    int64 flat_ind = 0;\n",
      "    for (int j = 0; j < rank; ++j) {\n",
      "      const int64 output_ind = ind_ptrs[input_num][input_nz * rank + j] +\n",
      "                               (j == concat_dim ? ind_offset : 0);\n",
      "      if (!need_to_sort) {\n",
      "        output_inds[nz * rank + j] = output_ind;\n",
      "      } else {\n",
      "        flat_ind = flat_ind * output_shape[j] + output_ind;\n",
      "        output_flat_inds[nz] = flat_ind;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "template <typename T>\n",
      "__global__ void SparseConcatPermuteKernel(\n",
      "    int64 output_nnz, int rank, GpuDeviceArrayStruct<const T*> val_ptrs_data,\n",
      "    GpuDeviceArrayStruct<int64_t> nnz_scan_data,\n",
      "    GpuDeviceArrayStruct<int64_t> output_shape_data,\n",
      "    const int64* __restrict__ output_flat_inds,\n",
      "    const int64* __restrict__ permutation, int64* __restrict__ output_inds,\n",
      "    T* __restrict__ output_vals) {\n",
      "  const T* __restrict__* __restrict__ val_ptrs =\n",
      "      GetGpuDeviceArrayOnDevice(&val_ptrs_data);\n",
      "  const int64* __restrict__ nnz_scan =\n",
      "      GetGpuDeviceArrayOnDevice(&nnz_scan_data);\n",
      "  const int64* __restrict__ output_shape =\n",
      "      GetGpuDeviceArrayOnDevice(&output_shape_data);\n",
      "  const int64 num_inputs = val_ptrs_data.size;\n",
      "\n",
      "  for (int64 nz : GpuGridRangeX<int64_t>(output_nnz)) {\n",
      "    const int64 permuted_nz = permutation[nz];\n",
      "    const int64 input_num =\n",
      "        gpu_helper::upper_bound<int64_t>(nnz_scan, num_inputs, permuted_nz) - 1;\n",
      "    const int64 input_nz = permuted_nz - nnz_scan[input_num];\n",
      "    output_vals[nz] = val_ptrs[input_num][input_nz];\n",
      "    int64 output_flat_ind = output_flat_inds[permuted_nz];\n",
      "    for (int j = rank - 1; j >= 0; --j) {\n",
      "      const int64 output_dim_size = output_shape[j];\n",
      "      output_inds[nz * rank + j] = output_flat_ind % output_dim_size;\n",
      "      output_flat_ind /= output_dim_size;\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "}  // namespace\n",
      "\n",
      "template <typename T>\n",
      "struct SparseConcatFunctor<GPUDevice, T> {\n",
      "  void operator()(OpKernelContext* context, const OpInputList& inds,\n",
      "                  const OpInputList& vals, const OpInputList& shapes,\n",
      "                  int concat_dim) {\n",
      "    const int N = inds.size();\n",
      "    const TensorShape input_shape0(shapes[0].vec<int64_t>());\n",
      "    const int rank = input_shape0.dims();\n",
      "\n",
      "    // The input non-zeros are assumed to be sorted by increasing dimension\n",
      "    // number (i.e., row-major order), so if the concatenation is along the\n",
      "    // first dimension then they remain in order and we can directly compute the\n",
      "    // output indices and values. To concatenate along other dimensions, we\n",
      "    // first compute the flattened (1D) row-major output indices, then sort\n",
      "    // these to obtain the required permutation, and finally gather the permuted\n",
      "    // input values.\n",
      "\n",
      "    GpuDeviceArrayOnHost<const int64*> ind_ptrs(context, N);\n",
      "    GpuDeviceArrayOnHost<const T*> val_ptrs(context, N);\n",
      "    GpuDeviceArrayOnHost<int64_t> nnz_scan(context, N + 1);\n",
      "    GpuDeviceArrayOnHost<int64_t> concat_size_scan(context, N + 1);\n",
      "    OP_REQUIRES_OK(context, ind_ptrs.Init());\n",
      "    OP_REQUIRES_OK(context, val_ptrs.Init());\n",
      "    OP_REQUIRES_OK(context, nnz_scan.Init());\n",
      "    OP_REQUIRES_OK(context, concat_size_scan.Init());\n",
      "    int64 nnz_sum = 0;\n",
      "    int64 concat_size_sum = 0;\n",
      "    nnz_scan.Set(0, nnz_sum);\n",
      "    concat_size_scan.Set(0, concat_size_sum);\n",
      "    for (int i = 0; i < N; ++i) {\n",
      "      ind_ptrs.Set(i, inds[i].matrix<int64_t>().data());\n",
      "      val_ptrs.Set(i, vals[i].vec<T>().data());\n",
      "      nnz_sum += inds[i].dim_size(0);\n",
      "      nnz_scan.Set(i + 1, nnz_sum);\n",
      "      const TensorShape current_shape(shapes[i].vec<int64_t>());\n",
      "      concat_size_sum += current_shape.dim_size(concat_dim);\n",
      "      concat_size_scan.Set(i + 1, concat_size_sum);\n",
      "    }\n",
      "    OP_REQUIRES_OK(context, ind_ptrs.Finalize());\n",
      "    OP_REQUIRES_OK(context, val_ptrs.Finalize());\n",
      "    OP_REQUIRES_OK(context, nnz_scan.Finalize());\n",
      "    OP_REQUIRES_OK(context, concat_size_scan.Finalize());\n",
      "    const int64 output_nnz = nnz_sum;\n",
      "    const int64 output_concat_size = concat_size_sum;\n",
      "\n",
      "    const bool need_to_sort = concat_dim != 0;\n",
      "\n",
      "    GpuDeviceArrayOnHost<int64_t> output_shape(context, rank);\n",
      "    int64 output_dense_elements;\n",
      "    if (need_to_sort) {\n",
      "      OP_REQUIRES_OK(context, output_shape.Init());\n",
      "      output_dense_elements = 1;\n",
      "      for (int j = 0; j < rank; ++j) {\n",
      "        int64 output_dim_size =\n",
      "            j == concat_dim ? output_concat_size : input_shape0.dim_size(j);\n",
      "        output_shape.Set(j, output_dim_size);\n",
      "        output_dense_elements *= output_dim_size;\n",
      "      }\n",
      "      OP_REQUIRES_OK(context, output_shape.Finalize());\n",
      "    }\n",
      "\n",
      "    int64* output_inds_ptr = nullptr;\n",
      "    T* output_vals_ptr = nullptr;\n",
      "    int64* output_flat_inds_ptr = nullptr;\n",
      "    Tensor output_flat_inds;\n",
      "    if (need_to_sort) {\n",
      "      // SparseConcatKernel will (only) produce output_flat_inds.\n",
      "      OP_REQUIRES_OK(context,\n",
      "                     context->allocate_temp(DT_INT64, TensorShape({output_nnz}),\n",
      "                                            &output_flat_inds));\n",
      "      output_flat_inds_ptr = output_flat_inds.vec<int64_t>().data();\n",
      "    } else {\n",
      "      OP_REQUIRES_OK(\n",
      "          context, allocate_outputs(context, rank, output_nnz, &output_inds_ptr,\n",
      "                                    &output_vals_ptr));\n",
      "    }\n",
      "\n",
      "    const GPUDevice& device = context->eigen_gpu_device();\n",
      "\n",
      "    GpuLaunchConfig config = GetGpuLaunchConfig(\n",
      "        output_nnz, device, &SparseConcatKernel<T>,\n",
      "        /*dynamic_shared_memory_size=*/0, /*block_size_limit=*/0);\n",
      "    OP_REQUIRES_OK(\n",
      "        context, GpuLaunchKernel(\n",
      "                     SparseConcatKernel<T>, config.block_count,\n",
      "                     config.thread_per_block, 0, device.stream(), output_nnz,\n",
      "                     rank, concat_dim, need_to_sort, ind_ptrs.data(),\n",
      "                     val_ptrs.data(), nnz_scan.data(), concat_size_scan.data(),\n",
      "                     (need_to_sort ? output_shape.data()\n",
      "                                   : GpuDeviceArrayStruct<int64_t>()),\n",
      "                     output_inds_ptr, output_vals_ptr, output_flat_inds_ptr));\n",
      "\n",
      "    if (!need_to_sort) return;\n",
      "\n",
      "    OP_REQUIRES_OK(context,\n",
      "                   allocate_outputs(context, rank, output_nnz, &output_inds_ptr,\n",
      "                                    &output_vals_ptr));\n",
      "\n",
      "    Tensor permutation;\n",
      "    OP_REQUIRES_OK(context,\n",
      "                   context->allocate_temp(DT_INT64, TensorShape({output_nnz}),\n",
      "                                          &permutation));\n",
      "    int64* permutation_ptr = permutation.vec<int64_t>().data();\n",
      "    OP_REQUIRES_OK(\n",
      "        context,\n",
      "        GpuRadixSort(context, /*size=*/output_nnz,\n",
      "                     /*keys_in=*/output_flat_inds_ptr,\n",
      "                     /*keys_out=*/static_cast<int64*>(nullptr),\n",
      "                     /*indices_in=*/static_cast<const int64*>(nullptr),\n",
      "                     /*indices_out=*/permutation_ptr,\n",
      "                     /*num_bits=*/Log2Ceiling64(output_dense_elements)));\n",
      "\n",
      "    config = GetGpuLaunchConfig(\n",
      "        output_nnz, device, &SparseConcatPermuteKernel<T>,\n",
      "        /*dynamic_shared_memory_size=*/0, /*block_size_limit=*/0);\n",
      "    OP_REQUIRES_OK(\n",
      "        context,\n",
      "        GpuLaunchKernel(SparseConcatPermuteKernel<T>, config.block_count,\n",
      "                        config.thread_per_block, 0, device.stream(), output_nnz,\n",
      "                        rank, val_ptrs.data(), nnz_scan.data(),\n",
      "                        output_shape.data(), output_flat_inds_ptr,\n",
      "                        permutation_ptr, output_inds_ptr, output_vals_ptr));\n",
      "  }\n",
      "\n",
      " private:\n",
      "  Status allocate_outputs(OpKernelContext* context, int rank, int64 output_nnz,\n",
      "                          int64** output_inds_ptr, T** output_vals_ptr) const {\n",
      "    Tensor* output_inds = nullptr;\n",
      "    TF_RETURN_IF_ERROR(context->allocate_output(\n",
      "        0, TensorShape({output_nnz, rank}), &output_inds));\n",
      "    *output_inds_ptr = output_inds->matrix<int64_t>().data();\n",
      "    Tensor* output_vals = nullptr;\n",
      "    TF_RETURN_IF_ERROR(\n",
      "        context->allocate_output(1, TensorShape({output_nnz}), &output_vals));\n",
      "    *output_vals_ptr = output_vals->vec<T>().data();\n",
      "    return Status::OK();\n",
      "  }\n",
      "};\n",
      "\n",
      "#define DEFINE_SPARSE_CONCAT_FUNCTOR(T) \\\n",
      "  template struct SparseConcatFunctor<GPUDevice, T>;\n",
      "TF_CALL_POD_TYPES(DEFINE_SPARSE_CONCAT_FUNCTOR);\n",
      "\n",
      "#undef DEFINE_SPARSE_CONCAT_FUNCTOR\n",
      "\n",
      "}  // namespace functor\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n",
      "\n",
      "Average Line Length: 40.19924812030075\n",
      "\n",
      "Example 2:\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#include \"tensorflow/core/tpu/kernels/tpu_compilation_cache_grpc.h\"\n",
      "\n",
      "#include <functional>\n",
      "\n",
      "#include \"grpcpp/impl/codegen/async_stream.h\"\n",
      "#include \"grpcpp/impl/codegen/async_unary_call.h\"\n",
      "#include \"grpcpp/impl/codegen/channel_interface.h\"\n",
      "#include \"grpcpp/impl/codegen/client_callback.h\"\n",
      "#include \"grpcpp/impl/codegen/client_unary_call.h\"\n",
      "#include \"grpcpp/impl/codegen/method_handler.h\"\n",
      "#include \"grpcpp/impl/codegen/rpc_service_method.h\"\n",
      "#include \"grpcpp/impl/codegen/server_callback.h\"\n",
      "#include \"grpcpp/impl/codegen/service_type.h\"\n",
      "#include \"grpcpp/impl/codegen/sync_stream.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace tpu {\n",
      "\n",
      "static const char* grpcTpuCompilationCacheService_method_names[] = {\n",
      "#if defined(LIBTPU_ON_GCE)\n",
      "    \"/tensorflow.tpu.TpuCompilationCacheServiceExternal/GetTpuProgram\",\n",
      "#else  // LIBTPU_ON_GCE\n",
      "    \"/tensorflow.tpu.TpuCompilationCacheService/GetTpuProgram\",\n",
      "#endif  // LIBTPU_ON_GCE\n",
      "};\n",
      "\n",
      "std::unique_ptr<grpc::TpuCompilationCacheService::Stub>\n",
      "grpc::TpuCompilationCacheService::NewStub(\n",
      "    const std::shared_ptr< ::grpc::ChannelInterface>& channel,\n",
      "    const ::grpc::StubOptions& options) {\n",
      "  (void)options;\n",
      "  std::unique_ptr<grpc::TpuCompilationCacheService::Stub> stub(\n",
      "      new grpc::TpuCompilationCacheService::Stub(channel));\n",
      "  return stub;\n",
      "}\n",
      "\n",
      "grpc::TpuCompilationCacheService::Stub::Stub(\n",
      "    const std::shared_ptr< ::grpc::ChannelInterface>& channel)\n",
      "    : channel_(channel),\n",
      "      rpcmethod_get_tpu_program_(grpcTpuCompilationCacheService_method_names[0],\n",
      "                                 ::grpc::internal::RpcMethod::NORMAL_RPC,\n",
      "                                 channel) {}\n",
      "\n",
      "::grpc::Status grpc::TpuCompilationCacheService::Stub::GetTpuProgram(\n",
      "    ::grpc::ClientContext* context, const RequestType& request,\n",
      "    ResponseType* response) {\n",
      "  return ::grpc::internal::BlockingUnaryCall(\n",
      "      channel_.get(), rpcmethod_get_tpu_program_, context, request, response);\n",
      "}\n",
      "\n",
      "::grpc::ClientAsyncResponseReader<\n",
      "    grpc::TpuCompilationCacheService::ResponseType>*\n",
      "grpc::TpuCompilationCacheService::Stub::AsyncGetTpuProgramRaw(\n",
      "    ::grpc::ClientContext* context, const RequestType& request,\n",
      "    ::grpc::CompletionQueue* cq) {\n",
      "  return ::grpc::internal::ClientAsyncResponseReaderFactory<\n",
      "      ResponseType>::Create(channel_.get(), cq, rpcmethod_get_tpu_program_,\n",
      "                            context, request, true);\n",
      "}\n",
      "\n",
      "::grpc::ClientAsyncResponseReader<\n",
      "    grpc::TpuCompilationCacheService::ResponseType>*\n",
      "grpc::TpuCompilationCacheService::Stub::PrepareAsyncGetTpuProgramRaw(\n",
      "    ::grpc::ClientContext* context, const RequestType& request,\n",
      "    ::grpc::CompletionQueue* cq) {\n",
      "  return ::grpc::internal::ClientAsyncResponseReaderFactory<\n",
      "      ResponseType>::Create(channel_.get(), cq, rpcmethod_get_tpu_program_,\n",
      "                            context, request, false);\n",
      "}\n",
      "\n",
      "grpc::TpuCompilationCacheService::Service::Service() {\n",
      "  AddMethod(new ::grpc::internal::RpcServiceMethod(\n",
      "      grpcTpuCompilationCacheService_method_names[0],\n",
      "      ::grpc::internal::RpcMethod::NORMAL_RPC,\n",
      "      new ::grpc::internal::RpcMethodHandler<\n",
      "          grpc::TpuCompilationCacheService::Service, RequestType, ResponseType>(\n",
      "          std::mem_fn(\n",
      "              &grpc::TpuCompilationCacheService::Service::GetTpuProgram),\n",
      "          this)));\n",
      "}\n",
      "\n",
      "grpc::TpuCompilationCacheService::Service::~Service() {}\n",
      "\n",
      "::grpc::Status grpc::TpuCompilationCacheService::Service::GetTpuProgram(\n",
      "    ::grpc::ServerContext* context, const RequestType* request,\n",
      "    ResponseType* response) {\n",
      "  (void)context;\n",
      "  (void)request;\n",
      "  (void)response;\n",
      "  return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, \"\");\n",
      "}\n",
      "\n",
      "}  // namespace tpu\n",
      "}  // namespace tensorflow\n",
      "\n",
      "Average Line Length: 38.48623853211009\n",
      "\n",
      "Example 3:\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/compiler/xla/service/memory_space_assignment_utils.h\"\n",
      "\n",
      "#include \"tensorflow/compiler/xla/service/hlo_casting_utils.h\"\n",
      "#include \"tensorflow/compiler/xla/service/hlo_instructions.h\"\n",
      "\n",
      "namespace xla {\n",
      "\n",
      "bool MemorySpaceAssignmentUtils::IsValueAllowedInAlternateMemory(\n",
      "    const HloValue* value) {\n",
      "  // If the buffer is a tuple, don't use this algorithm for now. The buffers\n",
      "  // that are pointed to by the tuple will still use this algorithm.  Because\n",
      "  // tuples are cheap to place in the alternate memory (they are just pointers)\n",
      "  // we don't need to use prefetch/evict logic.\n",
      "  if (value->shape().IsTuple()) {\n",
      "    VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "            << \" in default mem because it is a tuple.\";\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  // Don't place scalars in the alternate memory.\n",
      "  if (ShapeUtil::IsEffectiveScalar(value->shape())) {\n",
      "    VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "            << \" in default mem because it is a scalar.\";\n",
      "    return false;\n",
      "  }\n",
      "\n",
      "  // The semantics of TupleSelect are weird: TupleSelect doesn't define a\n",
      "  // buffer, but just forwards the buffers in the either left or right side.\n",
      "  // This means the two different inputs to TupleSelect must not alias, yet they\n",
      "  // should be allocated in the same memory space, and both buffers must be kept\n",
      "  // alive for the entire live range of TupleSelect. Instead, just don't\n",
      "  // allocate TupleSelect in the alternate memory space.\n",
      "  // TODO(berkin): Not allocating add-dependencies either since they need to be\n",
      "  // treated specially. We should revisit this later.\n",
      "  for (const HloPosition& position : value->positions()) {\n",
      "    if (position.instruction->opcode() == HloOpcode::kTupleSelect ||\n",
      "        position.instruction->opcode() == HloOpcode::kAddDependency) {\n",
      "      VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "              << \" in default mem because it has a tuple-select or \"\n",
      "              << \"add-dependency position.\";\n",
      "      return false;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  // Send and Recv HLOs return a request identifier. These should not be\n",
      "  // allocated in the alternate memory.\n",
      "  for (const HloPosition& position : value->positions()) {\n",
      "    if ((position.instruction->opcode() == HloOpcode::kSend ||\n",
      "         position.instruction->opcode() == HloOpcode::kRecv) &&\n",
      "        DynCast<HloSendRecvInstruction>(position.instruction)\n",
      "            ->is_host_transfer()) {\n",
      "      // TODO(berkin): Host transfers using alternate memory space doesn't seem\n",
      "      // to work at the moment.\n",
      "      VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "              << \" in default mem because it is a send/recv buffer used for \"\n",
      "                 \"host transfer.\";\n",
      "      return false;\n",
      "    }\n",
      "\n",
      "    if (auto* custom_call =\n",
      "            DynCast<HloCustomCallInstruction>(position.instruction)) {\n",
      "      for (const auto& pair : custom_call->output_to_operand_aliasing()) {\n",
      "        if (position.index == pair.first) {\n",
      "          VLOG(4) << \"Keeping value \" << value->ToShortString()\n",
      "                  << \" in default mem because it is a custom-call output that \"\n",
      "                     \"aliases an operand buffer.\";\n",
      "          return false;\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return true;\n",
      "}\n",
      "\n",
      "bool MemorySpaceAssignmentUtils::IsIntervalAllowedInAlternateMemory(\n",
      "    const GlobalDecreasingSizeBestFitHeap<HloValue>::BufferInterval& interval) {\n",
      "  return IsValueAllowedInAlternateMemory(interval.buffer) &&\n",
      "         absl::c_all_of(interval.colocations, IsValueAllowedInAlternateMemory);\n",
      "}\n",
      "\n",
      "/*static*/ void MemorySpaceAssignmentUtils::HoistConstantOperations(\n",
      "    HloModule& module) {\n",
      "  CHECK(module.has_schedule());\n",
      "  HloSchedule& schedule = module.schedule();\n",
      "  for (const HloComputation* computation : module.MakeNonfusionComputations()) {\n",
      "    CHECK(schedule.is_computation_scheduled(computation));\n",
      "    const HloInstructionSequence& sequence = schedule.sequence(computation);\n",
      "    // Conservatively don't modify the schedule if any instruction has a control\n",
      "    // successor or predecessor on a constant op. Computations with these\n",
      "    // dependencies should be very rare anyway.\n",
      "    bool contains_constant_successor_or_predecessors = false;\n",
      "    for (HloInstruction* instruction : sequence.instructions()) {\n",
      "      if (instruction->opcode() == HloOpcode::kConstant) {\n",
      "        contains_constant_successor_or_predecessors |=\n",
      "            !instruction->control_predecessors().empty();\n",
      "        contains_constant_successor_or_predecessors |=\n",
      "            !instruction->control_successors().empty();\n",
      "      } else {\n",
      "        auto is_constant = [](const HloInstruction* inst) {\n",
      "          return inst->opcode() == HloOpcode::kConstant;\n",
      "        };\n",
      "        contains_constant_successor_or_predecessors |=\n",
      "            absl::c_find_if(instruction->control_predecessors(), is_constant) !=\n",
      "            instruction->control_predecessors().end();\n",
      "        contains_constant_successor_or_predecessors |=\n",
      "            absl::c_find_if(instruction->control_successors(), is_constant) !=\n",
      "            instruction->control_successors().end();\n",
      "      }\n",
      "    }\n",
      "    if (contains_constant_successor_or_predecessors) {\n",
      "      continue;\n",
      "    }\n",
      "    HloInstructionSequence new_sequence;\n",
      "\n",
      "    for (HloInstruction* instruction : sequence.instructions()) {\n",
      "      if (instruction->opcode() == HloOpcode::kConstant) {\n",
      "        new_sequence.push_back(instruction);\n",
      "      }\n",
      "    }\n",
      "    for (HloInstruction* instruction : sequence.instructions()) {\n",
      "      if (instruction->opcode() != HloOpcode::kConstant) {\n",
      "        new_sequence.push_back(instruction);\n",
      "      }\n",
      "    }\n",
      "    CHECK_EQ(new_sequence.size(), sequence.size());\n",
      "    schedule.set_sequence(computation, new_sequence);\n",
      "  }\n",
      "}\n",
      "\n",
      "}  // namespace xla\n",
      "\n",
      "Average Line Length: 41.91156462585034\n",
      "\n",
      "Example 4:\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/lite/delegates/gpu/common/tasks/add_test_util.h\"\n",
      "\n",
      "#include \"tensorflow/lite/delegates/gpu/common/operations.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/status.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/task/testing_util.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/tasks/add.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace gpu {\n",
      "\n",
      "absl::Status AddTwoEqualTensorsTest(TestExecutionEnvironment* env) {\n",
      "  TensorFloat32 src0, src1;\n",
      "  src0.shape = BHWC(1, 2, 1, 2);\n",
      "  src0.data = {0.0f, -1.0f, -0.05f, 0.045f};\n",
      "  src1.shape = BHWC(1, 2, 1, 2);\n",
      "  src1.data = {0.0f, 1.0f, -0.05f, -0.045f};\n",
      "  std::vector<int> channels = {2, 2};\n",
      "\n",
      "  for (auto storage : env->GetSupportedStorages()) {\n",
      "    for (auto precision : env->GetSupportedPrecisions()) {\n",
      "      const float eps = precision == CalculationsPrecision::F32 ? 1e-6f : 1e-3f;\n",
      "      OperationDef op_def;\n",
      "      op_def.precision = precision;\n",
      "      auto data_type = DeduceDataTypeFromPrecision(precision);\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.dst_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      TensorFloat32 dst_tensor;\n",
      "      GPUOperation operation = CreateAdd(op_def, channels, channels[0]);\n",
      "      RETURN_IF_ERROR(env->ExecuteGPUOperation(\n",
      "          {src0, src1}, absl::make_unique<GPUOperation>(std::move(operation)),\n",
      "          BHWC(1, 2, 1, 2), &dst_tensor));\n",
      "      RETURN_IF_ERROR(\n",
      "          PointWiseNear({0.0f, 0.0f, -0.1f, 0.0f}, dst_tensor.data, eps));\n",
      "    }\n",
      "  }\n",
      "  return absl::OkStatus();\n",
      "}\n",
      "\n",
      "absl::Status AddFirstTensorHasMoreChannelsThanSecondTest(\n",
      "    TestExecutionEnvironment* env) {\n",
      "  TensorFloat32 src0, src1;\n",
      "  src0.shape = BHWC(1, 2, 1, 6);\n",
      "  src0.data = {0.0f,   -1.0f,  -0.05f, 0.045f, 1.0f,   -2.0f,\n",
      "               -1.05f, 1.045f, 2.0f,   -3.0f,  -2.05f, 2.045f};\n",
      "  src1.shape = BHWC(1, 2, 1, 2);\n",
      "  src1.data = {0.0f, 1.0f, -0.05f, -0.045f};\n",
      "  std::vector<int> channels = {6, 2};\n",
      "  for (auto storage : env->GetSupportedStorages()) {\n",
      "    for (auto precision : env->GetSupportedPrecisions()) {\n",
      "      const float eps = precision == CalculationsPrecision::F32 ? 1e-6f : 1e-3f;\n",
      "      OperationDef op_def;\n",
      "      op_def.precision = precision;\n",
      "      auto data_type = DeduceDataTypeFromPrecision(precision);\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.dst_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      TensorFloat32 dst_tensor;\n",
      "      GPUOperation operation = CreateAdd(op_def, channels, channels[0]);\n",
      "      RETURN_IF_ERROR(env->ExecuteGPUOperation(\n",
      "          {src0, src1}, absl::make_unique<GPUOperation>(std::move(operation)),\n",
      "          BHWC(1, 2, 1, 6), &dst_tensor));\n",
      "      RETURN_IF_ERROR(PointWiseNear({0.0f, 0.0f, -0.05f, 0.045f, 1.0f, -2.0f,\n",
      "                                     -1.1f, 1.0f, 2.0f, -3.0f, -2.05f, 2.045f},\n",
      "                                    dst_tensor.data, eps));\n",
      "    }\n",
      "  }\n",
      "  return absl::OkStatus();\n",
      "}\n",
      "\n",
      "absl::Status AddFirstTensorHasLessChannelsThanSecond(\n",
      "    TestExecutionEnvironment* env) {\n",
      "  TensorFloat32 src0, src1;\n",
      "  src1.shape = BHWC(1, 2, 1, 6);\n",
      "  src1.data = {0.0f,   -1.0f,  -0.05f, 0.045f, 1.0f,   -2.0f,\n",
      "               -1.05f, 1.045f, 2.0f,   -3.0f,  -2.05f, 2.045f};\n",
      "  src0.shape = BHWC(1, 2, 1, 2);\n",
      "  src0.data = {0.0f, 1.0f, -0.05f, -0.045f};\n",
      "  std::vector<int> channels = {2, 6};\n",
      "  for (auto storage : env->GetSupportedStorages()) {\n",
      "    for (auto precision : env->GetSupportedPrecisions()) {\n",
      "      const float eps = precision == CalculationsPrecision::F32 ? 1e-6f : 1e-3f;\n",
      "      OperationDef op_def;\n",
      "      op_def.precision = precision;\n",
      "      auto data_type = DeduceDataTypeFromPrecision(precision);\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.src_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      op_def.dst_tensors.push_back({data_type, storage, Layout::HWC});\n",
      "      TensorFloat32 dst_tensor;\n",
      "      GPUOperation operation = CreateAdd(op_def, channels, 6);\n",
      "      RETURN_IF_ERROR(env->ExecuteGPUOperation(\n",
      "          {src0, src1}, absl::make_unique<GPUOperation>(std::move(operation)),\n",
      "          BHWC(1, 2, 1, 6), &dst_tensor));\n",
      "      RETURN_IF_ERROR(PointWiseNear({0.0f, 0.0f, -0.05f, 0.045f, 1.0f, -2.0f,\n",
      "                                     -1.1f, 1.0f, 2.0f, -3.0f, -2.05f, 2.045f},\n",
      "                                    dst_tensor.data, eps));\n",
      "    }\n",
      "  }\n",
      "  return absl::OkStatus();\n",
      "}\n",
      "\n",
      "}  // namespace gpu\n",
      "}  // namespace tflite\n",
      "\n",
      "Average Line Length: 42.84033613445378\n",
      "\n",
      "Example 5:\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/core/framework/common_shape_fns.h\"\n",
      "#include \"tensorflow/core/framework/op.h\"\n",
      "#include \"tensorflow/core/framework/shape_inference.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace {\n",
      "\n",
      "// TODO(kttian): Support non-scalar values\n",
      "REGISTER_OP(\"EmptyTensorMap\")\n",
      "    .Output(\"handle: variant\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapSize\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Output(\"size: int32\")\n",
      "    .SetShapeFn(shape_inference::ScalarShape);\n",
      "\n",
      "REGISTER_OP(\"TensorMapLookup\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"value: value_dtype\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->UnknownShape());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapInsert\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Input(\"value: value_dtype\")\n",
      "    .Output(\"output_handle: variant\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapErase\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"output_handle: variant\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());  // output map\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapHasKey\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"has_key: bool\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .SetShapeFn(shape_inference::ScalarShape);\n",
      "\n",
      "REGISTER_OP(\"TensorMapStackKeys\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Output(\"keys: key_dtype\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->UnknownShape());  // output keys\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "}  // namespace\n",
      "}  // namespace tensorflow\n",
      "\n",
      "Average Line Length: 30.738636363636363\n",
      "\n",
      "Example 6:\n",
      "<gh_stars>1000+\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#include \"tensorflow/c/eager/unified_api_testutil.h\"\n",
      "\n",
      "#include \"absl/container/flat_hash_set.h\"\n",
      "#include \"tensorflow/c/eager/c_api_experimental.h\"\n",
      "#include \"tensorflow/c/eager/c_api_test_util.h\"\n",
      "#include \"tensorflow/c/eager/c_api_unified_experimental.h\"\n",
      "#include \"tensorflow/c/eager/c_api_unified_experimental_internal.h\"\n",
      "#include \"tensorflow/c/tf_status.h\"\n",
      "#include \"tensorflow/c/tf_status_helper.h\"\n",
      "#include \"tensorflow/core/framework/tensor_shape.h\"\n",
      "#include \"tensorflow/core/lib/llvm_rtti/llvm_rtti.h\"\n",
      "#include \"tensorflow/core/platform/errors.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "AbstractContext* BuildFunction(const char* fn_name) {\n",
      "  std::unique_ptr<TF_Status, decltype(&TF_DeleteStatus)> status(\n",
      "      TF_NewStatus(), TF_DeleteStatus);\n",
      "  TF_ExecutionContext* graph_ctx = TF_CreateFunction(fn_name, status.get());\n",
      "  return unwrap(graph_ctx);\n",
      "}\n",
      "\n",
      "Status CreateParamsForInputs(AbstractContext* ctx,\n",
      "                             absl::Span<AbstractTensorHandle* const> inputs,\n",
      "                             std::vector<AbstractTensorHandle*>* params) {\n",
      "  tracing::TracingTensorHandle* handle = nullptr;\n",
      "  for (auto input : inputs) {\n",
      "    PartialTensorShape shape;\n",
      "    TF_RETURN_IF_ERROR(input->Shape(&shape));\n",
      "    TF_RETURN_IF_ERROR(dyn_cast<tracing::TracingContext>(ctx)->AddParameter(\n",
      "        input->DataType(), shape, &handle));\n",
      "    params->emplace_back(handle);\n",
      "  }\n",
      "  return Status::OK();\n",
      "}\n",
      "\n",
      "// Runs `model` maybe wrapped in a function.\n",
      "Status RunModel(Model model, AbstractContext* ctx,\n",
      "                absl::Span<AbstractTensorHandle* const> inputs,\n",
      "                absl::Span<AbstractTensorHandle*> outputs, bool use_function) {\n",
      "  if (use_function) {\n",
      "    const char* fn_name = \"test_fn\";\n",
      "    core::RefCountPtr<AbstractFunction> scoped_func;\n",
      "    // Returning null tensors from a tf.function is not supported, so we keep\n",
      "    // track of indices in the model's outputs are nullptr in this set.\n",
      "    // The FunctionDef only outputs the non-null tensors. We later pad the\n",
      "    // function op outputs to have nullptrs at the `null_indices`.\n",
      "    absl::flat_hash_set<int> null_indices;\n",
      "    {\n",
      "      AbstractContextPtr func_ctx(BuildFunction(fn_name));\n",
      "      std::vector<AbstractTensorHandle*> func_inputs;\n",
      "      func_inputs.reserve(inputs.size());\n",
      "      TF_RETURN_IF_ERROR(\n",
      "          CreateParamsForInputs(func_ctx.get(), inputs, &func_inputs));\n",
      "      std::vector<AbstractTensorHandle*> model_outputs;\n",
      "      model_outputs.resize(outputs.size());\n",
      "      TF_RETURN_IF_ERROR(model(func_ctx.get(), absl::MakeSpan(func_inputs),\n",
      "                               absl::MakeSpan(model_outputs)));\n",
      "      for (auto func_input : func_inputs) {\n",
      "        func_input->Unref();\n",
      "      }\n",
      "      AbstractFunction* func = nullptr;\n",
      "      OutputList output_list;\n",
      "      output_list.expected_num_outputs = 0;\n",
      "      output_list.outputs.reserve(outputs.size());\n",
      "      for (int i = 0; i < model_outputs.size(); i++) {\n",
      "        if (model_outputs[i]) {\n",
      "          output_list.outputs.emplace_back(model_outputs[i]);\n",
      "          output_list.expected_num_outputs += 1;\n",
      "        } else {\n",
      "          null_indices.insert(i);\n",
      "        }\n",
      "      }\n",
      "      TF_RETURN_IF_ERROR(dyn_cast<tracing::TracingContext>(func_ctx.get())\n",
      "                             ->Finalize(&output_list, &func));\n",
      "      scoped_func.reset(func);\n",
      "      for (auto output : output_list.outputs) {\n",
      "        output->Unref();\n",
      "      }\n",
      "      TF_RETURN_IF_ERROR(ctx->RegisterFunction(func));\n",
      "    }\n",
      "\n",
      "    AbstractOperationPtr fn_op(ctx->CreateOperation());\n",
      "    TF_RETURN_IF_ERROR(fn_op->Reset(fn_name, /*raw_device_name=*/nullptr));\n",
      "    for (auto input : inputs) {\n",
      "      TF_RETURN_IF_ERROR(fn_op->AddInput(input));\n",
      "    }\n",
      "    int retvals = outputs.size() - null_indices.size();\n",
      "    std::vector<AbstractTensorHandle*> fn_outputs(retvals);\n",
      "    TF_RETURN_IF_ERROR(fn_op->Execute(\n",
      "        absl::Span<AbstractTensorHandle*>(fn_outputs.data(), fn_outputs.size()),\n",
      "        &retvals));\n",
      "    int skipped_indices = 0;\n",
      "    for (int i = 0; i < outputs.size(); i++) {\n",
      "      if (!null_indices.contains(i)) {\n",
      "        outputs[i] = fn_outputs[i - skipped_indices];\n",
      "      } else {\n",
      "        skipped_indices += 1;\n",
      "      }\n",
      "    }\n",
      "    TF_RETURN_IF_ERROR(ctx->RemoveFunction(fn_name));\n",
      "    return Status::OK();\n",
      "  } else {\n",
      "    return model(ctx, inputs, outputs);\n",
      "  }\n",
      "}\n",
      "\n",
      "Status BuildImmediateExecutionContext(bool use_tfrt, AbstractContext** ctx) {\n",
      "  std::unique_ptr<TF_Status, decltype(&TF_DeleteStatus)> status(\n",
      "      TF_NewStatus(), TF_DeleteStatus);\n",
      "  TFE_ContextOptions* opts = TFE_NewContextOptions();\n",
      "  TFE_ContextOptionsSetTfrt(opts, use_tfrt);\n",
      "  *ctx = unwrap(TF_NewEagerExecutionContext(opts, status.get()));\n",
      "  TF_RETURN_IF_ERROR(StatusFromTF_Status(status.get()));\n",
      "  TFE_DeleteContextOptions(opts);\n",
      "  return Status::OK();\n",
      "}\n",
      "\n",
      "Status GetValue(AbstractTensorHandle* t, TF_Tensor** result_tensor) {\n",
      "  std::unique_ptr<TF_Status, decltype(&TF_DeleteStatus)> status(\n",
      "      TF_NewStatus(), TF_DeleteStatus);\n",
      "  TFE_TensorHandle* result_t =\n",
      "      TF_AbstractTensorGetEagerTensor(wrap(t), status.get());\n",
      "  TF_RETURN_IF_ERROR(StatusFromTF_Status(status.get()));\n",
      "  *result_tensor = TFE_TensorHandleResolve(result_t, status.get());\n",
      "  return StatusFromTF_Status(status.get());\n",
      "}\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "Average Line Length: 39.4\n",
      "\n",
      "Example 7:\n",
      "<filename>tensorflow/c/experimental/saved_model/core/revived_types/asset.cc\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/c/experimental/saved_model/core/revived_types/asset.h\"\n",
      "\n",
      "#include <string>\n",
      "\n",
      "#include \"tensorflow/c/eager/immediate_execution_context.h\"\n",
      "#include \"tensorflow/c/eager/immediate_execution_tensor_handle.h\"\n",
      "#include \"tensorflow/c/tensor_interface.h\"\n",
      "#include \"tensorflow/cc/saved_model/constants.h\"\n",
      "#include \"tensorflow/core/platform/errors.h\"\n",
      "#include \"tensorflow/core/platform/path.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "\n",
      "Asset::Asset(ImmediateTensorHandlePtr handle)\n",
      "    : TensorHandleConvertible(std::move(handle)) {}\n",
      "\n",
      "Status Asset::Create(ImmediateExecutionContext* ctx,\n",
      "                     const std::string& saved_model_dir,\n",
      "                     const std::string& asset_filename,\n",
      "                     std::unique_ptr<Asset>* output) {\n",
      "  std::string abs_path =\n",
      "      io::JoinPath(saved_model_dir, kSavedModelAssetsDirectory, asset_filename);\n",
      "  AbstractTensorPtr tensor(ctx->CreateStringScalar(abs_path));\n",
      "  if (tensor.get() == nullptr) {\n",
      "    return errors::Internal(\n",
      "        \"Failed to create scalar string tensor for Asset at path \", abs_path);\n",
      "  }\n",
      "\n",
      "  ImmediateTensorHandlePtr handle(ctx->CreateLocalHandle(tensor.get()));\n",
      "  output->reset(new Asset(std::move(handle)));\n",
      "  return Status();\n",
      "}\n",
      "\n",
      "}  // namespace tensorflow\n",
      "\n",
      "Average Line Length: 37.78431372549019\n",
      "\n",
      "Example 8:\n",
      "<filename>tensorflow/core/tfrt/eager/backends/gpu/gpu_registration.cc\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/core/common_runtime/device/device_id.h\"\n",
      "#include \"tensorflow/core/common_runtime/device_mgr.h\"\n",
      "#include \"tensorflow/core/common_runtime/gpu/gpu_id_manager.h\"\n",
      "#include \"tensorflow/core/runtime_fallback/runtime/runtime_fallback_op_handler.h\"\n",
      "#include \"tensorflow/core/tfrt/eager/core_runtime/op_handler_registry.h\"\n",
      "#include \"tfrt/gpu/core_runtime/gpu_op_handler.h\"  // from @tf_runtime\n",
      "#include \"tfrt/gpu/device/device.h\"  // from @tf_runtime\n",
      "#include \"tfrt/gpu/device/device_util.h\"  // from @tf_runtime\n",
      "#include \"tfrt/core_runtime/core_runtime.h\"  // from @tf_runtime\n",
      "#include \"tfrt/host_context/resource_context.h\"  // from @tf_runtime\n",
      "\n",
      "namespace tfrt {\n",
      "namespace tf {\n",
      "namespace gpu {\n",
      "\n",
      "using ::tfrt::CoreRuntime;\n",
      "\n",
      "static void RegisterGpuOpHandler(CoreRuntime* core_runtime,\n",
      "                                 ResourceContext* resource_context,\n",
      "                                 const DeviceMgr* device_mgr) {\n",
      "  for (auto& device : device_mgr->ListDevices()) {\n",
      "    auto& parsed_name = device->parsed_name();\n",
      "    assert(parsed_name.has_id && parsed_name.has_type);\n",
      "    if (parsed_name.type == \"GPU\") {\n",
      "      // Please see the difference between tf_device_id and platform_device_id\n",
      "      // here in tensorflow/core/common_runtime/device/device_id.h\n",
      "      tensorflow::TfDeviceId tf_device_id(parsed_name.id);\n",
      "      tensorflow::PlatformDeviceId platform_device_id;\n",
      "      tensorflow::Status s = tensorflow::GpuIdManager::TfToPlatformDeviceId(\n",
      "          tf_device_id, &platform_device_id);\n",
      "      if (!s.ok()) {\n",
      "        LOG(ERROR) << \"Failed to convert gpu device [\" << device->name()\n",
      "                   << \"] to platform device id due to error: \"\n",
      "                   << s.error_message();\n",
      "        continue;\n",
      "      }\n",
      "      auto gpu = tfrt::gpu::GetOrCreateGpuDevice(\n",
      "          device->name(), platform_device_id.value(),\n",
      "          core_runtime->GetHostContext());\n",
      "      if (!gpu) {\n",
      "        LOG(ERROR) << \"Failed to create gpu device [\" << device->name()\n",
      "                   << \"]. Error: \" << StrCat(gpu.takeError());\n",
      "        continue;\n",
      "      }\n",
      "      LOG(INFO) << \"Found a GPU device: \" << device->name();\n",
      "      auto expected_fallback_op_handler =\n",
      "          tensorflow::tfd::CreateRuntimeFallbackOpHandler(core_runtime,\n",
      "                                                          device->name());\n",
      "      assert(expected_fallback_op_handler);\n",
      "\n",
      "      auto expected_gpu_op_handler =\n",
      "          ::tfrt::gpu::CreateGpuOpHandler(core_runtime, std::move(gpu.get()),\n",
      "                                          expected_fallback_op_handler.get());\n",
      "      assert(expected_gpu_op_handler);\n",
      "\n",
      "      core_runtime->RegisterOpHandler(device->name(),\n",
      "                                      expected_gpu_op_handler.get());\n",
      "\n",
      "      // TODO(fishx): Remove this when lowering pass can use full device name.\n",
      "      if (parsed_name.id == 0) {\n",
      "        core_runtime->RegisterOpHandler(\"gpu\", expected_gpu_op_handler.get());\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "static OpHandlerRegistration register_gpu(RegisterGpuOpHandler);\n",
      "\n",
      "}  // namespace gpu\n",
      "}  // namespace tf\n",
      "}  // namespace tfrt\n",
      "\n",
      "Average Line Length: 42.01136363636363\n",
      "\n",
      "Example 9:\n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#include \"tensorflow/core/kernels/data/experimental/random_dataset_op.h\"\n",
      "\n",
      "#include \"tensorflow/core/data/dataset_test_base.h\"\n",
      "#include \"tensorflow/core/lib/random/philox_random.h\"\n",
      "#include \"tensorflow/core/lib/random/random_distributions.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace data {\n",
      "namespace experimental {\n",
      "namespace {\n",
      "\n",
      "constexpr char kNodeName[] = \"random_dataset\";\n",
      "constexpr char kIteratorPrefix[] = \"Iterator\";\n",
      "\n",
      "// Number of random samples generated per test\n",
      "constexpr int kCount = 10;\n",
      "\n",
      "// Generate the first `count` random numbers that the kernel should produce\n",
      "// for a given seed/seed2 combo.\n",
      "// For compatibility with the test harness, return value is a vector of scalar\n",
      "// Tensors.\n",
      "std::vector<Tensor> GenerateExpectedData(int64_t seed, int64_t seed2,\n",
      "                                         int count) {\n",
      "  std::vector<Tensor> ret;\n",
      "  auto parent_generator = random::PhiloxRandom(seed, seed2);\n",
      "  auto generator =\n",
      "      random::SingleSampleAdapter<random::PhiloxRandom>(&parent_generator);\n",
      "\n",
      "  for (int i = 0; i < count; ++i) {\n",
      "    ret.push_back(CreateTensor<int64_t>(TensorShape({}), {generator()}));\n",
      "  }\n",
      "  return ret;\n",
      "}\n",
      "\n",
      "class RandomDatasetParams : public DatasetParams {\n",
      " public:\n",
      "  RandomDatasetParams(int64_t seed, int64_t seed2, DataTypeVector output_dtypes,\n",
      "                      std::vector<PartialTensorShape> output_shapes,\n",
      "                      string node_name)\n",
      "      : DatasetParams(std::move(output_dtypes), std::move(output_shapes),\n",
      "                      std::move(node_name)),\n",
      "        seed_(CreateTensor<int64_t>(TensorShape({}), {seed})),\n",
      "        seed2_(CreateTensor<int64_t>(TensorShape({}), {seed2})) {}\n",
      "\n",
      "  virtual std::vector<Tensor> GetInputTensors() const override {\n",
      "    return {seed_, seed2_};\n",
      "  }\n",
      "\n",
      "  virtual Status GetInputNames(\n",
      "      std::vector<string>* input_names) const override {\n",
      "    *input_names = {RandomDatasetOp::kSeed, RandomDatasetOp::kSeed2};\n",
      "    return Status::OK();\n",
      "  }\n",
      "\n",
      "  virtual Status GetAttributes(AttributeVector* attributes) const override {\n",
      "    *attributes = {{\"output_types\", output_dtypes_},\n",
      "                   {\"output_shapes\", output_shapes_},\n",
      "                   {\"metadata\", \"\"}};\n",
      "    return Status::OK();\n",
      "  }\n",
      "\n",
      "  virtual string dataset_type() const override {\n",
      "    return RandomDatasetOp::kDatasetType;\n",
      "  }\n",
      "\n",
      " private:\n",
      "  Tensor seed_;\n",
      "  Tensor seed2_;\n",
      "};\n",
      "\n",
      "class RandomDatasetOpTest : public DatasetOpsTestBase {};\n",
      "\n",
      "RandomDatasetParams FortyTwo() {\n",
      "  return {/*seed=*/42,\n",
      "          /*seed2=*/42,\n",
      "          /*output_dtypes=*/{DT_INT64},\n",
      "          /*output_shapes=*/{PartialTensorShape({})},\n",
      "          /*node_name=*/kNodeName};\n",
      "}\n",
      "\n",
      "// Change just first seed relative to FortyTwo\n",
      "RandomDatasetParams ChangeSeed() {\n",
      "  return {/*seed=*/1000,\n",
      "          /*seed2=*/42,\n",
      "          /*output_dtypes=*/{DT_INT64},\n",
      "          /*output_shapes=*/{PartialTensorShape({})},\n",
      "          /*node_name=*/kNodeName};\n",
      "}\n",
      "\n",
      "// Change just second seed relative to FortyTwo\n",
      "RandomDatasetParams ChangeSeed2() {\n",
      "  return {/*seed=*/42,\n",
      "          /*seed2=*/1000,\n",
      "          /*output_dtypes=*/{DT_INT64},\n",
      "          /*output_shapes=*/{PartialTensorShape({})},\n",
      "          /*node_name=*/kNodeName};\n",
      "}\n",
      "\n",
      "class ParameterizedGetNextTest : public RandomDatasetOpTest,\n",
      "                                 public ::testing::WithParamInterface<\n",
      "                                     GetNextTestCase<RandomDatasetParams>> {};\n",
      "\n",
      "std::vector<GetNextTestCase<RandomDatasetParams>> GetNextTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_outputs=*/GenerateExpectedData(42, 42, kCount)},\n",
      "          {/*dataset_params=*/ChangeSeed(),\n",
      "           /*expected_outputs=*/GenerateExpectedData(1000, 42, kCount)},\n",
      "          {/*dataset_params=*/ChangeSeed2(),\n",
      "           /*expected_outputs=*/GenerateExpectedData(42, 1000, kCount)}};\n",
      "}\n",
      "\n",
      "TEST_P(ParameterizedGetNextTest, GetNext) {\n",
      "  auto test_case = GetParam();\n",
      "  TF_ASSERT_OK(Initialize(test_case.dataset_params));\n",
      "\n",
      "  // Can't use DatasetOpsTestBase::CheckIteratorGetNext because the kernel\n",
      "  // under test produces unbounded input.\n",
      "  bool end_of_sequence = false;\n",
      "  std::vector<Tensor> out_tensors;\n",
      "  while (out_tensors.size() < test_case.expected_outputs.size()) {\n",
      "    std::vector<Tensor> next;\n",
      "    TF_ASSERT_OK(\n",
      "        iterator_->GetNext(iterator_ctx_.get(), &next, &end_of_sequence));\n",
      "\n",
      "    ASSERT_FALSE(end_of_sequence);  // Dataset should never stop\n",
      "\n",
      "    out_tensors.insert(out_tensors.end(), next.begin(), next.end());\n",
      "  }\n",
      "\n",
      "  TF_ASSERT_OK(ExpectEqual(out_tensors, test_case.expected_outputs,\n",
      "                           /*compare_order=*/true));\n",
      "}\n",
      "\n",
      "INSTANTIATE_TEST_SUITE_P(\n",
      "    RandomDatasetOpTest, ParameterizedGetNextTest,\n",
      "    ::testing::ValuesIn(\n",
      "        std::vector<GetNextTestCase<RandomDatasetParams>>(GetNextTestCases())));\n",
      "\n",
      "std::vector<DatasetNodeNameTestCase<RandomDatasetParams>>\n",
      "DatasetNodeNameTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(), /*expected_node_name=*/kNodeName}};\n",
      "}\n",
      "\n",
      "DATASET_NODE_NAME_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                         DatasetNodeNameTestCases());\n",
      "\n",
      "std::vector<DatasetTypeStringTestCase<RandomDatasetParams>>\n",
      "DatasetTypeStringTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_dataset_type_string=*/name_utils::OpName(\n",
      "               RandomDatasetOp::kDatasetType)}};\n",
      "}\n",
      "\n",
      "DATASET_TYPE_STRING_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                           DatasetTypeStringTestCases());\n",
      "\n",
      "std::vector<DatasetOutputDtypesTestCase<RandomDatasetParams>>\n",
      "DatasetOutputDtypesTestCases() {\n",
      "  return {\n",
      "      {/*dataset_params=*/FortyTwo(), /*expected_output_dtypes=*/{DT_INT64}}};\n",
      "}\n",
      "\n",
      "DATASET_OUTPUT_DTYPES_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                             DatasetOutputDtypesTestCases());\n",
      "\n",
      "std::vector<DatasetOutputShapesTestCase<RandomDatasetParams>>\n",
      "DatasetOutputShapesTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_output_shapes=*/{PartialTensorShape({})}}};\n",
      "}\n",
      "\n",
      "DATASET_OUTPUT_SHAPES_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                             DatasetOutputShapesTestCases());\n",
      "\n",
      "std::vector<CardinalityTestCase<RandomDatasetParams>> CardinalityTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_cardinality=*/kInfiniteCardinality}};\n",
      "}\n",
      "\n",
      "DATASET_CARDINALITY_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                           CardinalityTestCases());\n",
      "\n",
      "std::vector<IteratorOutputDtypesTestCase<RandomDatasetParams>>\n",
      "IteratorOutputDtypesTestCases() {\n",
      "  return {\n",
      "      {/*dataset_params=*/FortyTwo(), /*expected_output_dtypes=*/{DT_INT64}}};\n",
      "}\n",
      "\n",
      "ITERATOR_OUTPUT_DTYPES_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                              IteratorOutputDtypesTestCases());\n",
      "\n",
      "std::vector<IteratorOutputShapesTestCase<RandomDatasetParams>>\n",
      "IteratorOutputShapesTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_output_shapes=*/{PartialTensorShape({})}}};\n",
      "}\n",
      "\n",
      "ITERATOR_OUTPUT_SHAPES_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                              IteratorOutputShapesTestCases());\n",
      "\n",
      "std::vector<IteratorPrefixTestCase<RandomDatasetParams>>\n",
      "IteratorOutputPrefixTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(),\n",
      "           /*expected_iterator_prefix=*/name_utils::IteratorPrefix(\n",
      "               RandomDatasetOp::kDatasetType, kIteratorPrefix)}};\n",
      "}\n",
      "\n",
      "ITERATOR_PREFIX_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                       IteratorOutputPrefixTestCases());\n",
      "\n",
      "std::vector<IteratorSaveAndRestoreTestCase<RandomDatasetParams>>\n",
      "IteratorSaveAndRestoreTestCases() {\n",
      "  return {{/*dataset_params=*/FortyTwo(), /*breakpoints=*/{2, 5, 8},\n",
      "           /*expected_outputs=*/GenerateExpectedData(42, 42, 9 /* 8 + 1 */)}};\n",
      "}\n",
      "\n",
      "ITERATOR_SAVE_AND_RESTORE_TEST_P(RandomDatasetOpTest, RandomDatasetParams,\n",
      "                                 IteratorSaveAndRestoreTestCases());\n",
      "\n",
      "}  // namespace\n",
      "}  // namespace experimental\n",
      "}  // namespace data\n",
      "}  // namespace tensorflow\n",
      "\n",
      "Average Line Length: 35.41702127659575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def calculate_avg_line_length(example):\n",
    "    lines = example['content'].split('\\n')\n",
    "    avg_length = sum(len(line) for line in lines) / len(lines)\n",
    "    example['avg_line_length'] = avg_length\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(calculate_avg_line_length)\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    if i < 10:\n",
    "        print(f\"Example {i}:\")\n",
    "        print(example['content'])\n",
    "        print(f\"Average Line Length: {example['avg_line_length']}\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_dataset = dataset[:2500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(sliced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for avg_line_length:\n",
      "count    2.500000e+06\n",
      "mean     2.814501e+01\n",
      "std      8.725241e+00\n",
      "min      7.176871e-01\n",
      "25%      2.233842e+01\n",
      "50%      2.756098e+01\n",
      "75%      3.300000e+01\n",
      "max      1.230000e+02\n",
      "Name: avg_line_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "avg_line_length_stats = df['avg_line_length'].describe()\n",
    "print(\"Statistics for avg_line_length:\")\n",
    "print(avg_line_length_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=df['avg_line_length'])\n",
    "plt.title('Boxplot of Average Line Length')\n",
    "plt.xlabel('Average Line Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['avg_line_length'].quantile(0.25)\n",
    "Q3 = df['avg_line_length'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 47508\n"
     ]
    }
   ],
   "source": [
    "outliers = df[(df['avg_line_length'] < (Q1 - 1.5 * IQR)) | (df['avg_line_length'] > (Q3 + 1.5 * IQR))]\n",
    "print(f\"Number of outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    47508.000000\n",
       "mean        57.765783\n",
       "std         13.339976\n",
       "min          0.717687\n",
       "25%         51.198068\n",
       "50%         55.139672\n",
       "75%         63.389311\n",
       "max        123.000000\n",
       "Name: avg_line_length, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers['avg_line_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(outliers.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.452492e+06\n",
       "mean     2.757121e+01\n",
       "std      7.538698e+00\n",
       "min      6.347826e+00\n",
       "25%      2.223684e+01\n",
       "50%      2.738279e+01\n",
       "75%      3.264286e+01\n",
       "max      4.899232e+01\n",
       "Name: avg_line_length, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['avg_line_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.96666666666667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.percentile(df['avg_line_length'], 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.452492e+06\n",
       "mean     2.384202e+02\n",
       "std      7.018846e+02\n",
       "min      1.000000e+00\n",
       "25%      4.500000e+01\n",
       "50%      9.800000e+01\n",
       "75%      2.230000e+02\n",
       "max      7.964100e+04\n",
       "Name: line_count, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['line_count'] = df['content'].apply(lambda x: len(x.split('\\n')))\n",
    "df['line_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_range_dict = {\n",
    "    \"0-100\": 0,\n",
    "    \"101-200\": 0,\n",
    "    \"201-300\": 0,\n",
    "    \"301-400\": 0,\n",
    "    \"401-500\": 0,\n",
    "    \"501-600\": 0,\n",
    "    \"601-700\": 0,\n",
    "    \"701-800\": 0,\n",
    "    \"801-900\": 0,\n",
    "    \"901-1000\": 0,\n",
    "    \"1000+\": 0\n",
    "}\n",
    "\n",
    "for count in df['line_count']:\n",
    "    if count <= 100:\n",
    "        count_range_dict[\"0-100\"] += 1\n",
    "    elif count <= 200:\n",
    "        count_range_dict[\"101-200\"] += 1\n",
    "    elif count <= 300:\n",
    "        count_range_dict[\"201-300\"] += 1\n",
    "    elif count <= 400:\n",
    "        count_range_dict[\"301-400\"] += 1\n",
    "    elif count <= 500:\n",
    "        count_range_dict[\"401-500\"] += 1\n",
    "    elif count <= 600:\n",
    "        count_range_dict[\"501-600\"] += 1\n",
    "    elif count <= 700:\n",
    "        count_range_dict[\"601-700\"] += 1\n",
    "    elif count <= 800:\n",
    "        count_range_dict[\"701-800\"] += 1\n",
    "    elif count <= 900:\n",
    "        count_range_dict[\"801-900\"] += 1\n",
    "    elif count <= 1000:\n",
    "        count_range_dict[\"901-1000\"] += 1\n",
    "    else:\n",
    "        count_range_dict[\"1000+\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples with line count in range 0-100: 1250980\n",
      "Number of examples with line count in range 101-200: 521910\n",
      "Number of examples with line count in range 201-300: 232842\n",
      "Number of examples with line count in range 301-400: 128321\n",
      "Number of examples with line count in range 401-500: 78688\n",
      "Number of examples with line count in range 501-600: 52171\n",
      "Number of examples with line count in range 601-700: 36269\n",
      "Number of examples with line count in range 701-800: 26549\n",
      "Number of examples with line count in range 801-900: 20060\n",
      "Number of examples with line count in range 901-1000: 15775\n",
      "Number of examples with line count in range 1000+: 88927\n"
     ]
    }
   ],
   "source": [
    "for key, value in count_range_dict.items():\n",
    "    print(f\"Number of examples with line count in range {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(count_range_dict.keys()), y=list(count_range_dict.values()))\n",
    "plt.title(\"Distribution of C++ code lengths\")\n",
    "plt.xlabel(\"Length of C++ code\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Count</th>\n",
       "      <th>Cumulative Count</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>Cumulative Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-100</td>\n",
       "      <td>1250980</td>\n",
       "      <td>1250980</td>\n",
       "      <td>51.008525</td>\n",
       "      <td>51.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101-200</td>\n",
       "      <td>521910</td>\n",
       "      <td>1772890</td>\n",
       "      <td>21.280803</td>\n",
       "      <td>72.289329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201-300</td>\n",
       "      <td>232842</td>\n",
       "      <td>2005732</td>\n",
       "      <td>9.494098</td>\n",
       "      <td>81.783427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>301-400</td>\n",
       "      <td>128321</td>\n",
       "      <td>2134053</td>\n",
       "      <td>5.232270</td>\n",
       "      <td>87.015697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401-500</td>\n",
       "      <td>78688</td>\n",
       "      <td>2212741</td>\n",
       "      <td>3.208492</td>\n",
       "      <td>90.224188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>501-600</td>\n",
       "      <td>52171</td>\n",
       "      <td>2264912</td>\n",
       "      <td>2.127265</td>\n",
       "      <td>92.351453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>601-700</td>\n",
       "      <td>36269</td>\n",
       "      <td>2301181</td>\n",
       "      <td>1.478863</td>\n",
       "      <td>93.830316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>701-800</td>\n",
       "      <td>26549</td>\n",
       "      <td>2327730</td>\n",
       "      <td>1.082532</td>\n",
       "      <td>94.912848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>801-900</td>\n",
       "      <td>20060</td>\n",
       "      <td>2347790</td>\n",
       "      <td>0.817944</td>\n",
       "      <td>95.730791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901-1000</td>\n",
       "      <td>15775</td>\n",
       "      <td>2363565</td>\n",
       "      <td>0.643223</td>\n",
       "      <td>96.374015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1000+</td>\n",
       "      <td>88927</td>\n",
       "      <td>2452492</td>\n",
       "      <td>3.625985</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length    Count  Cumulative Count  Percentage  Cumulative Percentage\n",
       "0      0-100  1250980           1250980   51.008525              51.008525\n",
       "1    101-200   521910           1772890   21.280803              72.289329\n",
       "2    201-300   232842           2005732    9.494098              81.783427\n",
       "3    301-400   128321           2134053    5.232270              87.015697\n",
       "4    401-500    78688           2212741    3.208492              90.224188\n",
       "5    501-600    52171           2264912    2.127265              92.351453\n",
       "6    601-700    36269           2301181    1.478863              93.830316\n",
       "7    701-800    26549           2327730    1.082532              94.912848\n",
       "8    801-900    20060           2347790    0.817944              95.730791\n",
       "9   901-1000    15775           2363565    0.643223              96.374015\n",
       "10     1000+    88927           2452492    3.625985             100.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_line = pd.DataFrame(list(count_range_dict.items()), columns=[\"Length\", \"Count\"])\n",
    "df_line[\"Cumulative Count\"] = df_line[\"Count\"].cumsum()\n",
    "df_line[\"Percentage\"] = df_line[\"Count\"] / df_line[\"Count\"].sum() * 100\n",
    "df_line[\"Cumulative Percentage\"] = df_line[\"Percentage\"].cumsum()\n",
    "df_line.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id',\n",
       "       'content', 'avg_line_length', 'line_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>line_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/**\\n * Copyright (c) Facebook, Inc. and its a...</td>\n",
       "      <td>22.730000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/* Copyright 2021 The TensorFlow Authors. All ...</td>\n",
       "      <td>40.199248</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/* Copyright 2020 The TensorFlow Authors. All ...</td>\n",
       "      <td>38.486239</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/* Copyright 2020 The TensorFlow Authors. All ...</td>\n",
       "      <td>41.911565</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/* Copyright 2021 The TensorFlow Authors. All ...</td>\n",
       "      <td>42.840336</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  avg_line_length  \\\n",
       "0  /**\\n * Copyright (c) Facebook, Inc. and its a...        22.730000   \n",
       "1  /* Copyright 2021 The TensorFlow Authors. All ...        40.199248   \n",
       "2  /* Copyright 2020 The TensorFlow Authors. All ...        38.486239   \n",
       "3  /* Copyright 2020 The TensorFlow Authors. All ...        41.911565   \n",
       "4  /* Copyright 2021 The TensorFlow Authors. All ...        42.840336   \n",
       "\n",
       "   line_count  \n",
       "0         100  \n",
       "1         266  \n",
       "2         109  \n",
       "3         147  \n",
       "4         119  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df.drop(columns=['max_stars_repo_path', 'max_stars_repo_name', 'max_stars_count', 'id'], axis=1)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2452492, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2212741, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_filtered[df_filtered['line_count'] <= 500]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804627, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_filtered[df_filtered['avg_line_length'] <= 34]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>avg_line_length</th>\n",
       "      <th>line_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/**\\n * Copyright (c) Facebook, Inc. and its a...</td>\n",
       "      <td>22.730000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/* Copyright 2020 The TensorFlow Authors. All ...</td>\n",
       "      <td>30.738636</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;reponame&gt;EricRemmerswaal/tensorflow\\n/* Copyr...</td>\n",
       "      <td>30.736559</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;filename&gt;tensorflow/c/tf_shape.cc&lt;gh_stars&gt;10...</td>\n",
       "      <td>30.146341</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/* Copyright 2020 The TensorFlow Authors. All ...</td>\n",
       "      <td>33.913333</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  avg_line_length  \\\n",
       "0   /**\\n * Copyright (c) Facebook, Inc. and its a...        22.730000   \n",
       "5   /* Copyright 2020 The TensorFlow Authors. All ...        30.738636   \n",
       "16  <reponame>EricRemmerswaal/tensorflow\\n/* Copyr...        30.736559   \n",
       "19  <filename>tensorflow/c/tf_shape.cc<gh_stars>10...        30.146341   \n",
       "21  /* Copyright 2020 The TensorFlow Authors. All ...        33.913333   \n",
       "\n",
       "    line_count  \n",
       "0          100  \n",
       "5           88  \n",
       "16         186  \n",
       "19          41  \n",
       "21         300  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       avg_line_length    line_count\n",
      "count     2.452492e+06  2.452492e+06\n",
      "mean      2.757121e+01  2.384202e+02\n",
      "std       7.538698e+00  7.018846e+02\n",
      "min       6.347826e+00  1.000000e+00\n",
      "25%       2.223684e+01  4.500000e+01\n",
      "50%       2.738279e+01  9.800000e+01\n",
      "75%       3.264286e+01  2.230000e+02\n",
      "max       4.899232e+01  7.964100e+04\n",
      "\n",
      "       avg_line_length    line_count\n",
      "count     1.804627e+06  1.804627e+06\n",
      "mean      2.462387e+01  1.139572e+02\n",
      "std       5.636753e+00  1.062110e+02\n",
      "min       6.347826e+00  1.000000e+00\n",
      "25%       2.066667e+01  3.800000e+01\n",
      "50%       2.512500e+01  7.600000e+01\n",
      "75%       2.915000e+01  1.550000e+02\n",
      "max       3.400000e+01  5.000000e+02\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"avg_line_length\", \"line_count\"]].describe())\n",
    "print()\n",
    "print(df_filtered.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_cpp = {}\n",
    "\n",
    "for i, example in enumerate(df_filtered[\"content\"]):\n",
    "    data_dict_cpp[f\"cpp_{i}\"] = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_dict_cpp) == len(df_filtered), \"Data dictionary length does not match the number of examples in the dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example cpp_0:\n",
      "/**\n",
      " * Copyright (c) Facebook, Inc. and its affiliates.\n",
      " *\n",
      " * This source code is licensed under the MIT license found in the\n",
      " * LICENSE file in the root directory of this source tree.\n",
      " */\n",
      "\n",
      "#include \"thread-local.h\"\n",
      "\n",
      "#ifdef __linux__\n",
      "#include <link.h>\n",
      "#include <asm/prctl.h>\n",
      "#include <sys/prctl.h>\n",
      "extern \"C\" {\n",
      "extern int arch_prctl(int, unsigned long*);\n",
      "}\n",
      "#endif //__linux__\n",
      "\n",
      "namespace HPHP {\n",
      "\n",
      "#ifdef USE_GCC_FAST_TLS\n",
      "\n",
      "void ThreadLocalManager::OnThreadExit(void* p) {\n",
      "  auto list = getList(p);\n",
      "  p = list->head;\n",
      "  delete list;\n",
      "  while (p != nullptr) {\n",
      "    auto* pNode = static_cast<ThreadLocalNode<void>*>(p);\n",
      "    if (pNode->m_on_thread_exit_fn) {\n",
      "      pNode->m_on_thread_exit_fn(p);\n",
      "    }\n",
      "    p = pNode->m_next;\n",
      "  }\n",
      "}\n",
      "\n",
      "void ThreadLocalManager::PushTop(void* nodePtr, size_t nodeSize) {\n",
      "  auto& node = *static_cast<ThreadLocalNode<void>*>(nodePtr);\n",
      "  auto key = GetManager().m_key;\n",
      "  auto list = getList(pthread_getspecific(key));\n",
      "  if (UNLIKELY(!list)) {\n",
      "    ThreadLocalSetValue(key, list = new ThreadLocalList);\n",
      "  }\n",
      "  node.m_next = list->head;\n",
      "  node.m_size = nodeSize;\n",
      "  list->head = node.m_next;\n",
      "}\n",
      "\n",
      "ThreadLocalManager& ThreadLocalManager::GetManager() {\n",
      "  static ThreadLocalManager m;\n",
      "  return m;\n",
      "}\n",
      "\n",
      "#ifdef __APPLE__\n",
      "ThreadLocalManager::ThreadLocalList::ThreadLocalList() {\n",
      "  pthread_t self = pthread_self();\n",
      "  handler.__routine = ThreadLocalManager::OnThreadExit;\n",
      "  handler.__arg = this;\n",
      "  handler.__next = self->__cleanup_stack;\n",
      "  self->__cleanup_stack = &handler;\n",
      "}\n",
      "#endif\n",
      "\n",
      "#endif\n",
      "\n",
      "#ifdef __linux__\n",
      "\n",
      "static int visit_phdr(dl_phdr_info* info, size_t, void*) {\n",
      "  for (size_t i = 0, n = info->dlpi_phnum; i < n; ++i) {\n",
      "    const auto& hdr = info->dlpi_phdr[i];\n",
      "    auto addr = info->dlpi_addr + hdr.p_vaddr;\n",
      "    if (addr < 0x100000000LL && hdr.p_type == PT_TLS) {\n",
      "      // found the main thread-local section\n",
      "      assert(int(hdr.p_memsz) == hdr.p_memsz); // ensure no truncation\n",
      "      return hdr.p_memsz;\n",
      "    }\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "std::pair<void*,size_t> getCppTdata() {\n",
      "  uintptr_t addr;\n",
      "  if (!arch_prctl(ARCH_GET_FS, &addr)) {\n",
      "    // fs points to the end of the threadlocal area.\n",
      "    size_t size = dl_iterate_phdr(&visit_phdr, nullptr);\n",
      "    return {(void*)(addr - size), size};\n",
      "  }\n",
      "  return {nullptr, 0};\n",
      "}\n",
      "\n",
      "#else\n",
      "\n",
      "// how do you find the thread local section on your system?\n",
      "std::pair<void*,size_t> getCppTdata() {\n",
      "  return {nullptr, 0};\n",
      "}\n",
      "\n",
      "#endif //__linux__\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Example cpp_1:\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/core/framework/common_shape_fns.h\"\n",
      "#include \"tensorflow/core/framework/op.h\"\n",
      "#include \"tensorflow/core/framework/shape_inference.h\"\n",
      "\n",
      "namespace tensorflow {\n",
      "namespace {\n",
      "\n",
      "// TODO(kttian): Support non-scalar values\n",
      "REGISTER_OP(\"EmptyTensorMap\")\n",
      "    .Output(\"handle: variant\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapSize\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Output(\"size: int32\")\n",
      "    .SetShapeFn(shape_inference::ScalarShape);\n",
      "\n",
      "REGISTER_OP(\"TensorMapLookup\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"value: value_dtype\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->UnknownShape());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapInsert\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Input(\"value: value_dtype\")\n",
      "    .Output(\"output_handle: variant\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapErase\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"output_handle: variant\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .Attr(\"value_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->Scalar());  // output map\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "REGISTER_OP(\"TensorMapHasKey\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Input(\"key: key_dtype\")\n",
      "    .Output(\"has_key: bool\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .SetShapeFn(shape_inference::ScalarShape);\n",
      "\n",
      "REGISTER_OP(\"TensorMapStackKeys\")\n",
      "    .Input(\"input_handle: variant\")\n",
      "    .Output(\"keys: key_dtype\")\n",
      "    .Attr(\"key_dtype: type\")\n",
      "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n",
      "      c->set_output(0, c->UnknownShape());  // output keys\n",
      "      return Status::OK();\n",
      "    });\n",
      "\n",
      "}  // namespace\n",
      "}  // namespace tensorflow\n",
      "\n",
      "\n",
      "\n",
      "Example cpp_2:\n",
      "<reponame>EricRemmerswaal/tensorflow\n",
      "/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "#include \"tensorflow/cc/experimental/libtf/object.h\"\n",
      "\n",
      "#include <cstdint>\n",
      "\n",
      "#include \"tensorflow/cc/experimental/libtf/value.h\"\n",
      "#include \"tensorflow/cc/experimental/libtf/value_iostream.h\"\n",
      "#include \"tensorflow/core/lib/core/status_test_util.h\"\n",
      "#include \"tensorflow/core/platform/statusor.h\"\n",
      "#include \"tensorflow/core/platform/test.h\"\n",
      "\n",
      "namespace tf {\n",
      "namespace libtf {\n",
      "\n",
      "TEST(ObjectTest, TestDictionary) {\n",
      "  Dictionary foo;\n",
      "  foo.Set(String(\"a\"), Integer(33));\n",
      "  foo.Set(String(\"b\"), Integer(33));\n",
      "  EXPECT_EQ(foo.Get<Integer>(String(\"b\"))->get(), 33);\n",
      "}\n",
      "\n",
      "TEST(ObjectTest, TestTuple) {\n",
      "  Tuple foo(String(\"a\"), Integer(33), Float(10.f));\n",
      "  EXPECT_EQ(foo.size(), 3);\n",
      "  EXPECT_EQ(foo.Get<Integer>(1)->get(), 33);\n",
      "}\n",
      "\n",
      "TEST(ObjectTest, TestList) {\n",
      "  List l;\n",
      "  EXPECT_EQ(l.size(), 0);\n",
      "  l.append(Integer(3));\n",
      "  EXPECT_EQ(l.Get<Integer>(0)->get(), 3);\n",
      "  EXPECT_EQ(l.size(), 1);\n",
      "}\n",
      "\n",
      "TaggedValue AddIntegers(TaggedValue args_, TaggedValue kwargs_) {\n",
      "  auto& args = args_.tuple();\n",
      "  // auto& kwargs = kwargs_.dict();\n",
      "  return TaggedValue(args[0].i64() + args[1].i64());\n",
      "}\n",
      "\n",
      "TEST(ObjectTest, TestCast) {\n",
      "  Integer i(3);\n",
      "  auto result = Cast<String>(i);\n",
      "  ASSERT_TRUE(!result.ok());\n",
      "}\n",
      "\n",
      "TEST(ObjectTest, TestCall) {\n",
      "  TaggedValue add_func(AddIntegers);\n",
      "  Callable add(add_func);\n",
      "  TF_ASSERT_OK_AND_ASSIGN(Integer i,\n",
      "                          add.Call<Integer>(Integer(1), Integer(10)));\n",
      "  EXPECT_EQ(i.get(), 11);\n",
      "\n",
      "  TF_ASSERT_OK_AND_ASSIGN(\n",
      "      Integer i2, add.Call<Integer>(1, Integer(10), KeywordArg(\"foo\") = 3));\n",
      "  EXPECT_EQ(i2.get(), 11);\n",
      "}\n",
      "\n",
      "TEST(ObjectTest, MakeObject) {\n",
      "  // TaggedValue func(f);\n",
      "  Object parent;\n",
      "  parent.Set(String(\"test3\"), Integer(3));\n",
      "  Object child;\n",
      "  child.Set(String(\"test1\"), Integer(1));\n",
      "  child.Set(String(\"test2\"), Integer(2));\n",
      "  child.Set(Object::ParentKey(), parent);\n",
      "  EXPECT_EQ(child.Get<Integer>(String(\"test1\"))->get(), 1);\n",
      "  EXPECT_EQ(child.Get<Integer>(String(\"test2\"))->get(), 2);\n",
      "  EXPECT_EQ(child.Get<Integer>(String(\"test3\"))->get(), 3);\n",
      "  ASSERT_FALSE(child.Get<Integer>(String(\"test4\")).status().ok());\n",
      "  TF_ASSERT_OK(child.Get(String(\"test3\")).status());\n",
      "}\n",
      "\n",
      "TEST(ObjectTest, CallFunctionOnObject) {\n",
      "  Object module;\n",
      "  module.Set(String(\"add\"), Callable(TaggedValue(AddIntegers)));\n",
      "  TF_ASSERT_OK_AND_ASSIGN(Callable method, module.Get<Callable>(String(\"add\")));\n",
      "\n",
      "  TF_ASSERT_OK_AND_ASSIGN(Integer val, method.Call<Integer>(1, 2));\n",
      "  EXPECT_EQ(val.get(), 3);\n",
      "}\n",
      "\n",
      "TEST(ObjectTest, Capsule) {\n",
      "  Object obj;\n",
      "  int* hundred = new int(100);\n",
      "  Handle capsule =\n",
      "      Handle(TaggedValue::Capsule(static_cast<void*>(hundred), [](void* p) {\n",
      "        delete static_cast<int*>(p);\n",
      "      }));\n",
      "  obj.Set(String(\"hundred\"), capsule);\n",
      "  EXPECT_EQ(*static_cast<int*>(\n",
      "                obj.Get<internal::Capsule>(String(\"hundred\"))->cast<int*>()),\n",
      "            100);\n",
      "}\n",
      "\n",
      "None AppendIntegerToList(List a, Integer b) {\n",
      "  a.append(b);\n",
      "  return None();\n",
      "}\n",
      "Integer AddIntegersTyped(Integer a, Integer b) {\n",
      "  return Integer(a.get() + b.get());\n",
      "}\n",
      "Integer ReturnFive() { return Integer(5); }\n",
      "\n",
      "TEST(TypeUneraseCallTest, TestCallable) {\n",
      "  // Add two integers.\n",
      "  Callable add(TFLIB_CALLABLE_ADAPTOR(AddIntegersTyped));\n",
      "  auto res = add.Call<Integer>(Integer(3), Integer(1));\n",
      "  EXPECT_EQ(res->get(), 4);\n",
      "}\n",
      "\n",
      "TEST(TypeUneraseCallTest, TestAppend) {\n",
      "  // Append some indices to a list.\n",
      "  Callable append(TFLIB_CALLABLE_ADAPTOR(AppendIntegerToList));\n",
      "  List l;\n",
      "  TF_ASSERT_OK(append.Call<None>(l, Integer(3)).status());\n",
      "  TF_ASSERT_OK(append.Call<None>(l, Integer(6)).status());\n",
      "  EXPECT_EQ(l.size(), 2);\n",
      "  EXPECT_EQ(l.Get<Integer>(0)->get(), 3);\n",
      "  EXPECT_EQ(l.Get<Integer>(1)->get(), 6);\n",
      "}\n",
      "\n",
      "TEST(TypeUneraseCallTest, TestCallableWrongArgs) {\n",
      "  // Try variants of wrong argument types.\n",
      "  Callable append(TFLIB_CALLABLE_ADAPTOR(AddIntegersTyped));\n",
      "  ASSERT_FALSE(append.Call<None>(Object(), Integer(3)).ok());\n",
      "  ASSERT_FALSE(append.Call<None>(Object(), Object()).ok());\n",
      "  // Try variants of wrong numbers of arguments.\n",
      "  ASSERT_FALSE(append.Call().ok());\n",
      "  ASSERT_FALSE(append.Call(Integer(3)).ok());\n",
      "  ASSERT_FALSE(append.Call(Integer(3), Integer(4), Integer(5)).ok());\n",
      "}\n",
      "\n",
      "Handle Polymorph(Handle a) {\n",
      "  auto i = Cast<Integer>(a);\n",
      "  if (i.ok()) {\n",
      "    return Integer(i->get() * 2);\n",
      "  }\n",
      "  auto f = Cast<Float>(a);\n",
      "  if (f.ok()) {\n",
      "    return Float(f->get() * 2.f);\n",
      "  }\n",
      "  return None();\n",
      "}\n",
      "\n",
      "TEST(TypeUneraseCallTest, TestCallableGeneric) {\n",
      "  Callable f(TFLIB_CALLABLE_ADAPTOR(Polymorph));\n",
      "  EXPECT_EQ(f.Call<Float>(Float(.2))->get(), .4f);\n",
      "  EXPECT_EQ(Cast<Float>(*f.Call(Float(.2)))->get(), .4f);\n",
      "  EXPECT_EQ(f.Call<Integer>(Integer(3))->get(), 6);\n",
      "}\n",
      "\n",
      "TEST(TypeUneraseCallTest, TestLambda) {\n",
      "  // Test a trivial lambda that doubles an integer.\n",
      "  Callable c(\n",
      "      TFLIB_CALLABLE_ADAPTOR([](Integer a) { return Integer(a.get() * 2); }));\n",
      "  EXPECT_EQ(c.Call<Integer>(Integer(3))->get(), 6);\n",
      "  // Testa lambda that has captured state (call count).\n",
      "  int call_count = 0;\n",
      "  Callable f(TFLIB_CALLABLE_ADAPTOR([&call_count](Integer a, Integer b) {\n",
      "    call_count++;\n",
      "    return Integer(a.get() + b.get());\n",
      "  }));\n",
      "  EXPECT_EQ(f.Call<Integer>(Integer(3), Integer(-1))->get(), 2);\n",
      "  EXPECT_EQ(f.Call<Integer>(Integer(3), Integer(-3))->get(), 0);\n",
      "  EXPECT_EQ(call_count, 2);\n",
      "}\n",
      "\n",
      "}  // namespace libtf\n",
      "}  // namespace tf\n",
      "\n",
      "\n",
      "\n",
      "Example cpp_3:\n",
      "<filename>tensorflow/c/tf_shape.cc<gh_stars>1000+\n",
      "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/c/tf_shape.h\"\n",
      "\n",
      "#include <stdint.h>\n",
      "\n",
      "#include \"tensorflow/c/tf_shape_internal.h\"\n",
      "#include \"tensorflow/core/framework/tensor_shape.h\"\n",
      "\n",
      "extern \"C\" {\n",
      "\n",
      "TF_Shape* TF_NewShape() {\n",
      "  return tensorflow::wrap(new tensorflow::PartialTensorShape());\n",
      "}\n",
      "\n",
      "int TF_ShapeDims(const TF_Shape* shape) {\n",
      "  return tensorflow::unwrap(shape)->dims();\n",
      "}\n",
      "\n",
      "int64_t TF_ShapeDimSize(const TF_Shape* shape, int d) {\n",
      "  return tensorflow::unwrap(shape)->dim_size(d);\n",
      "}\n",
      "\n",
      "void TF_DeleteShape(TF_Shape* shape) { delete tensorflow::unwrap(shape); }\n",
      "\n",
      "}  // end extern \"C\"\n",
      "\n",
      "\n",
      "\n",
      "Example cpp_4:\n",
      "/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "#include \"tensorflow/lite/delegates/gpu/common/tasks/special/fc_fc_add.h\"\n",
      "\n",
      "#include <string>\n",
      "#include <utility>\n",
      "#include <vector>\n",
      "\n",
      "#include \"absl/memory/memory.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/operations.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/task/gpu_operation.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/task/tensor_linear_desc.h\"\n",
      "#include \"tensorflow/lite/delegates/gpu/common/types.h\"\n",
      "\n",
      "namespace tflite {\n",
      "namespace gpu {\n",
      "namespace {\n",
      "bool UseBufferForWeights(const GpuInfo& gpu_info) {\n",
      "  return gpu_info.IsAdreno() || gpu_info.IsAMD() || gpu_info.IsMali();\n",
      "}\n",
      "\n",
      "void RearrangeFCWeightsToOIO4I4(\n",
      "    const tflite::gpu::Tensor<OHWI, DataType::INT8>& weights, uint8_t* dst) {\n",
      "  const int src_depth = DivideRoundUp(weights.shape.i, 4);\n",
      "  const int dst_depth = DivideRoundUp(weights.shape.o, 4);\n",
      "\n",
      "  int counter = 0;\n",
      "  for (int d = 0; d < dst_depth; ++d) {\n",
      "    for (int s = 0; s < src_depth; ++s) {\n",
      "      for (int i = 0; i < 4; ++i) {\n",
      "        const int src_ch = s * 4 + i;\n",
      "        for (int j = 0; j < 4; ++j) {\n",
      "          const int dst_ch = d * 4 + j;\n",
      "          if (src_ch < weights.shape.i && dst_ch < weights.shape.o) {\n",
      "            int t =\n",
      "                127 +\n",
      "                weights.data[weights.shape.LinearIndex({dst_ch, 0, 0, src_ch})];\n",
      "            if (t < 0) {\n",
      "              t = 0;\n",
      "            }\n",
      "            dst[counter++] = t;\n",
      "          } else {\n",
      "            dst[counter++] = 127;\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "}  // namespace\n",
      "\n",
      "FCFCAdd::FCFCAdd(const OperationDef& definition, const GpuInfo& gpu_info)\n",
      "    : GPUOperation(definition) {\n",
      "  if (gpu_info.IsAdreno()) {\n",
      "    if (gpu_info.adreno_info.IsAdreno3xx()) {\n",
      "      work_group_size_ = int3(16, 4, 1);\n",
      "    } else if (gpu_info.adreno_info.IsAdreno4xx()) {\n",
      "      work_group_size_ = int3(32, 4, 1);\n",
      "    } else {\n",
      "      work_group_size_ = int3(32, 4, 1);\n",
      "    }\n",
      "  } else if (gpu_info.IsIntel()) {\n",
      "    work_group_size_ = int3(8, 4, 1);\n",
      "  } else if (gpu_info.IsNvidia()) {\n",
      "    work_group_size_ = int3(8, 4, 1);\n",
      "  } else if (gpu_info.IsPowerVR()) {\n",
      "    work_group_size_ = int3(8, 4, 1);\n",
      "  } else {\n",
      "    work_group_size_ = int3(16, 4, 1);\n",
      "  }\n",
      "}\n",
      "\n",
      "FCFCAdd::FCFCAdd(FCFCAdd&& kernel) : GPUOperation(std::move(kernel)) {}\n",
      "\n",
      "FCFCAdd& FCFCAdd::operator=(FCFCAdd&& kernel) {\n",
      "  if (this != &kernel) {\n",
      "    GPUOperation::operator=(std::move(kernel));\n",
      "  }\n",
      "  return *this;\n",
      "}\n",
      "\n",
      "// We split vec vec dot (every thread do vec vec dot product in basic\n",
      "// vec mat mult) on 4 parts to create more threads\n",
      "// tid.y thread process every 4-th element in vec vec dot\n",
      "// Good results for ~1024 x 1024 sizes, for other can be written more\n",
      "// optimized shaders\n",
      "\n",
      "std::string FCFCAdd::GetFCFCAddKernelCode(const OperationDef& op_def,\n",
      "                                          const GpuInfo& gpu_info,\n",
      "                                          bool weights_are_buffer,\n",
      "                                          bool quantized_0, bool quantized_1) {\n",
      "  AddSrcTensor(\"src_tensor_0\", op_def.src_tensors[0]);\n",
      "  AddSrcTensor(\"src_tensor_1\", op_def.src_tensors[1]);\n",
      "  AddDstTensor(\"dst_tensor\", op_def.dst_tensors[0]);\n",
      "\n",
      "  std::string c;\n",
      "  switch (op_def.precision) {\n",
      "    case CalculationsPrecision::F32:\n",
      "      c += \"#define FLT16 float16\\n\";\n",
      "      break;\n",
      "    case CalculationsPrecision::F32_F16:\n",
      "    case CalculationsPrecision::F16:\n",
      "      c += \"#define FLT16 half16\\n\";\n",
      "      break;\n",
      "  }\n",
      "\n",
      "  c += \"#define WG_X \" + std::to_string(work_group_size_.x) + \"\\n\";\n",
      "  c += \"#define WG_Y \" + std::to_string(work_group_size_.y) + \"\\n\";\n",
      "\n",
      "  c += R\"(MAIN_FUNCTION($0) {\n",
      "  int gid = get_global_id(0);\n",
      "  int2 tid;\n",
      "  tid.x = LOCAL_ID_0;\n",
      "  tid.y = LOCAL_ID_1;\n",
      "  ACCUM_FLT4 s = INIT_ACCUM_FLT4(0.0f);\n",
      "  if (gid < args.dst_tensor.Slices()) {\n",
      "    for (int c = tid.y; c < args.src_tensor_0.Slices(); c += WG_Y) {\n",
      "      FLT4 v = args.src_tensor_0.Read(0, 0, c);\n",
      ")\";\n",
      "  if (weights_are_buffer) {\n",
      "    c += R\"(FLT16 w = args.weights0.Read(c * args.dst_tensor.Slices() + gid);\n",
      "      FLT4 partial = v.x * FLT16_0123(w);\n",
      "      partial += v.y * FLT16_4567(w);\n",
      "      partial += v.z * FLT16_89ab(w);\n",
      "      partial += v.w * FLT16_cdef(w);\n",
      "      s += TO_ACCUM_TYPE(partial);\n",
      ")\";\n",
      "  } else {\n",
      "    c += R\"(FLT4 w0 = args.weights0.Read(c * 4 + 0, gid);\n",
      "      FLT4 w1 = args.weights0.Read(c * 4 + 1, gid);\n",
      "      FLT4 w2 = args.weights0.Read(c * 4 + 2, gid);\n",
      "      FLT4 w3 = args.weights0.Read(c * 4 + 3, gid);\n",
      "      )\";\n",
      "    if (quantized_0) {\n",
      "      c += R\"(w0 = w0 * args.q0_m + args.q0_a;\n",
      "      w1 = w1 * args.q0_m + args.q0_a;\n",
      "      w2 = w2 * args.q0_m + args.q0_a;\n",
      "      w3 = w3 * args.q0_m + args.q0_a;\n",
      ")\";\n",
      "    }\n",
      "    c += R\"(FLT4 partial = v.x * w0;\n",
      "      partial += v.y * w1;\n",
      "      partial += v.z * w2;\n",
      "      partial += v.w * w3;\n",
      "      s += TO_ACCUM_TYPE(partial);\n",
      ")\";\n",
      "  }\n",
      "  c += R\"(    }\n",
      "    for (int c = tid.y; c < args.src_tensor_1.Slices(); c += WG_Y) {\n",
      "      FLT4 v = args.src_tensor_1.Read(0, 0, c);\n",
      "      )\";\n",
      "  if (weights_are_buffer) {\n",
      "    c += R\"(FLT16 w = args.weights1.Read(c * args.dst_tensor.Slices() + gid);\n",
      "      FLT4 partial = v.x * FLT16_0123(w);\n",
      "      partial += v.y * FLT16_4567(w);\n",
      "      partial += v.z * FLT16_89ab(w);\n",
      "      partial += v.w * FLT16_cdef(w);\n",
      "      s += TO_ACCUM_TYPE(partial);\n",
      ")\";\n",
      "  } else {\n",
      "    c += R\"(FLT4 w0 = args.weights1.Read(c * 4 + 0, gid);\n",
      "      FLT4 w1 = args.weights1.Read(c * 4 + 1, gid);\n",
      "      FLT4 w2 = args.weights1.Read(c * 4 + 2, gid);\n",
      "      FLT4 w3 = args.weights1.Read(c * 4 + 3, gid);\n",
      "      )\";\n",
      "    if (quantized_1) {\n",
      "      c += R\"(w0 = w0 * args.q1_m + args.q1_a;\n",
      "      w1 = w1 * args.q1_m + args.q1_a;\n",
      "      w2 = w2 * args.q1_m + args.q1_a;\n",
      "      w3 = w3 * args.q1_m + args.q1_a;\n",
      ")\";\n",
      "    }\n",
      "    c += R\"(FLT4 partial = v.x * w0;\n",
      "      partial += v.y * w1;\n",
      "      partial += v.z * w2;\n",
      "      partial += v.w * w3;\n",
      "      s += TO_ACCUM_TYPE(partial);\n",
      ")\";\n",
      "  }\n",
      "  c += R\"(    }\n",
      "  }\n",
      "  __local ACCUM_FLT4 temp[WG_X][WG_Y];\n",
      "  temp[tid.x][tid.y] = s;\n",
      "  LOCAL_MEM_BARRIER;\n",
      "  if (gid >= args.dst_tensor.Slices()) {\n",
      "    return;\n",
      "  }\n",
      "  if (tid.y == 0) {\n",
      ")\";\n",
      "  for (int i = 1; i < work_group_size_.y; ++i) {\n",
      "    c += \"    s += temp[tid.x][\" + std::to_string(i) + \"];\\n\";\n",
      "  }\n",
      "  c +=\n",
      "      R\"(    FLT4 r0 = TO_FLT4(s) + args.biases0.Read(gid) + args.biases1.Read(gid);\n",
      "    args.dst_tensor.Write(r0, 0, 0, gid);\n",
      "  }\n",
      "})\";\n",
      "\n",
      "  return c;\n",
      "}\n",
      "\n",
      "int3 FCFCAdd::GetGridSize() const { return int3(dst_[0]->Slices(), 1, 1); }\n",
      "\n",
      "void FCFCAdd::UploadQuantizedWeights(\n",
      "    const tflite::gpu::Tensor<OHWI, DataType::INT8>& weights, float scale,\n",
      "    float zero_point, int index) {\n",
      "  const bool f32_weights = definition_.precision == CalculationsPrecision::F32;\n",
      "  const int src_depth = DivideRoundUp(weights.shape.i, 4);\n",
      "  const int dst_depth = DivideRoundUp(weights.shape.o, 4);\n",
      "  Texture2DDescriptor desc;\n",
      "  desc.element_type = DataType::UINT8;\n",
      "  desc.normalized = true;\n",
      "  desc.normalized_type = f32_weights ? DataType::FLOAT32 : DataType::FLOAT16;\n",
      "  desc.size = int2(src_depth * 4, dst_depth);\n",
      "  desc.data.resize(src_depth * 4 * dst_depth * 4);\n",
      "  RearrangeFCWeightsToOIO4I4(weights, desc.data.data());\n",
      "\n",
      "  std::string q_name = \"q\" + std::to_string(index) + \"_\";\n",
      "  if (definition_.precision == CalculationsPrecision::F32) {\n",
      "    args_.AddFloat(q_name + \"m\", scale * 255.0f);\n",
      "    args_.AddFloat(q_name + \"a\", -scale * (127.0 + zero_point));\n",
      "  } else {\n",
      "    args_.AddHalf(q_name + \"m\", half(scale * 255.0f));\n",
      "    args_.AddHalf(q_name + \"a\", half(-scale * (127.0 + zero_point)));\n",
      "  }\n",
      "  args_.AddObject(\"weights\" + std::to_string(index),\n",
      "                  absl::make_unique<Texture2DDescriptor>(std::move(desc)));\n",
      "}\n",
      "\n",
      "FCFCAdd CreateFCFCAdd(const GpuInfo& gpu_info, const OperationDef& definition,\n",
      "                      const FullyConnectedAttributes& attr0,\n",
      "                      const FullyConnectedAttributes& attr1) {\n",
      "  FCFCAdd result(definition, gpu_info);\n",
      "  bool weights_are_buffer = UseBufferForWeights(gpu_info);\n",
      "  result.UploadWeights(attr0.weights, \"weights0\", weights_are_buffer);\n",
      "  result.UploadWeights(attr1.weights, \"weights1\", weights_are_buffer);\n",
      "  result.code_ = result.GetFCFCAddKernelCode(definition, gpu_info,\n",
      "                                             weights_are_buffer, false, false);\n",
      "\n",
      "  TensorLinearDescriptor desc0;\n",
      "  desc0.storage_type = LinearStorageType::TEXTURE_2D;\n",
      "  desc0.element_type = definition.GetDataType();\n",
      "  desc0.UploadLinearData(attr0.bias);\n",
      "  result.args_.AddObject(\n",
      "      \"biases0\", absl::make_unique<TensorLinearDescriptor>(std::move(desc0)));\n",
      "\n",
      "  TensorLinearDescriptor desc1;\n",
      "  desc1.storage_type = LinearStorageType::TEXTURE_2D;\n",
      "  desc1.element_type = definition.GetDataType();\n",
      "  desc1.UploadLinearData(attr1.bias);\n",
      "  result.args_.AddObject(\n",
      "      \"biases1\", absl::make_unique<TensorLinearDescriptor>(std::move(desc1)));\n",
      "\n",
      "  return result;\n",
      "}\n",
      "\n",
      "FCFCAdd CreateFCFCAdd(const GpuInfo& gpu_info, const OperationDef& definition,\n",
      "                      const FullyConnectedInt8Attributes& attr0,\n",
      "                      const FullyConnectedInt8Attributes& attr1) {\n",
      "  FCFCAdd result(definition, gpu_info);\n",
      "  result.UploadQuantizedWeights(attr0.weights, attr0.scale, attr0.zero_point,\n",
      "                                0);\n",
      "  result.UploadQuantizedWeights(attr1.weights, attr1.scale, attr1.zero_point,\n",
      "                                1);\n",
      "  result.code_ =\n",
      "      result.GetFCFCAddKernelCode(definition, gpu_info, false, true, true);\n",
      "\n",
      "  TensorLinearDescriptor desc0;\n",
      "  desc0.storage_type = LinearStorageType::TEXTURE_2D;\n",
      "  desc0.element_type = definition.GetDataType();\n",
      "  desc0.UploadLinearData(attr0.bias);\n",
      "  result.args_.AddObject(\n",
      "      \"biases0\", absl::make_unique<TensorLinearDescriptor>(std::move(desc0)));\n",
      "\n",
      "  TensorLinearDescriptor desc1;\n",
      "  desc1.storage_type = LinearStorageType::TEXTURE_2D;\n",
      "  desc1.element_type = definition.GetDataType();\n",
      "  desc1.UploadLinearData(attr1.bias);\n",
      "  result.args_.AddObject(\n",
      "      \"biases1\", absl::make_unique<TensorLinearDescriptor>(std::move(desc1)));\n",
      "\n",
      "  return result;\n",
      "}\n",
      "\n",
      "}  // namespace gpu\n",
      "}  // namespace tflite\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (k,v) in enumerate(data_dict_cpp.items()):\n",
    "    if i < 5:\n",
    "        print(f\"Example {k}:\")\n",
    "        print(v)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to 'cpp_train.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('cpp_train.json', 'w') as f:\n",
    "    json.dump(data_dict_cpp, f)\n",
    "    print(\"Saved to 'cpp_train.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malcodeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
